from src.action.base_action import OptimizeAction
from src.evaluator import PromptEvaluator
from src.mcts.node import Node, Step
from src.pool.sample_pools import DynamicSamplePool
from src.logger import logger
from typing import List, Set
import numpy as np
import math

class PromptNode(Node):
    def __init__(self, 
                 action_set: Set[OptimizeAction],
                 action_seq: List[OptimizeAction], 
                 trajectory_prompts: List[str],
                 prompt: str, 
                 evaluator: PromptEvaluator, 
                 depth: int, 

                 sample_pool: DynamicSamplePool = None,

                 Q:float=0.0, N:int=0, 
                 uct_value:float=0.0, 
                 parent=None
                 ):
        super().__init__(depth=depth, Q=Q, N=N, uct_value=uct_value, parent=parent)
        self.type = prompt
        self.action_set: Set[OptimizeAction] = action_set
        self.evaluator: PromptEvaluator = evaluator
        self.trajectory_prompts: List[str] = trajectory_prompts
        self.current_prompt: str = prompt
        logger.info(f"ðŸ“œ Get new node at depth {depth} using action {action_seq[-1].name if len(action_seq) > 0 else None}")
        
        self.action_seq: List[OptimizeAction] = action_seq
        self.pool = sample_pool
        self.reward_value: float = self.reward()
        self.Q = self.reward_value

    
    def _weighted_random_choice(self, alpha=1.0, temperature=1.0) -> OptimizeAction:
        def _softmax(vals: np.ndarray, temperature: float):
            vals = vals - np.max(vals)  # é˜²æ­¢ exp æº¢å‡º
            exp_vals = np.exp(vals / max(temperature, 1e-6))
            probs = exp_vals / np.sum(exp_vals)
            return probs

        """
        åŠ¨ä½œé€‰æ‹©ï¼šèžåˆå¤±è´¥/ä½¿ç”¨æ¬¡æ•°å’Œæ ·æœ¬æ± çŠ¶æ€ï¼ˆcpool diagnosticsï¼‰ï¼Œå¸¦å¹³æ»‘ã€‚
        é€»è¾‘ï¼š
        - åŸºç¡€é¡¹ï¼šåŽ†å²é‡‡æ ·å¤±è´¥æ¬¡æ•°å’Œä½¿ç”¨æ¬¡æ•°é™ä½Žæ¦‚çŽ‡ï¼ˆæŽ¢ç´¢/é¿å…é‡å¤ï¼‰
        - æ ·æœ¬æ±  biasï¼šæ ¹æ®æ± å­ä¸­æˆåŠŸ/å¤±è´¥æ ·æœ¬æ¯”ä¾‹è°ƒèŠ‚ Positive / Negative åŠ¨ä½œåå¥½
        - æžç«¯æƒ…å†µæŠ‘åˆ¶ï¼šå½“æ± å­ä¸­æˆåŠŸ/å¤±è´¥æ ·æœ¬æžå°‘æ—¶ï¼ŒæŠ‘åˆ¶å¯¹åº”åŠ¨ä½œ
        """
        actions: list[OptimizeAction] = list(self.action_set)

        # åŸºç¡€é¡¹: failure + usage æŠ‘åˆ¶
        failure_counts = np.array([a.sample_failure_counter for a in actions])
        usage_counts = np.array([a.usage_count for a in actions])
        base_logits = - (failure_counts + usage_counts)

        # æ ·æœ¬æ± çŠ¶æ€ï¼Œå¯é€‰
        action_bias = np.zeros(len(actions))
        if self.pool is not None:
            cpool_diag = self.pool.compute_cpool()
            easy_ratio = cpool_diag.get("easy_ratio", 0.0)
            informative_ratio = cpool_diag.get("informative_ratio", 0.0)
            hard_ratio = cpool_diag.get("hard_ratio", 0.0)

            success_ratio = easy_ratio + informative_ratio
            failure_ratio = hard_ratio

            # ç”¨å¹³æ»‘å‡½æ•° (tanh) è®¡ç®—å·®å¼‚åç½®
            diff = success_ratio - failure_ratio
            # æ›²çº¿é™¡å³­åº¦éšæ ·æœ¬æ± è§„æ¨¡è°ƒæ•´
            scale = math.log1p(cpool_diag.get("total_samples", 0.0))
            smooth_diff = np.tanh(diff * scale)
            # æ ·æœ¬æ± è¶Šå¤§ï¼Œè¯´æ˜Žç»Ÿè®¡ç»“æžœè¶Šå¯é ï¼Œå°±å¯ä»¥æ›´â€œè‡ªä¿¡â€åœ°æ”¾å¤§ diff å€¼

            entropy = - (
                success_ratio * math.log(success_ratio + 1e-6) +
                failure_ratio * math.log(failure_ratio + 1e-6)
            )

            for i, a in enumerate(actions):
                if a.name.lower().startswith("SuccessDrivenAction"):
                    action_bias[i] = entropy * smooth_diff
                elif a.name.lower().startswith("FailureDrivenAction"):
                    action_bias[i] = - entropy * smooth_diff

        # èžåˆ
        logits = base_logits + alpha * action_bias
        probs = _softmax(logits, temperature)
        selected_index = np.random.choice(len(actions), p=probs)
        return actions[selected_index]
    
    def get_exploration_weight(self, exploration_weight=1.41):
        def cpool_mapping(cpool: float, k: float = 3.0) -> float:
            """
            å°† cpool å€¼ (0~1) å¹³æ»‘æ˜ å°„åˆ° [0.5, 2] èŒƒå›´
            - cpool è¶Šå¤§ï¼ŒæŽ¢ç´¢ç³»æ•°è¶Šå¤§
            - k æŽ§åˆ¶å¢žé•¿çš„å¿«æ…¢
            """
            return 0.5 + 1.5 * (1 - np.exp(-k * cpool))
        if self.pool:
            return cpool_mapping(self.pool.compute_cpool()['cpool'])
        return super().get_exploration_weight(exploration_weight)

    def take_action(self, step_type:Step):
        # Then apply the strategy-level semantic transformation.
        params_bundle = self.pool.get_net_controller().predict_and_apply()
        action:OptimizeAction = self._weighted_random_choice(params_bundle.get_mcts_alpha().item())
        new_prompt = action.do(
            current_prompt=self.current_prompt, 
            trajectory_prompts=self.trajectory_prompts, 
            sample_pool=self.pool
            )
        logger.info(f"ðŸ“Š Current Step Type:{step_type}")
        logger.info(f"ðŸ“Š Current Prompt:\n{new_prompt}")
        new_child = PromptNode(
            action_set=self.action_set,
            action_seq=self.action_seq + [action],
            trajectory_prompts=self.trajectory_prompts + [self.current_prompt],
            prompt=new_prompt,
            evaluator=self.evaluator,
            sample_pool=self.pool,

            depth=self.depth + 1,
            parent=self,
        )
        return new_child

    def reward(self):
        val_samples = self.evaluator.task.get_eval()
        score = self.evaluator.batch_reward(self.current_prompt, val_samples)
        logger.info(f"ðŸŽ¯ [Reward] Prompt evaluation score = {score:.4f}, Action sequence = {[a.name for a in self.action_seq]}")
        self.pool.get_net_controller().reforce(score)
        return score
    
    def q_value(self, last_q, rollout_reward):
        return last_q + rollout_reward
    
    def serialize(self, node_id:int):
        node_dict = {
            "id": node_id,
            "depth": self.depth,
            "action_sequence": [a.name for a in self.action_seq],
            "prompt": self.current_prompt,
            "Q": self.Q,
            "N": self.N, 
            "uct_value": self.uct_value,
            "reward": self.reward_value,
            "children": []
        }

        for child in self.children:
            node_id += 1
            child_serialized = child.serialize(node_id)
            if child_serialized:
                node_dict["children"].append(child_serialized)

        return node_dict