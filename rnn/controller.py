import torch
import torch.nn.functional as F
from torch.optim import Adam
from typing import List, Optional
from .rnn import RNN  # å‡è®¾ä½ çš„RNNå®šä¹‰åœ¨.rnnæ¨¡å—ä¸­
from logger import logger
from visualizer import Visualizer 

class TemplateController:
    def __init__(self, search_space: List[int], hidden_dim: int = 128, lr=1e-3, aux_loss_coef=0.1):
        self.model: RNN = RNN(search_space, hidden_dim)
        self.optimizer = Adam(self.model.parameters(), lr=lr)
        self.search_space = search_space

        self.baseline = 0.0
        self.baseline_alpha = 0.9
        self.aux_loss_coef = aux_loss_coef

        # å­˜å‚¨ forward æ—¶çš„ logit ç”¨äºç»“æ„å½’å› 
        self.last_logits: list = None

        logger.info(f"ğŸ“ˆ [RNNController] åˆå§‹åŒ–å®Œæˆ - å‚æ•°ä¸ªæ•°: {len(search_space)}")

    def train_step(self):
        self.model.train()
        flat_params, log_prob, entropy, logits_list = self.model(return_logits=True)
        self.last_logits = logits_list  
        return flat_params, log_prob, entropy

    def reinforce(self,
                  log_prob_sum: torch.Tensor,
                  reward: float,
                  entropy: torch.Tensor,
                  slot_rewards: Optional[List[float]] = None):
        """ä¸»ç­–ç•¥æ¢¯åº¦ + slot-level KLå½’å› è¾…åŠ©"""

        self.model.train()

        # ---- ä¸»ç­–ç•¥æ¢¯åº¦æ›´æ–° ----
        advantage = reward - self.baseline
        self.baseline = self.baseline_alpha * self.baseline + (1 - self.baseline_alpha) * reward
        entropy_weight = 0.01

        loss = -advantage * log_prob_sum - entropy_weight * entropy

        # ---- slot-level ç»“æ„å½’å› è¾…åŠ© loss ----
        if slot_rewards is not None and self.last_logits is not None:
            slot_rewards_tensor = torch.tensor(slot_rewards, dtype=torch.float32)

            # Normalize slot rewards into a probability distribution
            target_probs = torch.softmax(slot_rewards_tensor, dim=0).detach()  # [num_slots]

            aux_loss = 0.0
            for i, logits in enumerate(self.last_logits):
                pred_log_prob = F.log_softmax(logits.unsqueeze(0), dim=1)  # logits shape: [slot_dim] -> [1, slot_dim]
                target_prob_expanded = target_probs[i].unsqueeze(0).expand_as(pred_log_prob)  # [1, slot_dim]

                # KL divergence between pred and target probabilities for this slot
                # è¿™é‡Œ target_prob_expanded æ˜¯æ ‡é‡æ‰©å±•ï¼Œå®é™…ä¸Š KL ç”¨æ ‡é‡æ²¡æ„ä¹‰ï¼Œåº”è¯¥ç”¨ slot_rewards[i] æŒ‡å‘æŸä¸ªå…·ä½“ç±»åˆ«æ¦‚ç‡
                # å¦‚æœ slot_rewards æ˜¯ scalar reward per slotï¼Œç›´æ¥ç”¨å®ƒåš softmaxä¸åˆç†
                # é€šå¸¸ slot_rewards åº”è¯¥æ˜¯æ¯ä¸ª slot å„ä¸ªåŠ¨ä½œçš„ rewardå‘é‡ï¼Œè‹¥æ˜¯æ ‡é‡åˆ™éœ€è¦è®¾è®¡ä¸åŒçš„è¾…åŠ©ä¿¡å·ã€‚
                # æ­¤å¤„ç®€å•åšæ¯”ä¾‹æ”¾å¤§è¾…åŠ©ï¼Œæˆ–è€…ä½ å¯ä»¥ç”¨ slot_rewards åšæƒé‡åŠ æƒ
                # è¿™é‡Œå…ˆç”¨ç®€åŒ–çš„ MSE loss æ›¿ä»£ç¤ºèŒƒ
                target_dist = torch.full_like(pred_log_prob, target_probs[i].item())  # è®©ç›®æ ‡åˆ†å¸ƒæ˜¯å‡åŒ€æˆ–å®šå€¼(ç¤ºä¾‹)
                aux_loss += F.kl_div(pred_log_prob, target_dist, reduction='batchmean')

            loss += self.aux_loss_coef * aux_loss
            logger.info(f"ğŸ§© [RNNController] åŠ å…¥ç»“æ„å½’å› è¾…åŠ© loss = {aux_loss.item():.4f}")

        # ---- å‚æ•°æ›´æ–° ----
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()

        logger.info(f"ğŸ“ˆ [RNNController] REINFORCE å®Œæˆ - reward={reward:.4f}, loss={loss.item():.4f}, entropy={entropy.item():.4f}")
        Visualizer.log_train(loss.item(), reward, entropy.item())
    
