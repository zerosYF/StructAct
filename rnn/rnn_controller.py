import torch
from torch.optim import Adam
from typing import List
from Experiment.rnn.rnn import RNN
from logger import logger
from visualizer import Visualizer
from Experiment.rnn.blocks import PromptBlock

class RNNController:
    def __init__(self, blocks:List[PromptBlock], hidden_dim: int = 128, lr=1e-3):
        self.blocks = blocks
        self.search_space = [dim for block in blocks for dim in block.get_search_space()]
        self.model:RNN = RNN(self.search_space, hidden_dim)
        self.optimizer = Adam(self.model.parameters(), lr=lr)
        self.baseline = 0.0
        self.baseline_alpha = 0.9


    def decode(self, flat_params: List[int]) -> List[str]:
        """
        å°† RNN è¾“å‡ºçš„è¶…å‚æ•°æ‰å¹³å‘é‡è½¬æ¢ä¸ºæ¯ä¸ª Block çš„æ¸²æŸ“æ–‡æœ¬
        """
        idx = 0
        results = []
        for block in self.blocks:
            num_slots = block.get_num_slots()
            params = flat_params[idx:idx + num_slots]
            rendered = block.render(params)
            results.append(rendered)
            idx += num_slots
        return results
    
    def get_slot_dims(self) -> List[int]:
        return self.search_space

    def render_prompt(self, flat_params: List[int]) -> str:
        return "\n".join(self.decode(flat_params))
    
    def train_step(self):
        self.model.train()
        actions, log_prob, entropy = self.model()
        return actions, log_prob, entropy

    def reinforce(self, log_prob_sum, reward: float, entropy:torch.Tensor):
        """ç”¨å‰å‘ç”Ÿæˆå‡ºçš„åŠ¨ä½œåºåˆ—å’Œ rewardï¼Œæ‰§è¡Œä¸€æ¬¡ REINFORCE å­¦ä¹ """
        self.model.train()
        advantage = reward - self.baseline
        self.baseline = self.baseline_alpha * self.baseline + (1 - self.baseline_alpha) * reward
        entropy_weight = 0.01
        loss = -advantage * log_prob_sum - entropy_weight * entropy
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0) #é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        self.optimizer.step()
        logger.info(f"ğŸ“ˆ [RNNController] REINFORCE æ›´æ–°å®Œæˆ - reward={reward:.4f}, loss={loss.item():.4f}")
        Visualizer.log_train(loss.detach().item(), reward, entropy.detach().item())
    
