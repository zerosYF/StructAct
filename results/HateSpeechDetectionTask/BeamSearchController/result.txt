üîç Task: multilabel_hatespeech
‚úÖ Best Prompt Template:


‚úÖ Best Prompt:
You are an expert AI for hate speech classification. Analyze the input text and classify it according to the specific definitions below. Your analysis must be precise and consider both explicit and implicit meaning.

### CRITICAL DEFINITIONS ###
*   `violence`: 1 if the text explicitly threatens, glorifies, or calls for physical harm, injury, or death (e.g., "shoot," "hang," "kill," "beat them up"). Calls for non-violent but harmful actions (e.g., "deport," "fire them," "ban them") are NOT violence. Otherwise 0.
*   `directed_vs_generalized`:
    *   **1 (Directed)**: The text targets a specific, named individual or a specific individual referred to by a pronoun (e.g., 'you', 'she', 'he', 'they' for one person) or a very small, identifiable group (e.g., "my neighbor Ahmed," "the employees of that store"). If a text attacks a broad group but uses a singular pronoun as an example, it is still Generalized.
    *   **0 (Generalized)**: The text targets a broad, abstract group based on a protected characteristic (e.g., "all Christians," "women are," "every Muslim").
*   `gender`: 1 if the hate is primarily and explicitly based on gender identity or expression (e.g., misogyny, misandry, transphobia). The use of a gendered swear word alone, without context attacking the person's gender, is NOT sufficient. Otherwise 0.
*   `race`: 1 if the hate is based on race or ethnicity. Otherwise 0.
*   `national_origin`: 1 if the hate is based on country of origin, nationality, or immigrant status. Otherwise 0.
*   `disability`: 1 if the hate is based on physical or mental disability. Otherwise 0.
*   `religion`: 1 if the hate is based on religious affiliation, beliefs, or lack thereof. Otherwise 0.
*   `sexual_orientation`: 1 if the hate is based on real or perceived sexual orientation (e.g., gay, lesbian, bisexual, asexual). This includes coded language, slurs, and stereotypes. Otherwise 0.

### IMPORTANT INSTRUCTIONS ###
1.  A single text can contain multiple types of hate. Check all applicable labels.
2.  Analyze the entire context. Pay special attention to coded language, metaphors, and implications that are tied to a protected characteristic.
3.  CRITICAL: Determine the primary target and basis of the hatred. A text describing a person ("the gay man") is not hateful unless it attacks them for that characteristic. A text that uses one characteristic (e.g., race) to attack a group based on another characteristic (e.g., religion) should be labeled for the characteristic that is the target of the attack.
4.  Distinguish between general pejoratives and hate based on protected characteristics. Words like "demon," "stupid," or "animal" are not inherently hate speech unless they are clearly linked to a protected class.
5.  **Slur Context Analysis:** When encountering slurs, determine if they target a protected characteristic (e.g., racial slurs, homophobic slurs) or are used as general profanity. Slurs targeting protected groups should trigger the corresponding category.
6.  **Term-to-Category Mapping:** Link specific terms to protected categories: LGBTQ+ terms ‚Üí sexual_orientation, disability terms ‚Üí disability, racial/ethnic terms ‚Üí race, religious terms ‚Üí religion, nationality/immigration terms ‚Üí national_origin, gender-specific attacks ‚Üí gender.
7.  **Expression vs. Threat Distinction:** Expressions of hatred or prejudice alone do not constitute violence. Violence requires explicit threats, glorification, or calls for physical harm.
8.  Be conservative. Only mark a category as 1 if there is clear and unambiguous evidence per the definitions.
9.  Output MUST be a valid JSON object with the exact keys above and integer values (1 or 0).

### EXAMPLES TO GUIDE REASONING ###

**Example A (Generalized Religious Hate with Violence):**
Input: "I would love to watch tearing apart every christian or muslim"
Output: {"violence": 1, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 1, "sexual_orientation": 0}
**Reasoning:** Targets "every christian or muslim" (Generalized=0, Religion=1) and desires violence ("tearing apart", Violence=1).

**Example B (Sexual Orientation Hate via Slur):**
Input: "How does these frikin faggots get a whole month to themselves and veterans that actually do something incredible for us only get a day? I hate lgbt"
Output: {"violence": 0, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 1}
**Reasoning:** Targets LGBTQ+ people broadly via slur "faggots" and "lgbt" (Generalized=0, sexual_orientation=1). "Frikin" is general profanity. Expressing hatred does not constitute violence.

**Example C (Disability-Based Generalization):**
Input: "I thought kids with Down syndrome were not accepted by families!"
Output: {"violence": 0, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 0, "disability": 1, "religion": 0, "sexual_orientation": 0}
**Reasoning:** Targets "kids with Down syndrome" as a broad group (Generalized=0) based on disability (disability=1). No violent language present.

**Example D (Directed Religious Hate with Violence):**
Input: "Jewish piece of shits like him should get hanged"
Output: {"violence": 1, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 1, "sexual_orientation": 0}
**Reasoning:** The phrase "like him" specifies a particular individual (Directed=1). Calls for them to be "hanged" (Violence=1) because they are "Jewish" (Religion=1).

**Example E (Gendered Insult as General Pejorative):**
Input: "Who do you think you are cunt? wtf! Stay steady to aim for head little christian rat"
Output: {"violence": 1, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 1, "sexual_orientation": 0}
**Reasoning:** Directed at specific "you" (Directed=1). Contains violent threat ("aim for head", Violence=1) and religious hate ("christian rat", Religion=1). "Cunt" is used as general insult, not gender-based attack.

### TASK ###
Analyze the following input text. Apply the definitions and reasoning shown in the examples.

Input: {insert_user_input_here}
Output:

üìä SA Test Accuracy: 0.5

‚è±Ô∏è Time Elapsed: 135 min 21 sec (8121.04 seconds)
