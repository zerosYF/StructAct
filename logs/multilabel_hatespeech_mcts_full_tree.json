{
  "config": {
    "mcts_iters": 10,
    "rollout_length": 4,
    "depth_threshold": 5,
    "width_threshold": 3
  },
  "search_stats": {
    "total_nodes": 31,
    "total_Q_values": 31
  },
  "best_node": {
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "depth": 0,
    "Q": 3.0000000000000018,
    "N": 30
  },
  "search_tree": {
    "id": 0,
    "depth": 0,
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "Q": 3.0000000000000018,
    "N": 30,
    "reward": 0.1,
    "children": [
      {
        "id": 1,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a desire, intent, or threat of physical harm, injury, or death.\n*   **directed_vs_generalized (1/0):** 1 if the hate is directed at a specific individual or small, defined group. 0 if it is a generalized statement against a large, protected group (e.g., \"all women\").\n*   **gender (1/0):** 1 only if the hate is primarily based on the target's gender. A personal attack using a gendered slur does not automatically qualify; the target's gender must be the core reason for the attack.\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian, Brown).\n*   **national_origin (1/0):** 1 if the hate is based on the target's country of origin, nationality, or citizenship (e.g., Mexican, refugee, immigrant). This is distinct from race.\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice.\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation.\n\n**Critical Guidelines:**\n- Focus on the primary motivation of the hatred. Distinguish between a personal insult and group-based hate speech.\n- A text can have multiple labels. Analyze the author's intent.\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above.\n\n**Text to Classify:**\n\"{input_text}\"",
        "Q": 1.0,
        "N": 10,
        "reward": 0.1,
        "children": [
          {
            "id": 2,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a desire, intent, threat, or call for physical harm, injury, death, or eradication. This includes both explicit and implicit language.\n*   **directed_vs_generalized (1/0):** 1 **only** if the hate is directed at a **specific, named individual** or a very small, named group (e.g., \"my coworkers John and Steve\"). **0** if the target is any large group defined by a protected characteristic (e.g., \"all women,\" \"black people,\" \"Muslims,\" \"gay people,\" \"Indian subs\"). \"All [group]\" is always generalized (0).\n*   **gender (1/0):** 1 **only** if the hate is explicitly and primarily based on the target's gender identity (e.g., \"women are incompetent\"). A personal attack using a gendered slur (e.g., \"bitch\") does **not** qualify unless the target's gender is the core reason for the attack.\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). Use this for broad ethnic groups.\n*   **national_origin (1/0):** 1 **only** if the hate is explicitly based on the target's country of origin, nationality, or citizenship (e.g., \"Go back to Mexico!\"). If a term like \"Indian\" could be either, prefer `race`. Do not use for general terms like \"immigrant\" unless a specific nationality is clearly implied.\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice (e.g., Muslim, Jewish, Christian).\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation (e.g., gay, lesbian, bisexual). This is distinct from gender.\n\n**Critical Guidelines:**\n1.  **Primary Motivation:** Focus on the author's intent and the primary reason for the hatred. Distinguish between a personal insult and group-based hate speech.\n2.  **`directed_vs_generalized` Rule:** \"All [group]\" or \"[group] people\" is always a generalized statement (0). \"That [group] person\" is not specific enough; it is still generalized (0). A target is only \"directed\" (1) if they are specifically named or uniquely identifiable.\n3.  **Multiple Labels:** A text can have multiple labels. Analyze each category independently.\n4.  **Slurs:** The presence of a slur does not automatically mean a label is `1`. The hatred must be based on the protected characteristic itself.\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above. Do not include any other text.\n\n**Text to Classify:**\n\"{input_text}\"",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 3,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a desire, intent, threat, or call for physical harm, injury, death, or eradication. This includes both explicit and implicit language.\n*   **directed_vs_generalized (1/0):** 1 if the hateful statement or call to action is aimed at a **specific, identifiable individual or a very small, explicitly named group**. This includes individuals referred to by name, a unique descriptor (e.g., \"my boss,\" \"that guy\"), or a clear pronoun reference. 0 if the target is a general, non-specific group defined by a protected characteristic (e.g., \"all women,\" \"black people,\" \"Muslims\"). \"All [group]\" is always generalized (0).\n*   **gender (1/0):** 1 **only** if the hate is explicitly and primarily based on the target's gender identity (e.g., \"women are incompetent\"). A personal attack using a gendered slur (e.g., \"bitch\") does **not** qualify unless the target's gender is the core reason for the attack.\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). Use this for broad ethnic groups.\n*   **national_origin (1/0):** 1 **only** if the hate is explicitly based on the target's country of origin, nationality, or citizenship (e.g., \"Go back to Mexico!\"). If a term like \"Indian\" could be either, prefer `race`. Do not use for general terms like \"immigrant\" unless a specific nationality is clearly implied.\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice (e.g., Muslim, Jewish, Christian).\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation (e.g., gay, lesbian, bisexual). This is distinct from gender.\n\n**Critical Guidelines:**\n1.  **Primary Motivation:** Focus on the author's intent and the primary reason for the hatred. Distinguish between a personal insult and group-based hate speech.\n2.  **Specific Target vs. Group-Based Hatred:** A statement can be **directed** at a specific person (`directed_vs_generalized=1`) even if the *reason* for the hatred is their membership in a large group (e.g., `religion=1`). The `directed_vs_generalized` label is about the *target's specificity*, not the *nature of the characteristic*.\n3.  **Multiple Labels:** A text can have multiple labels. Analyze each category independently.\n4.  **Slurs:** The presence of a slur does not automatically mean a label is `1`. The hatred must be based on the protected characteristic itself.\n\n**Examples:**\n*   Text: \"John and Steve are f****ts and should die.\"\n  *   `{\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}`\n*   Text: \"That Jewish guy in apartment 3B is a problem we need to solve.\"\n  *   `{\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}`\n*   Text: \"All immigrants are criminals and should be deported.\"\n  *   `{\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}`\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above. Do not include any other text.\n\n**Text to Classify:**\n\"{input_text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 4,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a desire, intent, threat, or call for physical harm, injury, death, or eradication. This includes both explicit and implicit language.\n*   **directed_vs_generalized (1/0):** 1 **only** if the hate is directed at a **specific, named individual** or a very small, named group (e.g., \"my coworkers John and Steve\"). **0** if the target is any large group defined by a protected characteristic (e.g., \"all women,\" \"black people,\" \"Muslims\"). \"All [group]\" is always generalized (0). \"That [group] person\" is not specific enough; it is still generalized (0).\n*   **gender (1/0):** 1 **only** if the hate is explicitly and primarily based on the target's gender (e.g., \"women are incompetent\", \"men are trash\") **and is not related to LGBTQ+ status**. A personal attack using a gendered slur (e.g., \"bitch\") does **not** qualify unless the target's gender is the core reason for the attack.\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). Use this for broad ethnic groups.\n*   **national_origin (1/0):** 1 **only** if the hate is explicitly based on the target's country of origin, nationality, or citizenship (e.g., \"Go back to Mexico!\"). If a term like \"Indian\" could be either, prefer `race`. Do not use for general terms like \"immigrant\" unless a specific nationality is clearly implied.\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice (e.g., Muslim, Jewish, Christian).\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation, gender identity, or LGBTQ+ status broadly (e.g., gay, lesbian, bisexual, transgender, non-binary, queer). This includes attacks on individuals or groups for not conforming to gender norms.\n\n**Critical Guidelines:**\n1.  **Primary Motivation:** Determine if the attack is fundamentally based on a protected characteristic. Ask: \"If the target were different in this characteristic, would the same insult apply?\"\n    *   **Yes -> Label = 0:** \"John is a stupid bitch.\" (The insult is personal; if John were Jane, the attacker might just say \"idiot\").\n    *   **No -> Label = 1:** \"Women are stupid bitches.\" (The attack is inherently tied to the group characteristic).\n2.  **`directed_vs_generalized` Rule:** A target is only \"directed\" (1) if they are specifically named or uniquely identifiable (e.g., \"the CEO of Company X\").\n3.  **Multiple Labels:** A text can have multiple labels. Analyze each category independently. For composite groups (e.g., \"LGBTQ+\", \"Jews\"), assign a 1 to every category that is a core part of the targeted identity.\n4.  **Slurs:** The presence of a slur does not automatically mean a label is `1`. The primary hatred must be based on the protected characteristic itself, not just using a slur as a generic insult.\n\n**Illustrative Examples:**\n*   Text: \"I hate all women.\" -> `gender`: 1, `sexual_orientation`: 0\n*   Text: \"Trans people are disgusting.\" -> `sexual_orientation`: 1\n*   Text: \"That gay couple is corrupting our kids.\" -> `sexual_orientation`: 1\n*   Text: \"You're a stupid bitch.\" -> `gender`: 0 (It's a gendered insult, but the primary motivation is personal animus, not hatred of women as a group).\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above. Do not include any other text.\n\n**Text to Classify:**\n\"{input_text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a genuine desire, intent, threat, or call for physical harm, injury, death, or eradication. This includes both explicit and implicit language. *Note: A vulgar, non-literal sexual insult (e.g., \"suck my dick\") is not a threat of violence unless accompanied by language indicating a genuine intent to force physical harm.*\n*   **directed_vs_generalized (1/0):** 1 if the hate is directed at a **specific, named individual** OR a uniquely identifiable individual (e.g., through pronouns like \"you\" or \"this [group] person\" in a direct reply). 0 if the target is any large group defined by a protected characteristic (e.g., \"all women,\" \"black people,\" \"Muslims,\" \"gay people\"). \"All [group]\" is always generalized (0).\n*   **gender (1/0):** 1 **only** if the hate is explicitly and primarily based on the target's gender identity or characteristics (e.g., \"women are incompetent\"). A personal attack using a gendered slur (e.g., \"bitch\") does **not** qualify unless the target's gender is the core reason for the attack.\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). Use this for broad ethnic groups.\n*   **national_origin (1/0):** 1 **only** if the hate is explicitly based on the target's country of origin, nationality, or citizenship (e.g., \"Go back to Mexico!\"). If a term like \"Indian\" could be either, prefer `race`. Do not use for general terms like \"immigrant\" unless a specific nationality is clearly implicated by the context (e.g., \"Mexican immigrants\").\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice (e.g., Muslim, Jewish, Christian).\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation (e.g., gay, lesbian, bisexual). This is distinct from gender.\n\n**Critical Guidelines:**\n1.  **Primary Motivation:** Focus on the author's intent and the primary reason for the hatred. Distinguish between a personal insult and group-based hate speech.\n2.  **`directed_vs_generalized` Rule:** \"All [group]\" or \"[group] people\" is always a generalized statement (0). A target is \"directed\" (1) if they are specifically named or uniquely identifiable through context (e.g., \"you,\" \"this guy\").\n3.  **Multiple Labels:** A text can have multiple labels. Analyze each category independently.\n4.  **Slurs:** The presence of a slur does not automatically mean a label is `1`. The hatred must be based on the protected characteristic itself.\n5.  **Mutual Exclusivity:** An attack on \"lesbians,\" \"gay men,\" etc., is based on `sexual_orientation`, not `gender`. Only use the `gender` label if the attack is explicitly about gender roles, identity, or characteristics, independent of orientation.\n6.  **Context Clues:** Use pronouns and other context to determine if a target is directed. Text using \"you,\" \"this [group],\" or \"him/her\" in a likely personal exchange should be considered directed (1).\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above. Do not include any other text.\n\n**Text to Classify:**\n\"{input_text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 6,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a clear desire, intent, or threat of physical harm, injury, or death.\n*   **directed_vs_generalized (1/0):** 1 **only if the hate is aimed at a specific, named individual or a very small, named group (e.g., \"my neighbor John,\" \"the Smith family\")**. 0 if the hate is against a **large, protected group based on innate characteristics (e.g., all women, Black people, Mexicans, Muslims, gay people, people with disabilities)**. **Statements against large groups are the default (0).**\n*   **gender (1/0):** 1 only if the hate is **explicitly and primarily** based on the target's gender or gender identity. **A personal attack using a gendered slur does not automatically qualify; the target's gender must be the core reason for the attack.**\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). **This is about physical characteristics and heritage.**\n*   **national_origin (1/0):** 1 if the hate is based on the target's country of origin, nationality, or citizenship (e.g., Mexican, Indian, immigrant, refugee). **This is distinct from race (e.g., \"Mexican\" is national origin, \"Brown\" is race).**\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice.\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation.\n\n**Critical Guidelines:**\n- **CRITICAL: For `directed_vs_generalized`, assume the statement is generalized (0) unless a specific individual or tiny, named group is explicitly identified. Large groups (e.g., \"white people,\" \"Indians,\" \"Muslims\") are always generalized.**\n- Focus on the **primary motivation** of the hatred. A text may contain multiple elements, but you must identify the single most core reason for the attack. **If multiple protected characteristics are present, choose the one that is most central to the author's intent.**\n- A text can have multiple labels. Analyze the author's intent.\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above.\n\n**Text to Classify:**\n\"{input_text}\"",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a clear desire, intent, or threat of physical harm, injury, or death.\n*   **directed_vs_generalized (1/0):** 1 **if the hate is aimed at a specific, identifiable individual or a very small, named group.** This includes:\n    *   A specific, named person (e.g., \"my neighbor John\").\n    *   A very small, named group (e.g., \"the Smith family\").\n    *   **A specific individual identified by a descriptor, even if not named (e.g., \"that Norwegian\", \"this gay guy\", \"the Mexican cashier\")**. **The key is that the attacker is targeting one person they are referring to.**\n    *   0 if the hate is against a large, protected group (e.g., \"all Norwegians,\" \"Black people,\" \"Muslims\"). **Statements against large groups are the default (0).**\n*   **gender (1/0):** 1 only if the hate is **explicitly and primarily** based on the target's gender or gender identity. **The use of a gendered slur (e.g., \"bitch,\" \"shemale\") is NOT sufficient for this label unless the text makes it clear that gender is the fundamental reason for the attack.**\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian, Brown).\n*   **national_origin (1/0):** 1 if the hate is based on the target's country of origin, nationality, or citizenship (e.g., Mexican, Indian, immigrant, refugee). **If a term like \"Mexican\" is used, prefer `national_origin` over `race` unless the text explicitly mentions physical characteristics.**\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice.\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation (e.g., gay, lesbian, bisexual). **Slurs like \"f****t\" or \"ohomo\" are strong indicators for this label.**\n\n**Critical Guidelines:**\n- **CRITICAL: Analyze the author's core intent. The presence of a slur does not automatically determine the primary motivation. Look at the context and the target of the attack.**\n- **For `directed_vs_generalized`: If the text refers to \"this [group member]\" or \"that [group member],\" it is likely directed (1) because it points to a specific individual.**\n- **A text can have multiple hate speech labels (e.g., `race` and `national_origin` can both be 1). However, `directed_vs_generalized` is an independent modifier describing the target's specificity.**\n- **Tie-breaker: If a text could fit multiple protected characteristics (e.g., `gender` vs. `sexual_orientation`), prioritize the characteristic that is most clearly and directly insulted by the author's chosen words.**\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above. Do not include any other text.\n\n**Text to Classify:**\n\"{input_text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 8,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a clear desire, intent, or threat of physical harm, injury, or death.\n*   **directed_vs_generalized (1/0):** 1 **only if the hate is aimed at a specific, identifiable individual (e.g., using a name, a unique username, or a specific description like 'my neighbor') or a very small, explicitly named group (e.g., 'the Smith family'). A pronoun like 'her' or 'them' referring to a specific person counts as directed (1).** 0 if the hate is against a **large, protected group based on innate characteristics (e.g., all women, Black people, Mexicans, Muslims, gay people, people with disabilities)**. **Statements against large groups are the default (0).**\n*   **gender (1/0):** 1 only if the hate is **explicitly and primarily** based on the target's gender or gender identity. **A personal attack using a gendered slur does not automatically qualify; the target's gender must be the core reason for the attack. Attacks on groups defined by sexual orientation (e.g., 'gay people', 'lesbians') are primarily about sexual_orientation, not gender.**\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). **This is about physical characteristics and heritage.**\n*   **national_origin (1/0):** 1 if the hate is based on the target's country of origin, nationality, or citizenship (e.g., Mexican, Indian, immigrant, refugee). **This is distinct from race (e.g., \"Mexican\" is national origin if about nationality/citizenship, but \"Brown\" is race).**\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, practice, **or symbols (e.g., hijabs, crucifixes, kippahs).**\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation.\n\n**Critical Guidelines:**\n- **CRITICAL: For `directed_vs_generalized`, assume the statement is generalized (0) unless a specific, identifiable individual or tiny, named group is explicitly identified. Large groups (e.g., \"white people,\" \"Indians,\" \"Muslims\") are always generalized.**\n- **CRITICAL: Focus on the PRIMARY motivation of the hatred. Analyze the author's stated intent. If the text mentions a religious symbol (e.g., a hijab), the primary motivation is likely religion. If the text attacks a group defined by their sexual orientation, the primary motivation is sexual_orientation.**\n- A text can have multiple labels. Analyze the author's intent.\n- **If a protected characteristic is not the primary reason for the hate, mark it as 0.**\n- **Do not infer characteristics not explicitly mentioned or strongly implied by the text.**\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above.\n\n**Text to Classify:**\n\"{input_text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 9,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Analyze the text to first determine if it contains hate speech. If it does not, set all labels to 0. If it does, classify it based on the following strict definitions.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 if the text expresses a clear desire, intent, or threat of physical harm, injury, or death against a person or group.\n*   **directed_vs_generalized (1/0):** 1 **only if the hate is aimed at a specific, identifiable individual or a very small, named group.** This includes:\n    *   A specific person named directly (e.g., \"John Smith\") or by unambiguous reference (e.g., \"my neighbor\").\n    *   A direct address using second-person pronouns (\"you\", \"your\").\n    *   A very small, named group (e.g., \"the Smith family\").\n    **Otherwise, 0.** This is the default for attacks against large, protected groups (e.g., \"all women,\" \"Black people,\" \"Muslims\").\n*   **gender (1/0):** 1 only if the hate is **explicitly and primarily** based on the target's gender or gender identity. **A personal attack using a gendered slur does not automatically qualify; the target's gender must be the core reason for the attack.**\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). **This is about physical characteristics and heritage.**\n*   **national_origin (1/0):** 1 if the hate is based on the target's country of origin, nationality, or citizenship (e.g., Mexican, Indian, immigrant, refugee). **This is distinct from race (e.g., \"Mexican\" is national origin, \"Brown\" is race).**\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice.\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation.\n\n**Critical Classification Guidelines:**\n1.  **Hate Speech vs. Criticism:** Distinguish between hate speech (attacking people for innate characteristics) and strong criticism of ideas, actions, or institutions. Criticizing the ideology of a group is not necessarily hate speech against that group.\n2.  **Primary Motivation:** If multiple protected characteristics are referenced, identify the single most core reason for the attack. Ask: \"If I could only give one reason for this hatred, what would it be?\"\n3.  **A text can have multiple labels from the set [gender, race, national_origin, disability, religion, sexual_orientation].** The `directed_vs_generalized` and `violence` labels are independent.\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above.\n\n**Text to Classify:**\n\"{input_text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 10,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderator. Classify the text into multiple hate speech labels based on the following strict, literal definitions. Your analysis must be based solely on the author's intent and the primary reason for the hatred.\n\n**Label Definitions:**\n*   **violence (1/0):** 1 **only** if the text expresses a clear desire, intent, or threat of *physical* harm, injury, or death. Do not classify non-physical threats (e.g., sexual threats, verbal abuse) as violence.\n*   **directed_vs_generalized (1/0):** 1 if the hate is directed at a specific individual or a very small, named group (e.g., \"the people in apartment 3B\"). 0 if it is a generalized statement against a large, protected group (e.g., \"all women,\" \"black people,\" \"immigrants,\" \"lesbians\").\n*   **gender (1/0):** 1 **only** if the hate is explicitly and primarily based on the target's gender (e.g., \"women are stupid\"). A personal attack using a gendered slur (e.g., \"bitch\") does **not** qualify unless the target's gender is the fundamental reason for the attack.\n*   **race (1/0):** 1 if the hate is based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). **Prefer `national_origin` if a specific nationality or citizenship is mentioned.**\n*   **national_origin (1/0):** 1 if the hate is based on the target's country of origin, nationality, or citizenship (e.g., Mexican, Russian, Indian, immigrant). This is distinct from race.\n*   **disability (1/0):** 1 if the hate is based on physical or mental disability.\n*   **religion (1/0):** 1 if the hate is based on religious belief, identity, or practice (e.g., Muslim, Christian). **Note:** The term \"Jew\" is typically classified under religion, not race.\n*   **sexual_orientation (1/0):** 1 if the hate is based on sexual orientation (e.g., gay, lesbian, heterosexual). **Note:** This is distinct from gender identity.\n*   **gender_identity (1/0):** 1 if the hate is based on gender identity or expression (e.g., transgender, non-binary, or transphobic slurs like \"tranny\").\n\n**Critical Guidelines:**\n- **Focus on the primary motivation.** Distinguish between a personal insult and group-based hate speech.\n- **A text can have multiple labels.**\n- **Disambiguation Rule:** If a target fits multiple categories, choose the **most specific** label based on the wording. (e.g., \"Mexican\" -> `national_origin`; \"Brown people\" -> `race`).\n- **Absence of Evidence:** If there is no clear evidence for a label based on the strict definitions, output 0. Do not assume.\n\n**Output Format:**\nRespond with a valid JSON object only, using the exact keys above.\n\n**Text to Classify:**\n\"{input_text}\"",
            "Q": 0.10000000000000002,
            "N": 1,
            "reward": 0.1,
            "children": []
          }
        ]
      },
      {
        "id": 11,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict criteria. Output ONLY a valid JSON object with the specified keys, setting each to `1` only if the text explicitly meets the definition, otherwise `0`.\n\n### Category Definitions:\n- **violence**: The text must explicitly threaten, encourage, or express desire for physical harm. Do NOT mark for non-physical harm, offensive language, or general references to violent groups.\n- **directed_vs_generalized**: `1` for a targeted attack on a specific, identifiable individual. `0` for attacks against a generalized group, institution, or concept.\n- **gender**: Attack based on gender or gender identity (e.g., misogyny, transphobia).\n- **race**: Attack based on perceived racial background or physical characteristics (e.g., skin color). Note: Distinct from religion or national origin.\n- **national_origin**: Attack based on country of origin, citizenship, or ethnicity (e.g., xenophobia).\n- **disability**: Attack based on physical, mental, or developmental disabilities.\n- **religion**: Attack based on religious beliefs, affiliation, or lack thereof (e.g., Islamophobia, anti-Semitism). Note: Distinct from race or national origin.\n- **sexual_orientation**: Attack based on real or perceived sexual orientation (e.g., homophobia).\n\n### Critical Instructions:\n- **Precision is key:** Evaluate each category independently. A text can have multiple labels.\n- **Crucial Distinctions:** \n  - \"Jewish\" or \"Muslim\" is `religion`, not `race`.\n  - \"Indian\" or \"refugee\" is `national_origin`, not `race`.\n  - \"Black\" or \"white\" is `race`.\n- **Violence requires a direct physical threat.** Do not infer violence from degrading or hateful content alone.\n- **Directed means personal.** Criticism of a large group (e.g., \"all Hindus\") is generalized (`0`).\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText: {{Insert Text Here}}",
        "Q": 1.0,
        "N": 10,
        "reward": 0.1,
        "children": [
          {
            "id": 12,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict, hierarchical criteria. Output ONLY a valid JSON object with the specified keys. Set a key to `1` only if the text explicitly and unambiguously meets its definition; otherwise, set it to `0`.\n\n### Category Definitions & Hierarchy:\n1.  **violence**: The text must contain an explicit threat, encouragement, or desire for physical harm against a person or group. Do NOT mark for non-physical harm, offensive language, or general references to violence.\n2.  **directed_vs_generalized**: `1` **only** if the text targets a single, named individual or a very small, named group (e.g., \"John Doe,\" \"the Smith family\"). `0` for attacks against generalized groups (e.g., \"all lesbians\"), institutions, concepts, or large anonymous groups. **This flag depends on the specificity of the target, not just the presence of hate.**\n3.  **sexual_orientation**: Attack based on real or perceived sexual orientation (e.g., homophobia, lesbophobia). **This category takes precedence over `gender` for attacks on gay, lesbian, or bisexual individuals.**\n4.  **gender**: Attack explicitly and primarily based on gender or gender identity (e.g., misogyny, transphobia), and **not subsumed by `sexual_orientation`**.\n5.  **race**: Attack based on perceived racial background or immutable physical characteristics like skin color (e.g., \"Black,\" \"white\").\n6.  **national_origin**: Attack based on country of origin, citizenship, or ethnicity (e.g., \"Mexican,\" \"immigrants\").\n7.  **religion**: Attack based on religious beliefs, affiliation, or lack thereof (e.g., Islamophobia, anti-Semitism).\n8.  **disability**: Attack based on physical, mental, or developmental disabilities.\n\n### Critical Instructions:\n- **Adhere to the hierarchy.** If a text qualifies for both `sexual_orientation` and `gender`, mark only `sexual_orientation`.\n- **Default to `0` on ambiguity.** Classify based on the explicit, literal text. Do not infer meaning from misspellings, slang, or potential sarcasm.\n- **\"Attack\" requires malicious intent.** It is language that is degrading, dehumanizing, or intended to incite hatred against a protected category.\n- **Crucial Distinctions:**\n  - \"Jewish\" or \"Muslim\" is `religion`.\n  - \"Indian\" or \"refugee\" is `national_origin`.\n  - \"Black\" or \"white\" is `race`.\n- **For `directed_vs_generalized`:**\n  - The target must be named or directly identifiable (e.g., \"our mayor\").\n  - A call to \"shoot those companies\" is `0` (generalized institution).\n  - A call to \"shoot John Doe\" is `1` (specific individual).\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText: {{Insert Text Here}}",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 13,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict, hierarchical criteria. Output ONLY a valid JSON object with the specified keys. Set a key to `1` only if the text explicitly and unambiguously meets its definition; otherwise, set it to `0`.\n\n### Category Definitions & Hierarchy:\n1.  **violence**: The text must contain an explicit threat, encouragement, or stated desire for physical harm against a person or group. Do NOT mark for non-physical harm, offensive language, or general references to violence.\n2.  **directed_vs_generalized**: `1` **if the text targets a specific, identifiable individual or a very small, named group.** This includes a named person, a person identified by a unique pronoun (e.g., \"her,\" \"him\"), a specific role (e.g., \"our mayor\"), or a small, named group (e.g., \"the Smith family\"). `0` for attacks against large, generalized groups (e.g., \"all lesbians\"), institutions, concepts, or anonymous groups.\n3.  **sexual_orientation**: Attack based on real or perceived sexual orientation (e.g., homophobia, lesbophobia). **This category takes precedence over `gender` for attacks on gay, lesbian, or bisexual individuals.**\n4.  **gender**: Attack explicitly and primarily based on gender or gender identity (e.g., misogyny, transphobia), **and only if it is not subsumed by a higher-priority category like `sexual_orientation` or `religion`.**\n5.  **race**: Attack based on perceived racial background or immutable physical characteristics like skin color (e.g., \"Black,\" \"white\").\n6.  **national_origin**: Attack based on country of origin, citizenship, or ethnicity (e.g., \"Mexican,\" \"immigrants\").\n7.  **religion**: Attack based on religious beliefs, affiliation, or lack thereof (e.g., Islamophobia, anti-Semitism). **This category takes precedence over `gender` or `national_origin` if the attack is primarily motivated by religion.**\n8.  **disability**: Attack based on physical, mental, or developmental disabilities.\n\n### Critical Instructions & Conflict Resolution:\n-   **Adhere to the hierarchy and precedence rules above.**\n-   **Default to `0` on any ambiguity.** Classify based on the explicit, literal text. Do not infer meaning from misspellings, slang, or potential sarcasm.\n-   **An \"attack\" is language that is degrading, dehumanizing, or clearly hostile towards a protected category based on the literal text.**\n-   **Crucial Distinctions:**\n    -   \"Jewish\" or \"Muslim\" is `religion`.\n    -   \"Indian\" or \"refugee\" is `national_origin`.\n    -   \"Black\" or \"white\" is `race`.\n-   **For `directed_vs_generalized`:**\n    -   The target must be specific and identifiable. A singular pronoun like \"her\" in a context that implies a specific person qualifies as `1`.\n    -   A call to \"shoot those companies\" is `0` (generalized institution).\n    -   A call to \"shoot our mayor\" is `1` (specific, identifiable individual).\n\n```json\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"sexual_orientation\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"religion\": 0,\n  \"disability\": 0\n}\n```\n\nText: {{Insert Text Here}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 14,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict, hierarchical criteria. Output ONLY a valid JSON object with the specified keys. Set a key to `1` only if the text clearly meets its definition; otherwise, set it to `0`.\n\n### Category Definitions & Hierarchy:\n1.  **violence**: The text contains a clear threat, encouragement, or desire for physical harm against a person or group. Do NOT mark for non-physical harm or general references to violence.\n2.  **directed_vs_generalized**: `1` **only** if the text targets a specific, identifiable individual or a very small, named group. This includes a named person, a named small group, or a specific individual referred to by a pronoun like \"you\" in a direct address. `0` for attacks against large, generalized groups, institutions, concepts, or anonymous groups.\n3.  **sexual_orientation**: Attack based on real or perceived sexual orientation (e.g., homophobia, lesbophobia), including the use of homophobic slurs or stereotypes. **This category takes precedence over `gender` for attacks on gay, lesbian, or bisexual individuals.**\n4.  **gender**: Attack explicitly and primarily based on gender or gender identity (e.g., misogyny, transphobia), and **not subsumed by `sexual_orientation`**.\n5.  **race**: Attack based on perceived racial background or immutable physical characteristics like skin color (e.g., \"Black,\" \"white\").\n6.  **national_origin**: Attack based on country of origin, citizenship, or ethnicity (e.g., \"Mexican,\" \"immigrants\").\n7.  **religion**: Attack based on religious beliefs, affiliation, or lack thereof (e.g., Islamophobia, anti-Semitism).\n8.  **disability**: Attack based on physical, mental, or developmental disabilities.\n\n### Critical Analysis Instructions:\n-   **Adhere to the hierarchy.** If a text qualifies for both `sexual_orientation` and `gender`, mark only `sexual_orientation`.\n-   **\"Attack\" requires malicious intent.** It is language that is degrading, dehumanizing, derogatory, or intended to incite hatred against a protected category. This includes the use of well-known slurs.\n-   **Analyze Context and Intent:** Determine the primary target of the attack and the primary characteristic being attacked. The presence of a keyword (e.g., \"white\") does not automatically mean the attack is based on that category.\n-   **Crucial Distinctions:**\n    -   \"Jewish\" or \"Muslim\" is `religion`.\n    -   \"Indian\" or \"refugee\" is `national_origin`.\n    -   \"Black\" or \"white\" is `race`.\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText: {{Insert Text Here}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 15,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict, hierarchical criteria. Output ONLY and EXACTLY a valid JSON object with the specified keys. Set a key to `1` only if the text explicitly and unambiguously meets its definition; otherwise, set it to `0`.\n\n### Category Definitions & Hierarchy:\n1.  **violence**: The text must contain an explicit threat, encouragement, or desire for physical harm against a person or group. Do NOT mark for non-physical harm, offensive language, or general references to violence.\n2.  **directed_vs_generalized**: `1` **if the text targets a specific, identifiable individual or a very small, named group.** This includes:\n    -   A named person (e.g., \"John Doe\").\n    -   A directly addressed individual (e.g., using \"you\" in a threatening or hateful context).\n    -   A small, named group (e.g., \"the Smith family\").\n    `0` for attacks against generalized groups (e.g., \"all lesbians\"), institutions, concepts, or large anonymous groups. **This flag depends on the specificity of the target, not just the presence of hate.**\n3.  **sexual_orientation**: Attack based on real or perceived sexual orientation (e.g., homophobia, lesbophobia). **This category takes precedence over `gender` for attacks on gay, lesbian, or bisexual individuals.**\n4.  **gender**: Attack explicitly and primarily based on gender or gender identity (e.g., misogyny, transphobia), and **not subsumed by `sexual_orientation`**.\n5.  **race**: Attack based on perceived racial background or immutable physical characteristics like skin color (e.g., \"Black,\" \"white\"). **If a racial slur is used in the context of a national-origin attack, prioritize `national_origin`.**\n6.  **national_origin**: Attack based on country of origin, citizenship, or ethnicity (e.g., \"Mexican,\" \"immigrants,\" \"go back to X\"). **This takes precedence over `race` when the attack is clearly about origin or ethnicity.**\n7.  **religion**: Attack based on religious beliefs, affiliation, or lack thereof (e.g., Islamophobia, anti-Semitism).\n8.  **disability**: Attack based on physical, mental, or developmental disabilities.\n\n### Critical Instructions:\n-   **Adhere to the hierarchy.** If a text qualifies for multiple categories, use the following precedence order: **`sexual_orientation` > `gender`; `national_origin` > `race`.** For all other conflicts, mark all that apply but be strict with definitions.\n-   **Default to `0` on ambiguity.** Classify based on the explicit, literal text. Do not infer meaning from misspellings, slang, or potential sarcasm.\n-   **\"Attack\" requires malicious intent.** It is language that is degrading, dehumanizing, or intended to incite hatred against a protected category.\n-   **Crucial Distinctions (Context-Dependent):**\n    -   \"Jewish\" or \"Muslim\" is typically `religion`.\n    -   \"Indian\" or \"refugee\" is `national_origin`.\n    -   \"Black\" or \"white\" is typically `race`.\n    -   **However, consider the full context of the sentence. The phrase \"go back to [place]\" is a strong indicator of `national_origin`.**\n-   **For `directed_vs_generalized`:**\n    -   The target must be specific and identifiable (e.g., \"our mayor\", \"you\").\n    -   A call to \"shoot those companies\" is `0` (generalized institution).\n    -   A call to \"shoot John Doe\" is `1` (specific individual).\n    -   **A threat against \"you\" is `1` (directed).**\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText: {{Insert Text Here}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 16,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict criteria. Output ONLY a valid JSON object with the specified keys, setting each to `1` only if the text explicitly meets the definition, otherwise `0`.\n\n### Category Definitions:\n- **violence**: The text must explicitly and directly threaten, encourage, or express desire for physical harm (e.g., \"I will kill you,\" \"someone should shoot them\"). **Commands like \"kill yourself\" or \"die\" are direct threats of violence.** Do NOT mark for non-physical harm, offensive language, or general references to violent groups.\n- **directed_vs_generalized**: `1` for a targeted attack on a specific, identifiable individual or a small, defined group of individuals. **This includes attacks using second-person address (\"you\") or describing a specific but unnamed individual.** `0` for attacks against a large, generalized group, institution, or concept (e.g., \"all politicians,\" \"everyone from X country\").\n- **gender**: Attack based on gender or gender identity (e.g., misogyny, transphobia). **Includes derogatory use of gender-based slurs.**\n- **race**: Attack based on perceived racial background or physical characteristics (e.g., skin color). **Includes racial slurs and derogatory terms.** Note: Distinct from religion or national origin.\n- **national_origin**: Attack based on country of origin, citizenship, or ethnicity (e.g., xenophobia). **Includes pejorative use of terms like \"immigrant\" or \"foreigner\".** Note: Distinct from race.\n- **disability**: Attack based on physical, mental, or developmental disabilities. **Includes derogatory use of terms like \"retard\" or \"crazy,\" and mocking specific conditions.**\n- **religion**: Attack based on religious beliefs, affiliation, or lack thereof (e.g., Islamophobia, anti-Semitism). **Includes religious slurs and derogatory terms.** Note: Distinct from race or national origin.\n- **sexual_orientation**: Attack based on real or perceived sexual orientation (e.g., homophobia). **Includes derogatory use of slurs targeting sexual orientation.**\n\n### Critical Instructions:\n- **Precision is key:** Evaluate each category independently based on the definitions above. A text can have multiple labels. **Do not assume a label applies just because the text is hostile.**\n- **Crucial Distinctions:**\n  - \"Jewish\" or \"Muslim\" is `religion`, not `race`.\n  - \"Indian\" or \"refugee\" is `national_origin`, not `race`.\n  - \"Black\" or \"white\" is `race`.\n- **Violence requires a direct physical threat.** However, **\"kill yourself\" is always a direct threat of violence (`violence: 1`).**\n- **\"Directed\" means personal.** Criticism of a large group (e.g., \"all Hindus\") is generalized (`0`). **An attack on \"you\" or a described individual is directed (`1`).**\n- **Context is key:** Interpret words based on their likely derogatory meaning (e.g., \"sick\" as in mentally ill, not physically ill).\n\n```json\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n```\n\nText: {{Insert Text Here}}",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 17,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict criteria. Output ONLY a valid JSON object with the specified keys, setting each to `1` only if the text explicitly and directly meets the definition, otherwise `0`.\n\n### Category Definitions:\n- **violence**: The text must explicitly and directly threaten, encourage, or express desire for physical harm (e.g., \"I will kill you,\" \"someone should shoot them\"). **Commands like \"kill yourself\" or \"die\" are direct threats of violence and should be marked `1`.** Do NOT mark for non-physical harm, offensive language, or general references to violent groups.\n- **directed_vs_generalized**: `1` for a targeted attack on a specific, identifiable individual or a very small, named group (e.g., \"my neighbor John,\" \"the board of directors of Company X\"). **This includes attacks using second-person address (\"you\") where the 'you' is a specific person.** `0` for attacks against a large, generalized group, institution, or concept (e.g., \"all politicians,\" \"everyone from X country,\" \"any male,\" \"people like you\").\n- **gender**: Attack based on gender or gender identity (e.g., misogyny, transphobia). **Includes derogatory use of gender-based slurs when the attack is based on gender.**\n- **race**: Attack based on perceived racial background or physical characteristics (e.g., skin color). **Includes racial slurs and derogatory terms.** Note: Distinct from religion or national origin.\n- **national_origin**: Attack based on country of origin, citizenship, or ethnicity (e.g., xenophobia). **Includes pejorative use of terms like \"immigrant\" or \"foreigner\".** Note: Distinct from race.\n- **disability**: Attack based on physical, mental, or developmental disabilities. **Includes derogatory use of terms like \"retard\" or \"crazy,\" and mocking specific conditions.**\n- **religion**: Attack based on religious beliefs, affiliation, or lack thereof (e.g., Islamophobia, anti-Semitism). **This must be an attack on the person's religious identity, not just the use of religious language.** Includes religious slurs and derogatory terms. Note: Distinct from race or national origin.\n- **sexual_orientation**: Attack based on real or perceived sexual orientation (e.g., homophobia). **Includes derogatory use of slurs targeting sexual orientation.**\n\n### Critical Instructions:\n- **Precision is key:** Evaluate each category independently based on the definitions above. A text can have multiple labels. **Do not assume a label applies just because the text is hostile or uses offensive language.**\n- **Avoid keyword guessing:** The text must **explicitly and directly** meet the definition. If the link is only implied or metaphorical, set the value to `0`.\n- **Slur Context:** The presence of a slur does not automatically mean the corresponding category is `1`. You must determine if the slur is being used to attack the protected characteristic. For example:\n  - 'You're a stupid bitch' may be `gender: 1` if aimed at a woman, but `gender: 0` if aimed at a man (where it's a generic insult).\n  - 'That's retarded' is `disability: 1` if mocking a person, but `disability: 0` if criticizing an idea.\n- **Crucial Distinctions:**\n  - \"Jewish\" or \"Muslim\" is `religion`, not `race`.\n  - \"Indian\" or \"refugee\" is `national_origin`, not `race`.\n  - \"Black\" or \"white\" is `race`.\n- **Violence requires a direct physical threat.** However, **\"kill yourself\" is always a direct threat of violence (`violence: 1`).**\n- **\"Directed\" means personal and specific.** Criticism of a large group (e.g., \"all Hindus\") is generalized (`0`). An attack on \"you\" is directed (`1`) only if it targets a specific individual.\n- **Context is key:** Interpret words based on their likely derogatory meaning (e.g., \"sick\" as in mentally ill, not physically ill). Distinguish between attacks on identity and the use of metaphors or hyperbolic insults (e.g., \"demon\" as a general insult vs. an attack on religious belief).\n\n```json\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n```\n\nText: {{Insert Text Here}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 18,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict criteria. Output ONLY a valid JSON object with the specified keys, setting each to `1` only if the text explicitly meets the definition, otherwise `0`.\n\n### Category Definitions:\n- **violence**: The text must explicitly and directly threaten, encourage, or express desire for physical harm (e.g., \"I will kill you,\" \"someone should shoot them\"). **Commands like \"kill yourself\" or \"die\" are direct threats of violence and MUST be marked `1`.** Do NOT mark for non-physical harm, offensive language, or general references to violent groups.\n- **directed_vs_generalized**: `1` for a targeted attack on a specific, named OR identifiable individual (e.g., a politician, a user) or a very small, named group (e.g., \"my neighbors,\" \"the Smith family\"). **This includes attacks that use a second-person address (\"you\") to directly target the reader/listener.** `0` for attacks against a large, generalized group, institution, or concept (e.g., \"all politicians,\" \"everyone from X country,\" \"women\"). **An attack on a generalized group that uses a phrase like \"like him\" or \"such as [person]\" to target a specific individual SHOULD be considered directed (`1`). An attack using \"you\" is only directed (`1`) if it is a clear, direct address to the recipient, not a rhetorical question.**\n- **gender**: Attack based on gender or gender identity (e.g., misogyny, transphobia). **Includes derogatory use of gender-based slurs.**\n- **race**: Attack based on perceived racial background or physical characteristics (e.g., skin color). **Includes racial slurs and derogatory terms.** Note: Distinct from religion or national origin.\n- **national_origin**: Attack based on country of origin, citizenship, or ethnicity (e.g., xenophobia). **Includes pejorative use of terms like \"immigrant\" or \"foreigner\".** Note: Distinct from race.\n- **disability**: Attack based on physical, mental, or developmental disabilities. **Includes derogatory use of terms like \"retard\" or \"crazy,\" and mocking specific conditions.** **However, if a disability-related term is used as a pejorative to attack a group defined by another category (e.g., \"gays are mentally ill\"), the target category (`sexual_orientation`) should be marked `1` and `disability` should be marked `0` unless the text also directly attacks disabled people.**\n- **religion**: Attack based on religious beliefs, affiliation, or lack thereof (e.g., Islamophobia, anti-Semitism). **Includes religious slurs and derogatory terms.** Note: Distinct from race or national origin.\n- **sexual_orientation**: Attack based on real or perceived sexual orientation (e.g., homophobia, biphobia). **Includes derogatory use of slurs targeting sexual orientation.**\n\n### Critical Instructions:\n- **Precision is key:** Evaluate each category independently based on the definitions above. A text can have multiple labels. **Do not assume a label applies just because the text is hostile.**\n- **Identify the Primary Target:** When a text attacks a group using slurs from another category, the label should reflect the **target** of the attack, not just the words used. (See the `disability` rule above).\n- **Crucial Distinctions:**\n  - \"Jewish\" or \"Muslim\" is `religion`, not `race`.\n  - \"Indian\" or \"refugee\" is `national_origin`, not `race`.\n  - \"Black\" or \"white\" is `race`.\n- **Violence requires a direct physical threat.** However, **\"kill yourself\" is always a direct threat of violence (`violence: 1`).**\n- **Context is key:** Interpret words based on their likely derogatory meaning (e.g., \"sick\" as in mentally ill, not physically ill).\n\n```json\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n```\n\nText: {{Insert Text Here}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 19,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict criteria. Output ONLY a valid JSON object with the specified keys, setting each to `1` only if the text meets the definition, otherwise `0`.\n\n### Category Definitions:\n- **violence**: `1` if the text explicitly threatens, encourages, or expresses desire for **physical harm** (e.g., \"I will kill you,\" \"someone should shoot them\"). **Commands like \"kill yourself\" or \"die\" are always direct threats of violence (`1`).** `0` for non-physical harm, offensive language, or general references to violence.\n- **directed_vs_generalized**: `1` **only if the attack is targeted** at a specific, identifiable individual or a very small, defined group (e.g., using \"you,\" naming someone, describing a specific person). `0` for attacks against large, generalized groups, institutions, or concepts (e.g., \"all politicians,\" \"everyone from X country\").\n- **race**: `1` if the attack is based on **perceived racial background** (e.g., skin color). Includes racial slurs and derogatory terms. (Note: Distinct from religion or national origin).\n- **national_origin**: `1` if the attack is based on **country of origin, citizenship, or ethnicity** (e.g., xenophobia). Includes pejorative use of terms like \"immigrant\" or \"foreigner\". (Note: Distinct from race).\n- **religion**: `1` if the attack is based on **religious beliefs, affiliation, or lack thereof** (e.g., Islamophobia, anti-Semitism). Includes religious slurs. (Note: \"Jewish\" can be `religion` or `race`; classify based on context of the attack).\n- **gender**: `1` if the attack is based on **gender or gender identity** (e.g., misogyny, transphobia). Includes derogatory use of gender-based slurs and stereotypes.\n- **sexual_orientation**: `1` if the attack is based on **real or perceived sexual orientation** (e.g., homophobia, biphobia). Includes derogatory use of slurs and stereotypes.\n- **disability**: `1` if the attack is based on **physical, mental, or developmental disabilities**. Includes derogatory use of terms like \"retard\" or \"crazy,\" and mocking specific conditions.\n\n### **CRITICAL INSTRUCTIONS:**\n1.  **Precision and Context:** You must interpret words based on their likely derogatory, hateful meaning in context. An attack can be implicit and does not require a direct slur.\n2.  **Independent but Comprehensive Evaluation:** Evaluate each category according to its definition. A text can be hateful on multiple axes. **If an attack uses one category as the mechanism to attack another, set BOTH categories to `1`** (e.g., calling a religious group \"mentally ill\" sets both `religion` and `disability` to `1`).\n3.  **Identify the Primary Target:** When multiple categories apply, ensure the core characteristic being attacked is correctly identified. An attack on \"lesbians\" is primarily `sexual_orientation`, even if it contains gendered language.\n4.  **Key Distinctions:**\n    - \"Jewish\" attacking religious practices -> `religion`. Attacking perceived racial traits -> `race`.\n    - \"Indian\" -> `national_origin`. \"Black\" -> `race`.\n    - \"Kill yourself\" -> `violence: 1`.\n    - An attack on \"you\" -> `directed_vs_generalized: 1`.\n\n```json\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n```\n\nText: {{Insert Text Here}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 20,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it for hate speech using the following strict criteria. Output ONLY a valid JSON object with the specified keys, setting each to `1` only if the text explicitly meets the definition, otherwise `0`.\n\n### Category Definitions:\n- **violence**: `1` if the text explicitly threatens, encourages, or expresses a desire for physical harm against a person or group. This includes direct commands to self-harm (e.g., \"go kill yourself\"). Do NOT mark for non-physical harm, offensive language, general references to violent groups, or common idioms (e.g., \"save your heads\").\n- **directed_vs_generalized**: `1` ONLY for a targeted attack on a specific, named or otherwise uniquely identifiable individual (e.g., \"@username\"). `0` for attacks against a generalized group, institution, concept, or an unnamed/unidentifiable \"you\" (e.g., \"all women,\" \"you people\").\n- **gender**: `1` if the text attacks based primarily on gender or gender identity (e.g., misogyny, transphobia). Note: Gendered slurs used as generic insults do not qualify.\n- **race**: `1` if the text attacks based primarily on perceived racial background (e.g., \"Black,\" \"white,\" \"Asian\").\n- **national_origin**: `1` if the text attacks based primarily on country of origin, citizenship, or ethnicity (e.g., \"Indian,\" \"immigrants\").\n- **disability**: `1` if the text attacks based primarily on physical, mental, or developmental disabilities.\n- **religion**: `1` if the text attacks based primarily on religious beliefs or affiliation (e.g., \"Muslim,\" \"Jewish\").\n- **sexual_orientation**: `1` if the text attacks based primarily on real or perceived sexual orientation (e.g., \"gay,\" \"LGBTQ\").\n\n### Critical Instructions:\n- **Evaluate each category independently.** A text can have multiple labels.\n- **Context over keywords:** Analyze the primary intent. Do not trigger a category based solely on a word without context.\n- **Violence requires a direct, non-idiomatic physical threat.** Commands to cause harm qualify.\n- **Directed means personally identifiable.** The use of \"you\" is only `directed` if the target is clearly a specific person.\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText: {{Insert Text Here}}",
            "Q": 0.10000000000000002,
            "N": 1,
            "reward": 0.1,
            "children": []
          }
        ]
      },
      {
        "id": 21,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, or threat of *physical harm or injury* to a person or group. Do not mark for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\"). (0) otherwise.\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or implied individual*. (0) if it is directed at a *broad, generalized group* (e.g., \"all women,\" \"Indian people\").\n*   `gender`: (1) if the text attacks or demeans a person or group based on their gender, gender identity, or expression (e.g., misogyny, misandry, transphobia). (0) otherwise.\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race or ethnicity (e.g., Black, White, Asian). (0) otherwise.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., Indian, Norwegian, American). (0) otherwise. (Note: This is separate from `race`).\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs related to disability. (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., homophobia, biphobia). (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  Be precise. If the text does not clearly fit the definition, mark it as (0).\n3.  Analyze the *primary intent* of the text.\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
        "Q": 1.0,
        "N": 10,
        "reward": 0.1,
        "children": [
          {
            "id": 22,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, or threat of *physical harm or injury* to a person or group. Do not mark for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\"). (0) otherwise.\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or readily identifiable individual* (e.g., a famous person, a user with a username). (0) if it is directed at a *broad, generalized group* (e.g., \"all women\") or uses a singular pronoun (like \"she,\" \"he,\" \"they\") as a generic placeholder for a group archetype.\n*   `gender`: (1) if the text attacks or demeans a person or group *primarily* based on their gender, gender identity, or expression (e.g., misogyny, misandry, transphobia). (0) if the attack is based on another characteristic, even if the target is associated with a gender (e.g., \"lesbians\" is primarily `sexual_orientation`).\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race, ethnicity, or phenotypical features (e.g., Black, White, Asian). For country-derived terms (e.g., \"Indian\"), prefer `race` unless the context explicitly references nationality or immigration.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., \"Go back to your country,\" \"Norwegians are all smug\"). (0) for attacks based primarily on race/ethnicity.\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs related to disability. (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., homophobia, biphobia). (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  Be precise and conservative. If the text does not clearly and unambiguously fit the definition, mark it as (0).\n3.  Analyze the *primary intent* and the *primary characteristic* being attacked.\n4.  For `directed_vs_generalized`: Favor (0) when in doubt. A target must be clearly specific and identifiable to be marked as (1).\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"HOPE THEY KNOCK HER ASS TO THE GROUND AND RIP HER DRESS OFF...\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"this is why indian subs should only count 0,5\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"lesbians cant do math fucking mood lmao\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 23,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, or threat of *physical harm or injury* to a person or group. Do not mark for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\"). (0) otherwise.\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or readily identifiable individual* (e.g., a famous person, a user with a username, \"my boss,\" \"that senator\"). (0) if it is directed at a *broad, generalized group* (e.g., \"all women\") or uses a singular pronoun (like \"she,\" \"he,\" \"they\") as a generic placeholder for a group archetype.\n*   `gender`: (1) if the text attacks or demeans a person or group *primarily* based on their gender, gender identity, or expression (e.g., misogyny, misandry, transphobia). (0) if the attack is based on another characteristic, even if the target is associated with a gender (e.g., \"lesbians\" is primarily `sexual_orientation`).\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race, ethnicity, or phenotypical features (e.g., Black, White, Asian). For country-derived terms (e.g., \"Indian\"), prefer `race` unless the context explicitly references nationality or immigration.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., \"Go back to your country,\" \"Norwegians are all smug\"). (0) for attacks based primarily on race/ethnicity.\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs related to disability. (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., homophobia, biphobia). (0) otherwise.\n\n**CRITICAL CLARIFICATIONS:**\n*   **Metaphorical vs. Literal Language:** Interpret language in context. Do not mark `violence` for clearly idiomatic or hyperbolic language (e.g., \"I'll kill you in this game,\" \"save your heads\"). Reserve it for credible, literal threats or wishes for physical harm.\n*   **Religious Terminology:** The use of religious words like \"demon,\" \"devil,\" or \"hell\" as generic insults does not automatically qualify as `religion`. Mark `religion: 1` only if the text clearly attacks a person/group *for their religious beliefs or identity*.\n*   **Criticism vs. Identity Attack:** Distinguish between attacking a protected characteristic and criticizing behavior or ideology. For example, \"White supremacists are evil\" criticizes an ideology (`race: 0`), while \"White people are inferior\" attacks a race (`race: 1`).\n*   **Multiple Characteristics:** If a text attacks multiple characteristics (e.g., a gay trans person), mark ALL that are unambiguously present. Do not try to guess a \"primary\" characteristic unless one is clearly incidental.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously. Mark all that apply.\n2.  Be precise and conservative. If the text does not clearly and unambiguously fit the definition, mark it as (0).\n3.  Analyze the *primary characteristic* being attacked. If multiple characteristics are attacked, mark them all.\n4.  For `directed_vs_generalized`: A target is \"readily identifiable\" if a reasonable person in context would know who is being referred to. Favor (0) when in doubt.\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That demon woman is a terrible driver.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"Jewish piece of shits like him should get hanged.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\n- Input: \"We should deport that illegal Mexican f****t.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 24,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, or threat of *physical harm or injury* to a person or group. Do not mark for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\"). (0) otherwise.\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or readily identifiable person* (e.g., a famous person, a user with a username). (0) if it is directed at a *broad, generalized group* (e.g., \"all women\") or a named collective that is not a single person (e.g., a country, company, or subreddit).\n*   `gender`: (1) if the text attacks or demeans a person or group *primarily* based on their gender, gender identity, or expression (e.g., misogyny, misandry). (0) if the attack is based on another characteristic.\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race, ethnicity, or phenotypical features (e.g., Black, White, Asian). For country-derived terms (e.g., \"Indian\"), prefer `race` unless the context explicitly references nationality or immigration.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., \"Go back to your country,\" \"Norwegians are all smug\"). (0) for attacks based primarily on race/ethnicity.\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs related to disability. (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., homophobia, biphobia). This also includes broad slurs against transgender people that are used as homophobic epithets (e.g., \"tr***y\", \"transgenders\"). (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  Be precise and conservative. If the text does not clearly and unambiguously fit the definition, mark it as (0).\n3.  Analyze the *primary intent* and the *primary characteristic* being attacked. Do not classify based solely on keywords.\n4.  For `directed_vs_generalized`: A target must be a specific, identifiable person. Named collectives (countries, companies, subreddits) are (0). Favor (0) when in doubt.\n5.  For `gender` vs. `sexual_orientation`: Attacks on groups like gay men or lesbians are `sexual_orientation`. For attacks using terms like 'transgender', prefer `sexual_orientation` for broad slurs unless the text explicitly and primarily attacks the concept of gender identity itself.\n6.  For `race` vs. `national_origin`: For ambiguous terms (e.g., 'Indian', 'Swede'), if the context does not make the primary characteristic clear, default to `national_origin`.\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"HOPE THEY KNOCK HER ASS TO THE GROUND AND RIP HER DRESS OFF...\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"this is why indian subs should only count 0,5\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"lesbians cant do math fucking mood lmao\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"Hey Sweden do you remember back in time when your men were vikings...\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"EEEWWWWWWW TRANSGENDERS ARE FUCKING DISGUSTING\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 25,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, or threat of *physical harm or injury* to a person or group. This includes direct threats (\"I will kill you\"), veiled threats (\"You're going to get hurt\"), and wishes for physical harm (\"I hope someone breaks their legs\"). Do not mark for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\"). (0) otherwise.\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or readily identifiable individual* (e.g., a famous person, a user with a username, a person referred to by a unique identifier in a specific context). (0) if it is directed at a *broad, generalized group* (e.g., \"all women\") or uses a singular pronoun (like \"she,\" \"he,\" \"they\") as a generic placeholder for a group archetype.\n*   `gender`: (1) if the text attacks or demeans a person or group *primarily* based on their gender, gender identity, or expression (e.g., misogyny, misandry, transphobia). (0) if the attack is based on another characteristic, even if the target is associated with a gender (e.g., \"lesbians\" is primarily `sexual_orientation`).\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race, ethnicity, or phenotypical features (e.g., Black, White, Asian). For country-derived terms (e.g., \"Indian\"), prefer `race` unless the context explicitly references nationality or immigration.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., \"Go back to your country,\" \"Norwegians are all smug\"). (0) for attacks based primarily on race/ethnicity.\n*   `disability`: (1) if the text attacks or demeans a person or group **primarily** based on physical, mental, or developmental disability. This includes using slurs related to disability **when the disability itself is the focus of the attack**. (0) if a disability-related term is used as a general pejorative to attack a group based on a different primary characteristic (e.g., \"LGBTQ people are mentally ill\" attacks `sexual_orientation`, not `disability`).\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., homophobia, biphobia). (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  Be precise and conservative. If the text does not clearly and unambiguously fit the definition, mark it as (0). **For `violence`, consider the reasonable interpretation of the language used.**\n3.  Analyze the *primary intent* and the *primary characteristic* being attacked. **If an attack uses a characteristic from one category (e.g., `disability`: \"mentally ill\") to target a group defined by another category (e.g., `sexual_orientation`: \"gay people\"), classify based on the *target group*, not the mechanism of attack.**\n4.  For `directed_vs_generalized`: Favor (0) when in doubt. A target must be clearly specific and identifiable to be marked as (1).\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n- Input: \"You should be smacked upside the head.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n- Input: \"HOPE THEY KNOCK HER ASS TO THE GROUND AND RIP HER DRESS OFF...\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n- Input: \"this is why indian subs should only count 0,5\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n- Input: \"lesbians cant do math fucking mood lmao\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n- Input: \"LGBTQ+ are mentally ill. Period.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 26,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, threat, or implicit call to action for *physical harm or injury* to a person or group. This includes vague but threatening commands like \"stop them\" or \"deal with them\" when made in an aggressive, dehumanizing context. (0) for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\").\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or clearly implied individual* (e.g., \"that woman,\" \"my neighbor John\"). (0) if it is directed at a *broad, generalized group* (e.g., \"all women,\" \"Indian people,\" \"transgenders\").\n*   `gender`: (1) if the text attacks or demeans a person or group based on their sex (e.g., male/female) or gender-specific traits. This includes misogyny and misandry. (0) otherwise. (Note: For attacks on gender identity (e.g., transphobia) use `sexual_orientation`).\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race or ethnicity (e.g., Black, White, Asian). (0) otherwise.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., Indian, Norwegian, American). (0) otherwise. (Note: This is separate from `race`).\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs related to disability. (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., gay, lesbian, bisexual) OR gender identity (e.g., transgender, non-binary). This includes homophobia, biphobia, and transphobia. (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  Be precise. If the text does not clearly and unambiguously fit the definition, mark it as (0).\n3.  Analyze the *primary intent* of the text. The key question is: \"Is the primary intent to degrade, intimidate, or incite hatred against a protected group, or is it a personal insult between individuals?\" Stereotypes used in a personal argument may not qualify if the overall intent is not to attack the entire group.\n4.  For `directed_vs_generalized`, if the target is a generalized group (even if phrased with a plural like \"they\" or \"them\"), mark it as (0).\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"TRANSGENDERS ARE DISGUSTING.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"We need to stop these invaders by any means necessary!\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 27,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, threat, or implicit call to action for *physical harm or injury* to a person or group. This includes vague but threatening commands like \"stop them\" or \"deal with them\" when made in an aggressive, dehumanizing context. (0) for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\").\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or uniquely identifiable individual* (e.g., \"John Doe,\" \"my neighbor John,\" \"that woman over there\"). (0) if it is directed at a *broad, generalized group* or a non-specific member of a group. Look for keywords like 'all', 'every', 'any', 'they'/'them' referring to a group, or plural nouns (e.g., \"all women,\" \"Indian people,\" \"any male bitch\").\n*   `gender`: (1) if the text attacks or demeans a person or group based on their biological sex (e.g., male/female) or sex-specific traits. This includes misogyny and misandry. (0) otherwise. (Note: For attacks on gender identity (e.g., transphobia) use `gender_identity`).\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race or ethnicity (e.g., Black, White, Asian). (0) otherwise.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., Indian, Norwegian, American). (0) otherwise. (Note: This is separate from `race`).\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs or derogatory language related to disability (e.g., \"retard,\" \"psycho\") even as a general insult, and references to \"mental illness\" or \"being disabled\" in a demeaning context. (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., gay, lesbian, bisexual). This includes homophobia and biphobia. (0) otherwise.\n*   `gender_identity`: (1) if the text attacks or demeans a person or group based their gender identity or expression (e.g., transgender, non-binary). This includes transphobia and misgendering with malicious intent. (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  Be precise. If the text does not clearly and unambiguously fit the definition, mark it as (0).\n3.  Analyze the *primary intent* of the text. The key question is: \"Is the primary intent to degrade, intimidate, or incite hatred against a protected group, or is it a personal insult between individuals?\" Stereotypes used in a personal argument may not qualify if the overall intent is not to attack the entire group.\n4.  For `directed_vs_generalized`, if the target is a generalized group (even if phrased with a plural like \"they\" or \"them\" or a singular like \"a\" or \"any\"), mark it as (0).\n5.  Analyze the meaning and intent, not just keywords. A text can be offensive without being hate speech under these specific definitions.\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0, \"gender_identity\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0, \"gender_identity\": 0}\n\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1, \"gender_identity\": 0}\n\n- Input: \"TRANSGENDERS ARE DISGUSTING.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0, \"gender_identity\": 1}\n\n- Input: \"We need to stop these invaders by any means necessary!\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0, \"gender_identity\": 0}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 28,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, threat, or implicit call to action for *physical harm or injury* to a person or group. This includes vague but threatening commands like \"stop them\" or \"deal with them\" when made in an aggressive, dehumanizing context, as well as calls for boycotts or other actions conditional on the cessation of violence. (0) for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\").\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or clearly implied individual*. This is often signaled by words like \"this,\" \"that,\" \"you\" (referring to a specific person), or other demonstratives that single out an individual, even if unnamed (e.g., \"that woman,\" \"my neighbor John,\" \"this Norwegian\"). (0) if it is directed at a *broad, generalized group*, even if phrased with a plural like \"they,\" \"them,\" or \"all [group]\".\n*   `gender`: (1) if the text attacks or demeans a person or group based on their sex (e.g., male/female) or gender-specific traits. This includes misogyny and misandry. (0) otherwise. (Note: For attacks on gender identity (e.g., transphobia) use `sexual_orientation`).\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race or ethnicity (e.g., Black, White, Asian). (0) otherwise.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., Indian, Norwegian, American). (0) otherwise. (Note: This is separate from `race`).\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs related to disability. (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., gay, lesbian, bisexual) OR gender identity (e.g., transgender, non-binary). This includes homophobia, biphobia, and transphobia. (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  Be precise. If the text does not clearly and unambiguously fit the definition, mark it as (0).\n3.  Analyze the *primary intent* of the text. To determine this, ask: \"What is the fundamental reason for the attack?\" or \"What characteristic is the insult *because of*?\" Do not classify based on incidental characteristics of the target. The key question is: \"Is the primary intent to degrade, intimidate, or incite hatred against a protected group, or is it a personal insult between individuals?\" Stereotypes used in a personal argument may not qualify if the overall intent is not to attack the entire group.\n4.  Disentangling Race, National Origin, and Religion: Focus on the actual characteristic being attacked. If a text attacks \"Indians,\" it is `national_origin`. If it attacks \"Hindus,\" it is `religion`. If it attacks \"brown people,\" it is `race`. Base your classification on the actual group being targeted, not on misapplied labels used by the author (e.g., \"racist Hindus\" is an attack on religion, not race).\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"TRANSGENDERS ARE DISGUSTING.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"We need to stop these invaders by any means necessary!\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"Boycott all businesses from Country X until they stop the genocide.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That Jewish banker is greedy and controls the world.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 29,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, threat, or implicit call to action for *physical harm or injury* to a person or group. This includes vague but threatening commands like \"stop them\" and common hyperbolic commands for harm like \"go kill yourself\" or \"drink bleach.\" (0) for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\").\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or clearly implied individual* (e.g., \"that woman,\" \"my neighbor John\") and the hatred is based on personal animosity, not primarily on their membership in a protected group. (0) if it is directed at a *broad, generalized group* (e.g., \"all women,\" \"Indian people\") OR if a slur or attack on a protected group is used against an unnamed individual (e.g., \"you're a [slur]\"), as the hatred is generalized.\n*   `gender`: (1) if the text attacks or demeans a person or group based **solely** on their sex (e.g., male/female) or gender-specific traits and stereotypes. This includes misogyny and misandry. (0) for attacks on sexual orientation or gender identity (use `sexual_orientation` for those). Not every use of a gendered swear word (e.g., \"pussy\") qualifies; it must be an attack *on the group*.\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race or ethnicity (e.g., Black, White, Asian). (0) otherwise.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., Indian, Norwegian, American). (0) otherwise. (Note: This is separate from `race`).\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs related to disability. (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., gay, lesbian, bisexual) OR gender identity (e.g., transgender, non-binary). This includes homophobia, biphobia, and transphobia. **This category takes precedence over `gender` for attacks on LGBTQ+ individuals unless there is an *additional*, clear attack on gender itself.** (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  Be precise. If the text does not clearly and unambiguously fit the definition, mark it as (0).\n3.  Analyze the *primary intent* of the text. The key question is: \"Is the primary intent to degrade, intimidate, or incite hatred against a protected group, or is it a personal insult between individuals?\" Stereotypes used in a personal argument may not qualify if the overall intent is not to attack the entire group.\n4.  **For `sexual_orientation` vs. `gender`: If the attack is on a person's LGBTQ+ status (lesbian, gay, trans, etc.), use `sexual_orientation`. Only use `gender` if the attack is specifically about them being a man or a woman (e.g., \"all women are stupid\").**\n5.  **Treat commands for self-harm or harmful acts (e.g., \"go die,\" \"kill yourself,\" \"drink bleach\") as `violence: 1`, as they are clear calls to action for physical harm.**\n6.  **If a slur or group-based attack is used against an individual (e.g., \"you are a [slur]\"), the hatred is generalized. Classify it as `directed_vs_generalized: 0`. Reserve `directed_vs_generalized: 1` for attacks on named individuals based on personal grievance.**\n7.  For `directed_vs_generalized`, if the target is a generalized group (even if phrased with a plural like \"they,\" \"them,\" or \"ones\"), mark it as (0).\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"TRANSGENDERS ARE DISGUSTING.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"We need to stop these invaders by any means necessary!\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"Go bleach yourself.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 30,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON dictionary using the following labels and definitions:\n\n**LABELS AND DEFINITIONS:**\n*   `violence`: (1) if the text expresses a wish, encouragement, command, or threat of *physical harm or injury* to a person or group, including self-harm. Do not mark for non-physical harm (e.g., \"I hope they get fired\") or judicial outcomes (e.g., \"they should be in jail\"). (0) otherwise.\n*   `directed_vs_generalized`: (1) if the hateful speech is directed at a *specific, named or implied individual* (e.g., using a name, a username, or 'you' in a direct address to a specific person). (0) if it is directed at a *broad, generalized group* (e.g., \"all women,\" \"Indian people\") even if the word 'you' is used rhetorically to address the group.\n*   `gender`: (1) if the text attacks or demeans a person or group based on their gender, gender identity, or expression (e.g., misogyny, misandry, transphobia). This includes attacks on transgender and non-binary people. (0) otherwise.\n*   `race`: (1) if the text attacks or demeans a person or group based on their perceived race or ethnicity (e.g., Black, White, Asian). (0) otherwise.\n*   `national_origin`: (1) if the text attacks or demeans a person or group based on their nationality, citizenship, or native language (e.g., Indian, Norwegian, American). (Note: This is separate from `race`. Attacks based on immigration status often fall here.) (0) otherwise.\n*   `disability`: (1) if the text attacks or demeans a person or group based on physical, mental, or developmental disability. This includes using slurs related to disability and derogatory language about mental or physical health (e.g., \"crazy,\" \"retard,\" \"you have mental problems\"). (0) otherwise.\n*   `religion`: (1) if the text attacks or demeans a person or group based their religion or religious beliefs (e.g., Islamophobia, anti-Semitism). (0) otherwise.\n*   `sexual_orientation`: (1) if the text attacks or demeans a person or group based their sexual orientation (e.g., homophobia, biphobia). (Note: This is separate from `gender`. Attacks on being gay, lesbian, or bisexual are `sexual_orientation`. Attacks on being transgender are `gender`.) (0) otherwise.\n\n**RULES:**\n1.  A text can belong to multiple categories simultaneously.\n2.  **Be precise and conservative.** If the text does not **clearly and unambiguously** fit the definition, mark it as (0). Analyze the primary intent and context. A word being vulgar does not automatically make it hate speech in a specific category.\n3.  Analyze the *primary intent* of the text.\n4.  **Interpret language literally when it describes harmful actions. Commands like \"go kill yourself\" or \"bleach your skin\" are violence, not just metaphors.**\n\n**OUTPUT FORMAT:**\nOutput MUST be a valid JSON dictionary with the exact keys above. Do not add any other text or explanation before or after the JSON.\n\n**EXAMPLES:**\n- Input: \"I'm going to find and kill you, John.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"All immigrants are criminals and should go home.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"That gay guy is a freak.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\n\n- Input: \"Go bleach yourself you black coward.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"If you're depressed just end it all lol.\"\n  Output: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}\n\n- Input: \"Tranny is a slur for transgender people, which is a gender identity.\"\n  Output: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nNow, classify the following text:\nText: {{INPUT_TEXT}}",
            "Q": 0.10000000000000002,
            "N": 1,
            "reward": 0.1,
            "children": []
          }
        ]
      }
    ]
  }
}