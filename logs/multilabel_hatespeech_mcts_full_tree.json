{
  "config": {
    "mcts_iters": 10,
    "rollout_length": 4,
    "depth_threshold": 5,
    "width_threshold": 3
  },
  "search_stats": {
    "total_nodes": 31,
    "total_Q_values": 31
  },
  "best_node": {
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "depth": 0,
    "Q": 3.0000000000000018,
    "N": 30
  },
  "search_tree": {
    "id": 0,
    "depth": 0,
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "Q": 3.0000000000000018,
    "N": 30,
    "reward": 0.1,
    "children": [
      {
        "id": 1,
        "depth": 1,
        "action_sequence": [
          "SuccessDrivenAction"
        ],
        "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain:\n\n1.  **Intent:** Is there encouragement of harm or hateful language?\n2.  **Target:** Is the attack directed at a specific person (`1`) or a generalized group (`0`)?\n3.  **Attributes:** What characteristic is attacked? Map contextually to the schema (e.g., \"failure\" may be an insult, not a disability reference).\n4.  **Assign Labels:** Output `1` only for a clear, direct attack based on that specific attribute.\n\n**Schema:**\n`violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`\n\n**Output a valid JSON object with these keys.**\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
        "Q": 1.0,
        "N": 10,
        "reward": 0.1,
        "children": [
          {
            "id": 2,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain step-by-step:\n\n1.  **Intent & Violence:** Does the text encourage, threaten, or desire physical harm, suicide, or violence? If yes, set `violence: 1`.\n2.  **Target:** Who is the attack directed at?\n    - A specific, identifiable person (use `1` for `directed_vs_generalized`).\n    - A generalized group or category of people (use `0` for `directed_vs_generalized`).\n3.  **Attribute Analysis:** Determine the primary characteristic being attacked. Analyze the entire context.\n    - **Rule 1 (Specificity):** Identify the most specific and relevant attribute from the schema. (e.g., \"Go back to Asia\" is primarily `national_origin`, not `race`).\n    - **Rule 2 (Context):** An attribute is relevant if the attack is **based on, uses, or leverages** that characteristic, even if it's not the direct object of a harmful verb. (e.g., \"Being blind would make me want to kill myself\" uses `disability` as the reason for the violent suggestion).\n4.  **Assign Labels:** For the attribute keys (`gender`, `race`, etc.), output `1` only for the **single primary attribute** identified in Step 3. Output `0` for all others.\n5.  **Output a valid JSON object** with the exact keys: `violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`.\n\n**Schema:**\n`violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Yo nigga go back to Asia\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Being blind would make me want suicide. Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 3,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain step-by-step:\n\n1.  **Intent & Violence:** Does the text encourage, threaten, or desire physical harm, suicide, or violence? If yes, set `violence: 1`.\n2.  **Target:** Who is the attack directed at?\n    - A specific, identifiable person (use `1` for `directed_vs_generalized`).\n    - A generalized group or category of people (use `0` for `directed_vs_generalized`).\n3.  **Attribute Analysis:** Determine the **single primary characteristic** the attack is fundamentally *based on*. Analyze the entire context.\n    - **Rule 1 (Specificity):** Identify the most specific and relevant attribute from the schema.\n    - **Rule 2 (Context):** An attribute is relevant if the attack **uses or leverages** that characteristic as the reason for the harm, even if it's not the direct object of a verb.\n4.  **Assign Labels:** For the attribute keys (`gender`, `race`, etc.), output `1` **only for the single primary attribute** identified in Step 3. **Output `0` for all others.** This is a strict invariant. Ignore secondary insults that are not the core basis of the attack.\n5.  **Output a valid JSON object** with the exact keys: `violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`.\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Yo nigga go back to Asia\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Being blind would make me want suicide. Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 4,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain step-by-step:\n\n1.  **Intent & Violence:** Does the text explicitly encourage, threaten, or desire physical harm, suicide, or violence? Ignore strong but non-violent language (e.g., \"stupid,\" \"shame,\" \"must be stopped\"). If yes, set `violence: 1`.\n2.  **Target:** Who is the attack directed at? The target must be a person or a group of people.\n    - A specific, identifiable person (use `1` for `directed_vs_generalized`).\n    - A generalized group or category of people (use `0` for `directed_vs_generalized`).\n    - If the target is an idea, concept, or inanimate object, set `directed_vs_generalized: 0`.\n3.  **Attribute Analysis:** Determine the single primary characteristic that is the basis for the attack. An attribute is relevant if the attack uses that characteristic as the reason or leverage for the negative statement. Find the core trait being criticized.\n4.  **Assign Labels:** For the attribute keys, output `1` only for the single primary attribute identified in Step 3. Output `0` for all others.\n5.  **Output a valid JSON object** with the exact keys: `violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`.\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Easter is the most silly holiday ever existed. It must be stopped.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain step-by-step:\n\n1.  **Intent & Violence:** Does the text explicitly encourage, threaten, or desire **physical harm, suicide, or violence**? (Note: Non-physical harassment or commands do not count). If yes, set `violence: 1`.\n2.  **Target:** Who is the attack directed at?\n    - A specific, identifiable person or small group (use `1` for `directed_vs_generalized`).\n    - A broad, generalized, or vague category (use `0` for `directed_vs_generalized`).\n3.  **Primary Attribute Analysis:** Use the \"But-For\" test: **But for this attribute, would the attack lose its primary meaning?**\n    - **Rule 1 (Specificity):** Identify the most specific and relevant attribute from the schema.\n    - **Rule 2 (Context):** An attribute is primary if the attack is **fundamentally based on** that characteristic.\n4.  **Assign Labels:** For the attribute keys (`gender`, `race`, etc.), output `1` **only for the single primary attribute** identified in Step 3. Output `0` for all others.\n5.  **Output a valid JSON object** with the exact keys: `violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`.\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: kneel down, white boy. its the era of brown people now\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 6,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain:\n\n1.  **Intent:** Does the text encourage, glorify, or wish for physical harm, self-harm, or death? If yes, set `violence: 1`.\n2.  **Target:** Who is the target of the attack?\n    *   `directed_vs_generalized: 1` if aimed at a specific, named individual or a very small, identifiable group.\n    *   `directed_vs_generalized: 0` if aimed at a broad, generalized group defined by a protected attribute (e.g., \"all Muslims,\" \"white people\").\n3.  **Attributes:** Based on the context, what is the specific protected characteristic being attacked? A harmful statement must be **explicitly based on or linked to** this attribute. Be precise:\n    *   `disability`: Includes both physical and mental disabilities, mental health conditions, and neurodivergence. Includes slurs like \"retard\" and contextually used terms like \"sick,\" \"crazy,\" or \"mental\" when used as a pejorative attack on someone's state of mind.\n    *   `race`: Refers to perceived racial phenotypes (e.g., Black, white, Asian).\n    *   `national_origin`: Refers to nationality, ethnicity, or immigrant status (e.g., Mexican, Indian, Irish).\n    *   `religion`: Refers to religious affiliation (e.g., Muslim, Jewish, Christian).\n    *   `gender`: Refers to gender identity or expression (e.g., man, woman, non-binary, transgender).\n    *   `sexual_orientation`: Refers to romantic or sexual attraction (e.g., gay, straight, bisexual).\n    *   **Crucial:** If a violent suggestion (`violence: 1`) is made **because of** a stated attribute, you MUST mark that attribute as `1`. (e.g., \"Being blind would make me want suicide. Go kill yourself\" attacks based on `disability`).\n4.  **Assign Labels:** Output `1` only for a clear, direct attack based on that specific attribute. All other keys should be `0`.\n\n**Schema:**\n`violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`\n\n**Output a valid JSON object with these keys.**\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Disgusting sick queer kill yourself\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 1}\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain:\n\n1.  **Intent:** Does the text encourage, glorify, or wish for physical harm, self-harm, or death? If yes, set `violence: 1`. Note: Generic insults without a clear call to action are not violent.\n2.  **Target:** Who is the immediate target of the attack?\n    *   `directed_vs_generalized: 1` **ONLY if** the attack is directed at a **specific, named person** (e.g., \"Joe Biden should die\") or a **tiny, named group** (e.g., \"the Smith family\"). The attack must be personal.\n    *   `directed_vs_generalized: 0` if the attack is directed at a **broad group** based on a protected attribute (e.g., \"all Muslims,\" \"white people\") or a large, unnamed category (e.g., \"your kind,\" \"you people\"). **Major religions, large racial groups, and nationalities are ALWAYS generalized (0).**\n    *   **Final Check:** If `violence: 0` and all attribute flags are `0`, then `directed_vs_generalized` must also be `0`.\n3.  **Attributes:** Based on the context, what is the specific protected characteristic being attacked? A harmful statement must be **explicitly based on or linked to** this attribute. Be precise:\n    *   `race`: **ONLY** for attacks based on perceived **physical racial characteristics** (e.g., Black, white). (e.g., slurs like the n-word).\n    *   `national_origin`: For attacks based on **country of origin, ethnicity, or immigrant status** (e.g., Mexican, Indian, \"go back to your country\").\n    *   `religion`: Refers to religious affiliation (e.g., Muslim, Jewish, Christian, Buddhist).\n    *   `gender`: Refers to gender identity or expression (e.g., man, woman, non-binary, transgender).\n    *   `sexual_orientation`: **ONLY** for attacks based on romantic or sexual attraction (e.g., gay, lesbian, straight).\n    *   `disability`: Includes both physical and mental disabilities. Only mark `1` if the term is clearly being used to attack someone's mental or physical state (e.g., \"retard,\" \"crazy\" used as a pejorative linked to their condition).\n    *   **Crucial:** If a violent suggestion (`violence: 1`) is made **because of** a stated attribute, you MUST mark that attribute as `1`.\n4.  **Assign Labels:** Output `1` only for a clear, direct attack based on that specific attribute. All other keys should be `0`.\n\n**Schema:**\n`violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`\n\n**Output a valid JSON object with these keys.**\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Disgusting sick queer kill yourself\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 1}\n\nInput: Islam is a religion of hate.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 8,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain carefully:\n\n1.  **Intent:** Does the text encourage, glorify, or wish for physical harm, self-harm, or death? If yes, set `violence: 1`.\n\n2.  **Target:** Who is the target of the attack? This determines `directed_vs_generalized`.\n    *   `directed_vs_generalized: 1` if the attack is aimed at a **specific, named individual or is a personal insult.**\n    *   `directed_vs_generalized: 0` if the attack is aimed at a **broad group defined by a protected attribute.** Even if the group is identifiable, if the attack is based on a group attribute, use `0`.\n\n3.  **Attributes:** Based on the context, what is the specific protected characteristic being attacked? A harmful statement must be **explicitly and primarily based on this attribute. The attribute must be the core reason for the attack.**\n    *   `disability`: **Direct attacks** on physical/mental disabilities or neurodivergence. Includes slurs like \"retard.\" **Terms like \"sick,\" \"crazy,\" or \"mental\" are only `1` if they are clearly used to describe a disability/condition, not as a general insult.**\n    *   `race`: **Direct attacks** on perceived racial phenotypes (e.g., Black, white, Asian).\n    *   `national_origin`: **Direct attacks** on nationality, ethnicity, or immigrant status (e.g., Mexican, Indian, Irish). **For groups like Jewish people, prefer `religion` unless the context explicitly mentions ethnicity or origin without religion.**\n    *   `religion`: **Direct attacks** on religious affiliation (e.g., Muslim, Jewish, Christian). **Terms like \"demon\" or \"devil\" are only `1` if used in a religious context to attack someone's faith, not as a general metaphor for \"evil.\"**\n    *   `gender`: **Direct attacks** on gender identity or expression (e.g., woman, man, non-binary, transgender).\n    *   `sexual_orientation`: **Direct attacks** on romantic or sexual attraction (e.g., gay, straight, bisexual).\n    *   **Crucial:** If a violent suggestion (`violence: 1`) is made **explicitly because of** a stated attribute, you MUST mark that attribute as `1`.\n\n4.  **Assign Labels:** Output `1` only for a clear, direct attack based on that specific attribute. All other keys should be `0`. **If an attack could fit multiple attributes, choose the one that is the most directly and primarily referenced.**\n\n**Schema:**\n`violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`\n\n**Output a valid JSON object with these keys.**\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Disgusting sick queer kill yourself\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 1}\n\nInput: They are insolent parasites and always have been. I bear no ill will to Christ, but the jewish race itself is a plague.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\nInput: Racist ass white ass people\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: She is heavily relying on him to turn the other cheek...tough talking demon infested woman.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 9,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain carefully:\n\n1.  **Intent:** Does the text explicitly encourage, glorify, or wish for physical harm, self-harm, or death? If yes, set `violence: 1`.\n2.  **Target:** Who is the target of the attack?\n    *   `directed_vs_generalized: 1` **only if** aimed at a **clearly identified, named individual or a uniquely small, named group** (e.g., \"John Smith,\" \"the board of directors of Company X\").\n    *   `directed_vs_generalized: 0` if aimed at a **broad group** defined by a protected attribute, **even if the phrasing is ambiguous** (e.g., \"all Muslims,\" \"white people,\" \"this islam men,\" \"a tranny\"). **When in doubt, classify as generalized (0).**\n3.  **Attributes:** A harmful statement must be **explicitly and primarily based on** a protected characteristic. Be precise:\n    *   `disability`: Attacks based on physical/mental disabilities or neurodivergence. **Use caution with terms like \"sick\" or \"crazy\"; only mark `1` if the context clearly shows they are attacking mental state, not just used as a generic insult.**\n    *   `race`: Based on perceived racial phenotypes (e.g., Black, white, Asian).\n    *   `national_origin`: Based on nationality, ethnicity, or immigrant status (e.g., Mexican, Indian, Irish).\n    *   `religion`: Based on religious affiliation (e.g., Muslim, Jewish, Christian).\n    *   `gender`: **Primarily** attacks based on gender identity or expression (e.g., woman, transgender, non-binary). **For slurs like \"tranny,\" classify under `gender`.**\n    *   `sexual_orientation`: **Primarily** attacks based on romantic or sexual attraction (e.g., gay, straight, bisexual). **For slurs like \"f****t\" or \"dyke,\" classify under `sexual_orientation`.**\n    *   **Crucial Refinement:** Mark an attribute as `1` **only if the negative sentiment (e.g., insult, violence) is demonstrably motivated by that attribute.** For example, \"Kill yourself, you crazy queer\" has violence (`1`) and two insults. The violence is a general command; it is not explicitly *because* the person is \"crazy\" or \"queer.\" Therefore, only `disability: 1` and `sexual_orientation: 1` are marked for the insults, **not because they caused the violence.** If the text was \"You should be killed for being queer,\" then the violence is directly because of the attribute, and both `violence` and `sexual_orientation` would be `1`.\n4.  **Assign Labels:** Output `1` only for a clear, direct attack based on that specific attribute. All other keys should be `0`.\n\n**Schema:**\n`violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`\n\n**Output a valid JSON object with these keys.**\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n**New Clarifying Examples:**\nInput: Disgusting sick queer kill yourself\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 1}\n**Reasoning:** The violent command is general. \"Sick\" is a pejorative attack on mental state (`disability: 1`). \"Queer\" is a slur based on `sexual_orientation: 1`.\n\nInput: This is exactly how I feel like when I see a tranny like this disgusting af\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n**Reasoning:** \"Tranny\" is a slur targeting gender identity. The target is a generalized group (\"a tranny\"), not a specific person.\n\nInput: Because everyone knows this islam men is the devil\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n**Reasoning:** The attack is based on religion (\"islam\"). The ambiguous phrase \"this islam men\" is interpreted as a generalized group.\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 10,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "Classify the text by analyzing its intent, target, and specific attributes. Follow this reasoning chain:\n\n0.  **Overall Tone:** First, determine if the text is primarily hostile, harassing, or hateful. If the tone is neutral or critical of ideas/behaviors without attacking innate characteristics, all labels are likely 0.\n\n1.  **Intent:** Is there encouragement of harm? Harm includes direct physical violence (`violence:1`) as well as calls for exclusion, segregation, deportation, or any other action that inflicts societal or physical harm. Distinguish between attacking a protected characteristic and criticizing an idea or behavior. Criticism of ideas, even if strong, is not an attack on a protected attribute.\n\n2.  **Target:** Is the attack directed at a specific person (`1`) or a generalized group (`0`)? If the target is a vague group (e.g., 'them', 'these people'), use the context of the attributes mentioned to determine if it is generalized.\n\n3.  **Attributes:** What is the primary, most specific innate characteristic being attacked? Map contextually to the schema.\n    *   **Specificity Rule:** Identify the most direct characteristic used as the basis for the insult. (e.g., A racist trope about sending people to Africa is an attack on `race`, not `national_origin`).\n    *   **Ambiguity Rule:** If a word could be metaphorical (e.g., 'blind') or literal, favor the literal meaning if it is a protected attribute unless the context is overwhelmingly a common idiom (e.g., 'that's crazy').\n    *   **Criticism vs. Attack:** An attack must be on an innate characteristic. Criticizing an ideology (e.g., \"white supremacy\") or religious practice is not an attack on `race` or `religion` unless it clearly attacks people for their innate identity.\n\n4.  **Assign Labels:** Output `1` only for a clear, direct attack based on that specific attribute.\n\n**Schema:**\n`violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`\n\n**Output a valid JSON object with these keys.**\n\n**Examples:**\nInput: Go kill yourself kid\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: Your sex change was a failure.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nInput: The brother is speaking the truth based on facts. In the U.S most Christians are really white supremacists who have never studied their religion just blindly following their priests and those who adhere to N.O.I. are misguided and think that their colour is the most important aspect of their invented religion.\nOutput: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nExplanation: This criticizes ideologies (\"white supremacy,\" religious practices) and labels them as \"misguided,\" which is an attack on religion, not on the innate characteristic of race.\n\nInput: Fucking idiots. Keep them out. They change the culture. Take over small towns. Vote in their own people to rule the town. And start enforcing their religious beliefs on everyone. It CANNOT FUCKING HAPPEN. Stop them now while you still can.\nOutput: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nExplanation: The primary intent is a violent call to action (\"Stop them now while you still can\") to exclude a vague group (\"them\"). The attributes (religion, national origin) are implied but not specific enough to label; the core violation is the call to harm.\n\n**Now, classify this text:**\nInput: {insert_user_input_here}\nOutput:",
            "Q": 0.10000000000000002,
            "N": 1,
            "reward": 0.1,
            "children": []
          }
        ]
      },
      {
        "id": 11,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert content moderation AI. Classify the following text into specific hate speech categories based on these precise definitions:\n\n**Category Definitions:**\n- **violence (1/0)**: Label 1 **only** for explicit, literal calls to physical harm. Ignore metaphorical language (e.g., \"kill feminism\", \"save your heads\").\n- **directed_vs_generalized (1/0)**: Label 1 **only** for directed calls to action. Label 0 for generalized statements of hate without specific action directives.\n- **gender (1/0)**: Hate based on gender identity.\n- **race (1/0)**: Hate based on race or ethnicity.\n- **national_origin (1/0)**: Hate based on national origin.\n- **disability (1/0)**: Hate based on disability status.\n- **religion (1/0)**: Hate based on religious beliefs.\n- **sexual_orientation (1/0)**: Hate based on sexual orientation.\n\n**Output Requirements:**\n- Analyze the text strictly according to these definitions\n- Return a valid JSON object with these exact keys\n- Output only the JSON, no additional text\n\nText to classify: `{text}`",
        "Q": 1.0,
        "N": 10,
        "reward": 0.1,
        "children": [
          {
            "id": 12,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Classify the text below using these precise definitions and strict reasoning steps:\n\n**Reasoning Steps (Follow Sequentially):**\n1.  **Violence & Direction:** First, identify explicit, literal calls to physical harm (e.g., \"stab,\" \"kill\"). Ignore metaphors. If found, set `violence: 1`. Then, determine if the call is a directed command (\"I will hurt *you*\") for `1` or a generalized wish (\"They should all die\") for `0` in `directed_vs_generalized`.\n2.  **Protected Category:** For any hate, ask: \"Is this *explicitly* based on the target's...\" Assign `1` only to the specific category that is the clear reason for the attack (e.g., race, religion, gender). If no explicit link exists, the category remains `0`.\n\n**Category Definitions:**\n- **violence (1/0)**: Explicit, literal calls to physical harm.\n- **directed_vs_generalized (1/0)**: 1 for directed calls to action; 0 for generalized statements.\n- **gender (1/0)**: Hate based on gender identity.\n- **race (1/0)**: Hate based on race or ethnicity.\n- **national_origin (1/0)**: Hate based on national origin.\n- **disability (1/0)**: Hate based on disability status.\n- **religion (1/0)**: Hate based on religious beliefs.\n- **sexual_orientation (1/0)**: Hate based on sexual orientation.\n\n**Output Requirements:**\n- Analyze the text strictly according to these steps and definitions.\n- Return a valid JSON object with these exact keys.\n- Output only the JSON, no additional text.\n\nText to classify: `{text}`",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 13,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Classify the text below using these precise definitions and strict reasoning steps:\n\n**CRITICAL REASONING STEPS (FOLLOW IN ORDER):**\n1.  **Violence & Direction:** First, identify explicit, literal calls to physical harm (e.g., \"stab,\" \"kill,\" \"go hurt them\"). Ignore metaphors, jokes, and hyperbolic wishes (e.g., \"I wish they would die\"). If found, set `violence: 1`. Then, determine if the call is a directed command/threat (\"I will hurt *you*\") for `1` or a generalized wish (\"They should all die\") for `0` in `directed_vs_generalized`. If no explicit call to harm is found, set both to `0`.\n2.  **Protected Category:** For any hateful/attacking content, ask: \"Is this *explicitly* based on the target's...\" Assign `1` only to the single category that is the clear reason for the attack. Self-targeting counts (e.g., \"I hate being a man\"). If no explicit link exists, the category remains `0`.\n\n**CATEGORY DEFINITIONS:**\n- **violence (1/0)**: Explicit, literal calls to physical harm. Not metaphors or wishes.\n- **directed_vs_generalized (1/0)**: 1 for directed calls to action; 0 for generalized statements. (0 if violence=0)\n- **gender (1/0)**: Hate based on gender identity.\n- **race (1/0)**: Hate based on race or ethnicity.\n- **national_origin (1/0)**: Hate based on national origin.\n- **disability (1/0)**: Hate based on disability status.\n- **religion (1/0)**: Hate based on religious beliefs.\n- **sexual_orientation (1/0)**: Hate based on sexual orientation.\n\n**OUTPUT REQUIREMENTS:**\n- Analyze the text strictly according to these sequential steps and definitions.\n- Return a valid JSON object with these exact keys and integer values (1/0).\n- Output only the JSON, no additional text.\n\nText to classify: `{text}`",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 14,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Classify the text below using these precise definitions and strict reasoning steps:\n\n**Reasoning Steps (Follow Sequentially):**\n1.  **Violence & Direction:** First, identify *explicit, literal, and direct calls to perform physical harm* (e.g., \"kill,\" \"stab,\" \"bomb\"). Strictly ignore metaphors, hyperbole, and non-literal language (e.g., \"devil,\" \"must be stopped\"). If found, set `violence: 1`. Then, determine if the call is a directed command targeting a specific entity (\"I will hurt *you*\") for `1` or a generalized wish (\"They should all die\") for `0` in `directed_vs_generalized`.\n2.  **Protected Category:** For any hate, ask: \"Does this *explicitly and unambiguously* name the target's...\" Assign `1` only to the specific category that is the clear, stated reason for the attack (e.g., race, religion, gender). If no explicit link exists, the category remains `0`. Criticism of beliefs or institutions associated with a protected group (e.g., a religious holiday) can constitute hate if it explicitly names the group.\n\n**Category Definitions:**\n- **violence (1/0)**: Explicit, literal calls to physical harm.\n- **directed_vs_generalized (1/0)**: 1 for directed calls to action; 0 for generalized statements.\n- **gender (1/0)**: Hate explicitly based on gender identity.\n- **race (1/0)**: Hate explicitly based on race or ethnicity.\n- **national_origin (1/0)**: Hate explicitly based on national origin.\n- **disability (1/0)**: Hate explicitly based on disability status.\n- **religion (1/0)**: Hate explicitly based on religious beliefs.\n- **sexual_orientation (1/0)**: Hate explicitly based on sexual orientation.\n\n**Output Requirements:**\n- Analyze the text strictly according to these sequential steps and definitions.\n- Return a valid JSON object with these exact keys.\n- Output only the JSON, no additional text.\n\nText to classify: `{text}`",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 15,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Classify the text below using these precise definitions and strict reasoning steps.\n\n**CRITICAL REASONING STEPS (FOLLOW IN ORDER):**\n1.  **Violence Check:** Identify explicit, literal calls to physical harm (e.g., \"kill,\" \"stab\"). Ignore all metaphors, jokes, and non-literal language. If found, set `violence: 1`. If none, set it to `0` and skip step 2.\n2.  **Direction Check (Only if violence=1):** Is the call a directed command (\"I will hurt *you*\")? Set `directed_vs_generalized: 1`. Is it a generalized wish (\"They should die\")? Set `directed_vs_generalized: 0`.\n3.  **Protected Category Check:** For any hate, ask: \"What is the *explicitly stated* reason?\" **Inference is forbidden.** The target's protected characteristic must be explicitly named (e.g., \"white,\" \"Jewish,\" \"gay\"). If no explicit link exists, all categories remain `0`. Assign `1` only to the single, most specific category that is the clear reason for the attack.\n\n**CATEGORY DEFINITIONS:**\n- `violence` (1/0): Explicit, literal calls to physical harm.\n- `directed_vs_generalized` (1/0): 1 for directed commands; 0 for generalized statements or if no violence.\n- `gender` (1/0): Hate based explicitly on gender.\n- `race` (1/0): Hate based explicitly on race/ethnicity.\n- `national_origin` (1/0): Hate based explicitly on national origin.\n- `disability` (1/0): Hate based explicitly on disability.\n- `religion` (1/0): Hate based explicitly on religious beliefs.\n- `sexual_orientation` (1/0): Hate based explicitly on sexual orientation.\n\n**OUTPUT REQUIREMENTS:**\n- Analyze the text strictly according to these steps.\n- Output **ONLY** a valid JSON object with these exact keys and integer values (1 or 0).\n- No other text or formatting.\n\nText to classify: `{text}`",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 16,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Classify the text strictly using these precise definitions and reasoning rules:\n\n**CRITICAL ANALYSIS RULES:**\n- **Violence (1/0)**: Label 1 **only** for explicit, literal calls to physical harm. Ignore metaphors and hyperbolic language.\n- **Directed vs Generalized (1/0)**: Label 1 **only** for directed calls to action. Label 0 for generalized statements without specific action directives.\n- **Protected Categories**: Label 1 **only** if hate is explicitly based on that attribute. Do not assume context.\n\n**CATEGORIES:**\nviolence, directed_vs_generalized, gender, race, national_origin, disability, religion, sexual_orientation\n\n**OUTPUT:**\n- Return only a valid JSON object with these exact keys\n- No additional text or explanations\n\nText to classify: `{text}`",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 17,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Classify the text by strictly applying these rules in sequence:\n\n**ANALYSIS RULES:**\n1.  **Violence (1/0)**: Label 1 **only** for explicit, literal calls to physical harm. Ignore all metaphors, hyperbole, and non-literal language.\n2.  **Directed vs Generalized (1/0)**: Label 1 **only** for directed calls to action that urge a specific audience to act. Label 0 for generalized grievances, wishes, or insults without a clear directive.\n3.  **Protected Categories**: Label 1 **only** if hatred is explicitly and directly linked to the attribute in the text. Do not assume any context.\n\n**CATEGORIES:**\nviolence, directed_vs_generalized, gender, race, national_origin, disability, religion, sexual_orientation\n\n**OUTPUT:**\n- Return only a valid JSON object with these exact keys.\n- No additional text or explanations.\n\nText to classify: `{text}`",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 18,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Classify the text by strictly applying these rules and reasoning steps:\n\n**RULES:**\n1.  **Violence (1/0)**: 1 **only** for explicit, literal calls to physical harm. Ignore metaphors, hyperbole, and non-literal language.\n2.  **Directed vs Generalized (1/0)**: 1 **only** if a violent call targets a specific entity. Ignore if `violence=0`.\n3.  **Protected Categories**: 1 **only** if hate is explicitly based on that attribute. Do not assume context.\n\n**REASONING:**\n- **Step 1:** Scan for literal calls to harm. If none, set `violence: 0` and `directed_vs_generalized: 0`.\n- **Step 2:** If violence is present, determine if the call is directed (`directed_vs_generalized: 1`) or a generalized wish (`directed_vs_generalized: 0`).\n- **Step 3:** For protected categories, set a category to 1 **only** if the text explicitly links negative sentiment to that specific attribute.\n\n**OUTPUT:**\n- Return only a valid JSON object with these exact keys: `violence, directed_vs_generalized, gender, race, national_origin, disability, religion, sexual_orientation`\n- No additional text or explanations.\n\nText to classify: `{text}`",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 19,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Classify the text by strictly following this reasoning process:\n\n**ANALYSIS STEPS:**\n1.  **Violence (1/0)**: Label 1 **only** for explicit, literal calls to physical harm. Ignore all metaphors and hyperbole.\n2.  **Directed (1/0)**: If violence=1, label 1 **only** if it is a directed command to act. Label 0 for generalized statements.\n3.  **Protected Categories**: For each category, label 1 **only** if hate is explicitly and directly linked to that attribute. Do not infer context.\n\n**REASONING STRATEGIES (From Successful Examples):**\n- Interpret violence calls literally, not metaphorically.\n- \"Directed\" requires a specific action directive.\n- Attribute linkage must be explicit; \"men\" is not inherently gender-based hate.\n\n**OUTPUT:**\n- Return only a valid JSON object with these exact keys: `violence, directed_vs_generalized, gender, race, national_origin, disability, religion, sexual_orientation`\n- No additional text or explanations.\n\nText to classify: `{text}`",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 20,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Classify the following text into specific hate speech categories based on these precise definitions:\n\n**Category Definitions:**\n- **violence (1/0)**: Label 1 **only** for explicit, literal calls to physical harm (e.g., imperative statements like \"go hurt them\") or credible threats. A violent fantasy or wish (e.g., \"I would love to watch...\") is **not** a call to action and should be labeled 0 for this category. Ignore metaphors and hyperbolic language.\n- **directed_vs_generalized (1/0)**: Label 1 if the hate is **directed at a specific, identifiable target** (e.g., a named person, a specific \"baby\"). Label 0 for hate directed at a broad, generalized group (e.g., \"all Christians\") or an abstract concept.\n- **gender (1/0)**: Hate primarily based on gender identity or expression (e.g., woman, man, non-binary, transgender). **Note:** This is distinct from sexual orientation.\n- **race (1/0)**: Hate primarily based on race or ethnicity.\n- **national_origin (1/0)**: Hate primarily based on national origin or citizenship.\n- **disability (1/0)**: Hate primarily based on physical, mental, or developmental disability status.\n- **religion (1/0)**: Hate primarily based on religious beliefs or affiliation.\n- **sexual_orientation (1/0)**: Hate primarily based on sexual orientation (e.g., gay, lesbian, bisexual). **Note:** This is distinct from gender identity.\n\n**Critical Instructions:**\n1.  Analyze the text strictly according to these definitions.\n2.  Identify the **primary basis** for the hateful content. A single text should typically only have one hate-based category (gender, race, etc.) labeled as 1. Mark multiple categories **only** if the hate is explicitly and equally based on multiple, separate grounds.\n3.  For `violence`, focus on grammatical imperative mood for calls to action. Statements of personal desire are not sufficient.\n4.  For `directed_vs_generalized`, focus on the specificity and identifiability of the target, not the presence of an action.\n5.  Return a valid JSON object with the exact keys below.\n6.  Output **only** the JSON, with no additional text or commentary.\n\nText to classify: `{text}`",
            "Q": 0.10000000000000002,
            "N": 1,
            "reward": 0.1,
            "children": []
          }
        ]
      },
      {
        "id": 21,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, or threatens physical harm.\n- `directed_vs_generalized`: 1 ONLY if the text targets a specific, named individual or very small, identifiable group. Use 0 for statements against broad protected classes (e.g., \"women,\" \"Muslims,\" \"gay people\").\n- `gender`: 1 if hate is explicitly and primarily based on gender.\n- `race`: 1 if hate is explicitly and primarily based on race/ethnicity.\n- `national_origin`: 1 if hate is explicitly and primarily based on national origin.\n- `disability`: 1 if hate is explicitly and primarily based on disability.\n- `religion`: 1 if hate is explicitly and primarily based on religion.\n- `sexual_orientation`: 1 if hate is explicitly and primarily based on sexual orientation.\n\n**Critical Guidelines:**\n1. Generalized statements targeting entire demographic groups should have `directed_vs_generalized`: 0\n2. Only apply characteristic labels (gender, race, etc.) if the hate is fundamentally based on that characteristic\n3. Consider the scale of the target - \"all [group]\" is generalized, \"[Specific Person]\" is directed\n4. Output ONLY valid JSON with no additional text\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
        "Q": 1.0,
        "N": 10,
        "reward": 0.1,
        "children": [
          {
            "id": 22,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, or threatens physical harm.\n- `directed_vs_generalized`: 1 if the text is addressed to a specific, named individual OR is a direct address targeting an individual or very small group based on their characteristics (e.g., \"you are a [slur]\", \"that [group] over there\"). Use 0 for generalized statements about entire demographic groups (e.g., \"all [group]\" or \"[group] are\").\n- `gender`: 1 if hate is explicitly and primarily based on gender.\n- `race`: 1 if hate is explicitly and primarily based on race/ethnicity.\n- `national_origin`: 1 if hate is explicitly and primarily based on national origin.\n- `disability`: 1 if hate is explicitly and primarily based on disability.\n- `religion`: 1 if hate is explicitly and primarily based on religion.\n- `sexual_orientation`: 1 if hate is explicitly and primarily based on sexual orientation.\n\n**Critical Guidelines:**\n1. The key factor for `directed_vs_generalized` is whether the text is speaking *about* a large group (generalized) or *to/at* a specific individual or small group (directed). Direct insults using \"you\" or describing a specific target (\"that man\") are directed.\n2. Only apply characteristic labels (gender, race, etc.) if the hate is fundamentally and explicitly based on that characteristic.\n3. Consider the scale of the target - \"All [group]\" is generalized. \"[Specific Person]\" is directed. \"You [slur]\" or \"[characteristic] boy/girl\" is also directed.\n4. Output ONLY valid JSON with no additional text.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Go back to your country, you immigrant\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 23,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, or threatens physical harm.\n- `directed_vs_generalized`: 1 if the text is addressed to a specific, named individual OR is a direct address targeting an individual or very small group based on their characteristics (e.g., \"you are a [slur]\", \"that [group] over there\"). Use 0 for generalized statements about entire demographic groups (e.g., \"all [group]\" or \"[group] are\").\n- `gender`: 1 if hate is explicitly and primarily based on gender.\n- `race`: 1 if hate is explicitly and primarily based on race/ethnicity.\n- `national_origin`: 1 if hate is explicitly and primarily based on national origin.\n- `disability`: 1 if hate is explicitly and primarily based on disability.\n- `religion`: 1 if hate is explicitly and primarily based on religion.\n- `sexual_orientation`: 1 if hate is explicitly and primarily based on sexual orientation.\n\n**Critical Guidelines:**\n1. **For `directed_vs_generalized`:** Focus on whether the text speaks *to* a target (directed) or *about* a group (generalized). Direct addresses (\"you\", commands) to characterized individuals are directed. Statements using \"all [group]\", \"they\", or \"we\" are generalized.\n2. **For characteristics:** The named characteristic must be the explicit and fundamental basis for the hate. Identify the core subject of derogatory language. Set only the primary characteristic to 1.\n3. **For `violence`:** Reserve for clear threats or encouragement of physical harm. Non-physical harm or commands for submission do not qualify.\n4. **Analysis order:** First determine directed vs generalized, then identify the primary characteristic, then check for violence.\n5. Output ONLY valid JSON with no additional text.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Go back to your country, you immigrant\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 24,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate and the recognition of implicit language.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, or threatens physical harm or severe harassment.\n- `directed_vs_generalized`: 1 if the text is a direct address targeting a specific, named individual OR an identifiable small group (e.g., using \"you,\" \"that [slur]\"). Use 0 for statements about an entire demographic group, including pluralized references (e.g., \"lesbians are,\" \"[group] can't\").\n- `gender`: 1 if hate is based on gender (including sexist content, stereotypes, or tropes).\n- `race`: 1 if hate is based on race/ethnicity.\n- `national_origin`: 1 if hate is based on national origin, citizenship, or immigrant status.\n- `disability`: 1 if hate is based on disability.\n- `religion`: 1 if hate is based on religion.\n- `sexual_orientation`: 1 if hate is based on sexual orientation (including implicit tropes and stereotypes).\n\n**Critical Guidelines:**\n1. For `directed_vs_generalized`, a direct address using \"you\" or targeting a specific person is directed. Statements about groups, even without \"all,\" are generalized (e.g., \"lesbians can't\").\n2. Apply characteristic labels for both explicit mentions AND implicit hate based on well-known stereotypes, tropes, or coded language.\n3. If multiple characteristics are attacked, set ALL applicable keys to 1.\n4. Consider context and common hate speech tropes, not just explicit keywords.\n5. Output ONLY valid JSON with no additional text.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"Women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"lesbians cant do math fucking mood lmao\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 25,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, or threatens physical harm. This includes clear threats, calls to action, or severe dehumanization that implies a threat (e.g., comparing people to vermin, disease, or cancer that needs to be \"eradicated\").\n- `directed_vs_generalized`: 1 if the text is addressed to a specific, named individual OR is a direct address (using \"you\" or \"that [person]\") targeting an individual or very small, specific group based on their characteristics. Use 0 for generalized statements about entire demographic groups (e.g., \"all [group]\" or \"[group] are\").\n- `gender`: 1 if hate is explicitly based on gender (e.g., man, woman, non-binary). *Note: 'Gender' is separate from 'sexual_orientation'.*\n- `race`: 1 if hate is explicitly based on race or ethnicity (e.g., Black, White, Asian, Hispanic).\n- `national_origin`: 1 if hate is explicitly based on nationality, citizenship, or immigrant status (e.g., Mexican, American, immigrant). *If a statement could be based on race or origin (e.g., \"go back to your country\"), prefer `national_origin` if immigrant status/nationality is mentioned, otherwise use `race`.*\n- `disability`: 1 if hate is explicitly based on physical or mental disability.\n- `religion`: 1 if hate is explicitly based on religion or religious affiliation (e.g., Muslim, Jewish, Christian).\n- `sexual_orientation`: 1 if hate is explicitly based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n**Critical Guidelines:**\n1.  **Target Specificity:** For `directed_vs_generalized`, the key factor is whether the text is speaking *about* a large, abstract group (generalized) or *to/at* a specific, tangible individual or small group (directed). Direct insults using \"you\" or describing a specific target (\"that man\") are directed. Statements about \"all [group]\" are generalized.\n2.  **Characteristic Labels:** Only apply a characteristic label if the hate is explicitly based on that characteristic.\n3.  **Handling Umbrella Terms:** If a text targets an umbrella group (e.g., \"LGBT,\" \"BIPOC,\" \"people of color\"), analyze the explicit terms used:\n    - \"LGBT\" / \"LGBTQ+\" -> Set `sexual_orientation`: 1 and `gender`: 1.\n    - \"BIPOC\" / \"POC\" / \"people of color\" -> Set `race`: 1.\n4.  **Multiple Characteristics:** The characteristic categories are not mutually exclusive. A text can be hateful based on multiple characteristics (e.g., \"black women\" -> `race`:1, `gender`:1). Evaluate each category independently based on the explicit content of the text.\n5.  **Output Format:** Output ONLY a valid JSON object with no additional text, explanations, or formatting.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Go back to your country, you immigrant\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Lgbt now spread in asia. Many anti lgbt in asian worried their culture are ruined cuz this virus\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 26,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate, and be vigilant for implicit meaning and coded language.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, or threatens physical harm.\n- `directed_vs_generalized`: 1 ONLY if the text targets a specific, named individual or a very small, uniquely identifiable group (e.g., 'the managers at Starbucks on 5th Ave'). Use 0 for statements against broad protected classes (e.g., \"women,\" \"Muslims\") or ambiguous groups that represent a class (e.g., 'Indian subs'). If the target scale is unclear, default to generalized (0).\n- `gender`: 1 if hate is based on gender (e.g., misogyny, misandry).\n- `race_ethnicity`: 1 if hate is based on race or ethnicity (e.g., Black, White, Asian, Romani). This takes precedence over national origin for ethnic groups.\n- `national_origin`: 1 if hate is explicitly based on country of origin or citizenship (e.g., 'Mexicans', 'the French') and is not primarily about ethnicity.\n- `disability`: 1 if hate is based on physical or mental disability.\n- `religion`: 1 if hate is based on religion or beliefs (e.g., Muslim, Jewish, Christian).\n- `sexual_orientation`: 1 if hate is based on sexual orientation (e.g., gay, lesbian, bisexual).\n- `gender_identity`: 1 if hate is based on gender identity (e.g., transgender, non-binary, or using slurs like 'tranny', 'shemale').\n\n**Critical Guidelines:**\n1. A text can be classified into multiple characteristic categories if the hate is based on multiple factors.\n2. Analyze implied meaning and common bigoted tropes, not just explicit keywords. Consider the overall context and intent.\n3. Generalized statements targeting entire demographic groups should have `directed_vs_generalized`: 0.\n4. Only apply characteristic labels if the hate is fundamentally based on that characteristic.\n5. Output ONLY valid JSON with no additional text.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race_ethnicity\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race_ethnicity\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"He's a little light in his loafers, if you know what I mean.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race_ethnicity\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
            "Q": 0.4,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 27,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate, and be vigilant for implicit meaning and coded language.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, or threatens physical harm.\n- `directed_vs_generalized`: 1 ONLY if the text targets a specific, named individual or a very small, uniquely identifiable group (e.g., 'the managers at Starbucks on 5th Ave'). Use 0 for statements against broad protected classes (e.g., \"women,\" \"Muslims\") or ambiguous groups that represent a class (e.g., 'Indian subs'). If the target scale is unclear, default to generalized (0).\n- `gender`: 1 if hate is fundamentally based on gender (e.g., misogyny, misandry).\n- `race`: 1 if hate is fundamentally based on race or perceived race (e.g., Black, White, Asian).\n- `national_origin`: 1 if hate is fundamentally based on country of origin, citizenship, or immigration status (e.g., 'Mexicans', 'the French', 'immigrants', 'refugees').\n- `disability`: 1 if hate is based on physical or mental disability.\n- `religion`: 1 if hate is based on religion or beliefs (e.g., Muslim, Jewish, Christian).\n- `sexual_orientation`: 1 if hate is based on sexual orientation (e.g., gay, lesbian, bisexual).\n- `gender_identity`: 1 if hate is based on gender identity (e.g., transgender, non-binary).\n\n**Critical Guidelines:**\n1. A text can be classified into multiple characteristic categories if the hate is based on multiple factors.\n2. Analyze implied meaning and common bigoted tropes, not just explicit keywords. Consider the overall context and intent.\n3. For groups defined by nationality or origin (e.g., \"refugees\", \"immigrants\"), classify as `national_origin`: 1.\n4. Only apply characteristic labels if the hate is fundamentally based on that characteristic. Ignore secondary insults that are not the primary basis for hate (e.g., \"fat\" in \"fat lesbians\").\n5. Generalized statements targeting entire demographic groups should have `directed_vs_generalized`: 0.\n6. Output ONLY valid JSON with no additional text.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"He's a little light in his loafers, if you know what I mean.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nInput: \"I like dead refugees on my shore\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"Fat lesbians\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 28,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate, and be vigilant for implicit meaning and coded language.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, or threatens physical harm, including self-harm or suicide.\n- `directed_vs_generalized`: 1 ONLY if the text targets a specific, named individual OR uses direct second-person address ('you') in a hateful context OR targets a very small, uniquely identifiable group (e.g., 'the managers at Starbucks on 5th Ave'). Use 0 for statements against broad protected classes (e.g., \"women,\" \"Muslims\") or ambiguous groups that represent a class (e.g., 'Indian subs'). If the target scale is unclear, default to generalized (0).\n- `gender`: 1 if hate is fundamentally based on gender (e.g., misogyny, misandry).\n- `race_ethnicity`: 1 if hate is based on race or ethnicity (e.g., Black, White, Asian, Romani). This takes precedence over national origin for ethnic groups.\n- `national_origin`: 1 if hate is explicitly based on country of origin or citizenship (e.g., 'Mexicans', 'the French') and is not primarily about ethnicity.\n- `disability`: 1 if hate is based on physical or mental disability.\n- `religion`: 1 if hate is based on religion or beliefs (e.g., Muslim, Jewish, Christian).\n- `sexual_orientation`: 1 if hate is based on sexual orientation (e.g., gay, lesbian, bisexual).\n- `gender_identity`: 1 if hate is based on gender identity (e.g., transgender, non-binary, or using slurs like 'tranny', 'shemale').\n\n**Critical Guidelines:**\n1. A text can be classified into multiple characteristic categories if the hate is based on multiple factors.\n2. Analyze implied meaning and common bigoted tropes, not just explicit keywords. Consider the overall context and intent.\n3. Generalized statements targeting entire demographic groups should have `directed_vs_generalized`: 0.\n4. Only apply characteristic labels if the hate is fundamentally based on that characteristic. The presence of a non-protected characteristic insult (e.g., 'stupid', 'fat', 'ugly') does not automatically qualify as hate based on a protected characteristic.\n5. Output ONLY valid JSON with no additional text.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race_ethnicity\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race_ethnicity\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"He's a little light in his loafers, if you know what I mean.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race_ethnicity\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nInput: \"Fat lesbians\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race_ethnicity\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nInput: \"With no legs you are just a bag of meat. Get dead already.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race_ethnicity\":0,\"national_origin\":0,\"disability\":1,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 29,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate, and be vigilant for implicit meaning and coded language.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, threatens, or expresses a desire for physical harm.\n- `directed_vs_generalized`: 1 ONLY if the text targets a named individual (e.g., \"John Smith\") or a group so small and specific they could be named (e.g., \"the managers at Starbucks on 5th Ave\"). Use 0 for statements using singular pronouns (\"her,\" \"him,\" \"they\") against what is clearly a representative of a broad class, or for statements against broad protected classes (e.g., \"women,\" \"Muslims\"). If the target scale is unclear, default to generalized (0).\n- `gender`: 1 if hate is based on gender (e.g., misogyny, misandry).\n- `race`: 1 if hate is based on race or ethnicity (e.g., Black, White, Asian, Romani). This takes precedence over national origin for ethnic groups.\n- `national_origin`: 1 if hate is explicitly based on country of origin or citizenship (e.g., 'Mexicans', 'the French') and is not primarily about ethnicity.\n- `disability`: 1 if hate is based on physical or mental disability.\n- `religion`: 1 if hate is based on religion or beliefs (e.g., Muslim, Jewish, Christian).\n- `sexual_orientation`: 1 if hate is based on sexual orientation (e.g., gay, lesbian, bisexual).\n- `gender_identity`: 1 if hate is based on gender identity (e.g., transgender, non-binary).\n\n**Critical Guidelines:**\n1. Analyze implied meaning and common bigoted tropes, not just explicit keywords. Consider the overall context and intent.\n2. Generalized statements targeting entire demographic groups should have `directed_vs_generalized`: 0.\n3. Only apply characteristic labels if the hate is fundamentally based on that characteristic.\n4. For overlapping characteristics, identify the primary basis of the hate. Hate targeting a 'Muslim woman' is primarily about `religion`. Hate describing sexual violence against 'her' is primarily about `gender`. Hate targeting 'Black women' is primarily about `race`.\n5. A text can be classified into multiple characteristic categories only if the hate is based on multiple, clear, and primary factors.\n6. Output ONLY valid JSON with no additional text.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"Some blonde bitch cut me off in traffic; I wish I could run her off the road.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"He's a little light in his loafers, if you know what I mean.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 30,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Your task is to analyze text and classify it into hate speech categories using the following schema. Pay special attention to the distinction between directed and generalized hate.\n\n**Classification Schema:**\nOutput a JSON object with these keys and integer values (0 or 1):\n- `violence`: 1 if the text promotes, encourages, threatens physical harm, or describes/glorifies past violent acts to incite hatred.\n- `directed_vs_generalized`: 1 if the text targets a specific, named individual OR a specific person/entity referred to by direct address (e.g., \"you\"). Use 0 for statements against broad protected classes (e.g., \"women,\" \"Muslims\") or large, non-specific groups.\n- `gender`: 1 if hate is explicitly and primarily based on gender.\n- `race`: 1 if hate is explicitly and primarily based on race/ethnicity (e.g., Black, White, Asian). Note: 'Hindu' is a religious/cultural identifier, not primarily a racial one.\n- `national_origin`: 1 if hate is explicitly and primarily based on national origin, nationality, or citizenship (e.g., \"Indians,\" \"Americans\").\n- `disability`: 1 if hate is explicitly and primarily based on disability.\n- `religion`: 1 if hate is explicitly and primarily based on religion (e.g., Muslims, Christians, Hindus).\n- `sexual_orientation`: 1 if hate is explicitly and primarily based on sexual orientation.\n\n**Critical Guidelines:**\n1. A text can have multiple characteristic labels (e.g., `religion` and `national_origin`) if hate is based on multiple overlapping characteristics.\n2. For `violence`: A text that describes past atrocities to justify hatred or anger counts as promoting violence. Clear violent threats (e.g., \"I will kill you\") always count.\n3. Analyze the core characteristic being attacked. The word \"racist\" may be a slur for hate based on religion or nationality, not necessarily race.\n4. Generalized statements targeting entire demographic groups should have `directed_vs_generalized`: 0.\n5. Only apply characteristic labels if the hate is fundamentally based on that characteristic.\n6. Output ONLY valid JSON with no additional text.\n\n**Examples:**\nInput: \"That blogger Sarah Jones is a stupid lesbian and should be fired\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All women are incompetent and shouldn't be allowed to lead\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"You're a Christian fool and I'll break your knees\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":1,\"sexual_orientation\":0}\n\nInput: \"They celebrate the genocide of my people\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nNow classify this text:\nText: {{text}}\n\nOutput:",
            "Q": 0.10000000000000002,
            "N": 1,
            "reward": 0.1,
            "children": []
          }
        ]
      }
    ]
  }
}