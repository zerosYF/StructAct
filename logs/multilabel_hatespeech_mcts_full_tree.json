{
  "config": {
    "mcts_iters": 10,
    "rollout_length": 4,
    "depth_threshold": 5,
    "width_threshold": 3
  },
  "search_stats": {
    "total_nodes": 31,
    "total_Q_values": 31
  },
  "best_node": {
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "depth": 0,
    "Q": 2.5208333333333344,
    "N": 30
  },
  "search_tree": {
    "id": 0,
    "depth": 0,
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "Q": 2.5208333333333344,
    "N": 30,
    "reward": 0.1,
    "children": [
      {
        "id": 1,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert content moderation AI. Analyze the text input and classify it into hate speech categories. Output a valid JSON object with the following labels as integers (0/1):\n\n- `violence`: 1 ONLY for explicit threats, encouragement, or calls for physical harm/violence. Not for non-physical harm or systemic issues.\n- `directed_vs_generalized`: 1 if targeted at a SPECIFIC, named individual or very small, defined group. 0 for generalized attacks against broad protected categories.\n- `gender`: 1 for attacks based on gender or gender identity.\n- `race`: 1 for attacks based on perceived race, ethnicity, or skin color.\n- `national_origin`: 1 for attacks based on country, nation, or place of origin (e.g., \"Indians\", \"Norwegians\"). Distinct from race.\n- `disability`: 1 for attacks based on physical or mental disabilities.\n- `religion`: 1 for attacks based on religion or religious beliefs.\n- `sexual_orientation`: 1 for attacks based on sexual orientation.\n\n**Critical Guidelines:**\n- Apply definitions precisely. \"Directed\" requires specific targeting, not general groups.\n- Categories are NOT mutually exclusive; assign 1 to all that apply.\n- Output MUST be ONLY valid JSON, with no additional text.\n\nNow, classify the following text:\n\"{user_input}\"",
        "Q": 0.7750000000000001,
        "N": 10,
        "reward": 0.1,
        "children": [
          {
            "id": 2,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into the specified hate speech categories. Output MUST be a valid JSON object with the following keys, all present as integers (0 or 1):\n\n**Category Definitions:**\n- `violence`: 1 ONLY for explicit, direct threats, encouragement, or calls for physical harm/violence (e.g., \"kill yourself\", \"let's attack them\"). Do NOT classify idioms, hyperbolic language, or predictions of hypothetical consequences as violence (e.g., \"save your heads\" is 0).\n- `directed_vs_generalized`: 1 if the attack is targeted at a SPECIFIC, named individual or a very small, defined, named group (e.g., \"John Smith\", \"the Henderson family\"). 0 for attacks against broad, generalized groups or protected categories (e.g., \"women\", \"immigrants\").\n- `gender`: 1 for attacks based on gender identity (e.g., man, woman, non-binary, transgender). This is distinct from sexual orientation.\n- `race`: 1 for attacks based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian).\n- `national_origin`: 1 for attacks based on country, nation, or place of origin (e.g., \"Mexicans are lazy\"). Note: Merely mentioning a location (e.g., \"even in the UK\") is NOT an attack on origin unless it is the basis for the insult.\n- `disability`: 1 for attacks based on physical or mental disabilities or health conditions. This INCLUDES the use of slurs and pejoratives like \"retard\", \"crazy\", \"sicko\", and \"disgusting\" when used in this context.\n- `religion`: 1 for attacks based on religion or religious beliefs (e.g., Muslim, Christian, Jewish).\n- `sexual_orientation`: 1 for attacks based on sexual orientation (e.g., gay, lesbian, bisexual, straight). Attacks on the broader \"LGBTQ+ community\" or using terms like \"queer\" as a slur should be classified here, not under gender, unless gender identity is explicitly targeted.\n\n**Critical Guidelines:**\n- Apply definitions precisely. Categories are NOT mutually exclusive; assign 1 to all that apply based on the literal text.\n- Analyze the text in context. Do not infer intent that isn't explicitly stated.\n- Output MUST be ONLY valid JSON, with no additional text, thoughts, or formatting. All 8 keys must be present.\n\nNow, classify the following text:\n\"{user_input}\"",
            "Q": 0.3166666666666667,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 3,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into the specified hate speech categories. Output MUST be a valid JSON object with the following keys, all present as integers (0 or 1):\n\n**Category Definitions:**\n- `violence`: 1 ONLY for explicit, direct threats, encouragement, or calls for physical harm/violence (e.g., \"kill yourself\", \"let's attack them\"). Do NOT classify idioms, hyperbolic language, or predictions of hypothetical consequences as violence (e.g., \"save your heads\" is 0).\n- `directed_vs_generalized`: 1 **if and only if** the attack is targeted at a SPECIFIC, named individual (e.g., \"John Smith\") or a very small, named group that is identified by their proper name (e.g., \"the Henderson family\"). **ALWAYS 0 for attacks against broad, generalized groups or protected categories** (e.g., \"women\", \"immigrants\", \"black people\", \"gays\", \"Christians\"). The target must be named, not just described.\n- `gender`: 1 for attacks based on gender identity or expression (e.g., man, woman, non-binary, **transgender**). This is distinct from sexual orientation.\n- `race`: 1 for attacks based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). **This includes the use of racial slurs and stereotypes even if a location is mentioned** (e.g., \"go back to Africa\" is primarily a racial attack).\n- `national_origin`: 1 **ONLY** for attacks where country or place of origin is the **primary and explicit basis** for the insult (e.g., \"Mexicans are lazy\"). Merely mentioning a location is NOT an attack on origin. **If a location is used as part of a racial slur or stereotype, classify it as `race: 1`, not here.**\n- `disability`: 1 for attacks based on physical or mental disabilities or health conditions. This INCLUDES the use of slurs and pejoratives like \"retard\", \"crazy\", \"sicko\", and \"disgusting\" **when clearly used in the context of disability or mental health.**\n- `religion`: 1 for attacks based on religion or religious beliefs (e.g., Muslim, Christian, Jewish).\n- `sexual_orientation`: 1 for attacks based on sexual orientation (e.g., gay, lesbian, bisexual, straight). Attacks on the broader \"LGBTQ+ community\" or using terms like \"queer\" or \"f****t\" as a slur should be classified here, **unless gender identity is explicitly targeted (see `gender`).**\n\n**Critical Guidelines:**\n- **Apply definitions precisely and conservatively.** Categories are NOT mutually exclusive; assign 1 to all that apply based on the literal text and these explicit rules.\n- **Primary Basis Analysis:** When text could fit multiple categories, identify the primary basis of the attack. For example, \"Trans people are sick perverts\" is primarily a `gender` attack (against transgender identity) and secondarily could be `disability` (\"sick\"). Only the primary category should be 1 unless both are explicit and separate.\n- **Do not infer intent or context that isn't explicitly stated in the text.**\n- Output MUST be ONLY valid JSON, with no additional text, thoughts, or formatting. All 8 keys must be present.\n\nNow, classify the following text:\n\"{user_input}\"",
                "Q": 0.05,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 4,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into the specified hate speech categories. Output MUST be a valid JSON object with the following keys, all present as integers (0 or 1):\n\n**Category Definitions:**\n- `violence`: 1 ONLY for explicit, direct threats, encouragement, or calls for physical harm/violence (e.g., \"go kill yourself\", \"let's attack them\"). Do NOT classify idioms, hyperbolic language, or predictions of hypothetical consequences as violence (e.g., \"I'm so angry I could scream\" is 0).\n- `directed_vs_generalized`: 1 ONLY if the attack is targeted at a single, named individual or a very small, named group that is not a protected category (e.g., \"John Smith\", \"the Henderson family\"). 0 for ALL attacks against broad, generalized groups or protected categories (e.g., \"women\", \"immigrants\", \"Muslims\", \"gay people\", \"the French\"). An attack on any large demographic group is GENERALIZED.\n- `gender`: 1 ONLY for attacks based on gender identity or expression (e.g., man, woman, non-binary, transgender). This is distinct from sexual orientation. An attack on \"LGBTQ+\" or \"gay people\" is `sexual_orientation`, NOT `gender`, unless it explicitly targets gender identity.\n- `race`: 1 for attacks based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian).\n- `national_origin`: 1 for attacks based on country, nation, or place of origin (e.g., \"Mexicans are lazy\"). Note: Merely mentioning a location is NOT an attack on origin unless it is the basis for the insult.\n- `disability`: 1 for attacks based on physical or mental disabilities or health conditions. Use this category for slurs like \"retard\" or \"crazy\" ONLY when they are clearly directed at individuals with disabilities or used as a direct insult. Do NOT classify general hyperbolic language or broad prejudicial statements (e.g., \"X group is mentally ill\") under this category.\n- `religion`: 1 for attacks based on religion or religious beliefs (e.g., Muslim, Christian, Jewish).\n- `sexual_orientation`: 1 for attacks based on sexual orientation (e.g., gay, lesbian, bisexual, straight). Attacks on the broader \"LGBTQ+ community\" or using terms like \"queer\" as a slur should be classified here.\n\n**Critical Guidelines:**\n- Apply definitions precisely. Categories are NOT mutually exclusive; assign 1 to all that apply based on the literal text.\n- Analyze the text in context. Do not infer intent that isn't explicitly stated.\n- **CRUCIAL: An attack on any large demographic group (e.g., a religion, nationality, race, gender, or sexual orientation) is ALWAYS GENERALIZED (`directed_vs_generalized: 0`).**\n- Output MUST be ONLY valid JSON, with no additional text, thoughts, or formatting. All 8 keys must be present.\n\nNow, classify the following text:\n\"{user_input}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into the specified hate speech categories. Output MUST be a valid JSON object with the following keys, all present as integers (0 or 1):\n\n**Category Definitions:**\n- `violence`: 1 ONLY for explicit, direct threats, encouragement, or calls for physical harm/violence (e.g., \"kill yourself\", \"let's attack them\"). Do NOT classify idioms, hyperbolic language, or predictions of hypothetical consequences as violence (e.g., \"I'm so mad I could hit someone\" is 0).\n- `directed_vs_generalized`: 1 ONLY if the text attacks a specifically named individual (e.g., 'John Smith') or a uniquely identifiable, very small group (e.g., 'the Johnson family next door'). The target must be named or be so specifically described that they are unmistakable. Attacks using pronouns ('her') or general descriptors ('that woman') where the target is not uniquely identifiable should be 0.\n- `gender`: 1 for attacks based on gender identity (e.g., man, woman, non-binary, transgender). **Crucially, distinguish between general vulgarity and hate speech.** The use of gendered insults (e.g., 'cunt', 'bitch') alone does not automatically qualify as a gender-based attack. Only classify as 1 if the insult is explicitly tied to the target's gender or if the text attacks the gender group as a whole.\n- `race`: 1 for attacks based on perceived race, ethnicity, or skin color (e.g., Black, White, Asian). **Note:** Criticizing an ideology (e.g., 'white supremacism') is NOT an attack on a race.\n- `national_origin`: 1 for attacks based on country, nation, or place of origin (e.g., \"Mexicans are lazy\"). **Note:** Merely mentioning a location or group is NOT an attack. The group's origin must be the basis for the insult.\n- `disability`: 1 for attacks based on physical or mental disabilities or health conditions. This INCLUDES the use of slurs and pejoratives like \"retard\", \"crazy\", \"sicko\", and \"disgusting\" when used in this context.\n- `religion`: 1 for attacks based on religion or religious beliefs (e.g., Muslim, Christian, Jewish). **Note:** Criticizing a religious doctrine, institution, or extremist ideology is NOT an attack on all people of that religion.\n- `sexual_orientation`: 1 for attacks based on sexual orientation (e.g., gay, lesbian, bisexual, straight). Attacks on the broader \"LGBTQ+ community\" should be classified here. Only classify under `gender` if the attack explicitly and specifically targets gender identity (e.g., 'transgender people') separately from sexual orientation.\n\n**Critical Guidelines:**\n- Apply definitions precisely. Categories are NOT mutually exclusive; assign 1 to all that apply based on the literal text.\n- Analyze the text in context. Do not infer intent that isn't explicitly stated.\n- **Mentioning a group is not an attack. The group must be the target of the insult or harmful statement.**\n- Output MUST be ONLY valid JSON, with no additional text, thoughts, or formatting. All 8 keys must be present.\n\nNow, classify the following text:\n\"{user_input}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 6,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Analyze the provided text to determine if it constitutes a targeted attack. If it is an attack, classify it into the specific hate speech categories below. Output ONLY a valid JSON object with the following integer (0/1) labels:\n\n- `violence`: 1 ONLY for literal, explicit threats, encouragement, or calls for physical harm/violence. NOT for non-physical harm, systemic issues, or metaphorical language (e.g., \"kill the idea\").\n- `directed_vs_generalized`: 1 ONLY if the attack is targeted at a single, named individual or a very small, named group (e.g., \"my neighbor John\", \"the accounting team\"). 0 for attacks against broad, demographic categories (e.g., \"white people\", \"women\", \"Muslims\").\n- `gender`: 1 ONLY if the attack is primarily based on gender or gender identity. Not for general insults that incidentally use gendered language.\n- `race`: 1 for attacks based on perceived race, ethnicity, or skin color.\n- `national_origin`: 1 for attacks based on country, nation, or place of origin (e.g., \"Indians\"). Distinct from race.\n- `disability`: 1 for attacks based on physical or mental disabilities.\n- `religion`: 1 for attacks based on religion or religious beliefs.\n- `sexual_orientation`: 1 for attacks based on sexual orientation.\n\n**Critical Step-by-Step Reasoning (Internal):**\n1.  **Is this an attack?** If the text is merely profane, rude, or offensive but not directed as an attack against a person or group based on protected characteristics, output all 0s.\n2.  **For each category, ask: \"Is the attack PRIMARILY based on this characteristic?\"** The presence of a slur or profanity does not automatically mean a category is 1.\n3.  **Interpret language literally.** Discount metaphorical or hyperbolic language (e.g., \"kill them in the debate\" is not `violence: 1`).\n4.  **Apply the \"named\" rule for `directed_vs_generalized`.** If the target is not explicitly named or is a broad category, this label must be 0.\n\n**Output Instructions:**\n- The output must be a single, valid JSON object.\n- Do not include any other text, explanations, or formatting outside the JSON.\n\nNow, classify the following text:\n\"{user_input}\"",
            "Q": 0.31666666666666665,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text to determine if it constitutes a targeted attack. If it is an attack, classify it into the specific hate speech categories below. Output ONLY a valid JSON object with the following integer (0/1) labels:\n\n- `violence`: 1 for direct, indirect, or conditional threats, encouragement, or calls for physical harm/violence. This includes glorifying past violence with the intent to promote future violence. NOT for non-physical harm, systemic issues, or clear metaphorical/hyperbolic language (e.g., \"kill the idea\", \"I'm so angry I could explode\").\n- `directed_vs_generalized`: 1 if the attack is targeted at a specific, identifiable individual or a very small, specific group (e.g., \"my neighbor John\", \"the three managers on the 4th floor\"). 0 for attacks against large, demographic-based categories or unnamed groups (e.g., \"white people\", \"women\", \"Muslims\", \"all politicians\"). The target must be specified in the text, not just implied.\n- `gender`: 1 if the attack is based on gender or gender identity (including transphobic and non-binary phobic attacks). Not for general insults that only incidentally use gendered language (e.g., \"bitch\" used as a general insult).\n- `race`: 1 for attacks based on perceived race, ethnicity, or skin color. If the attack is based on nationality, use `national_origin`. If it's based on religion, use `religion`.\n- `national_origin`: 1 for attacks based on country, nation, or place of origin (e.g., \"Indians\", \"Mexicans\"). This takes precedence over `race` when the national origin is specified.\n- `disability`: 1 for attacks based on physical or mental disabilities.\n- `religion`: 1 for attacks based on religion or religious beliefs (e.g., \"Muslims\", \"Christians\").\n- `sexual_orientation`: 1 for attacks based on sexual orientation (e.g., \"gay people\").\n\n**Critical Step-by-Step Reasoning (Internal):**\n1.  **Is this an attack?** If the text is merely profane, rude, or offensive but not directed as an attack against a person or group based on protected characteristics, output all 0s.\n2.  **For each category, ask: \"Is the attack based on this characteristic?\"** An attack can be based on multiple characteristics; assign a 1 to every category that applies. The presence of a slur or profanity does not automatically mean a category is 1.\n3.  **Interpret language in context.** Discount clear metaphorical or hyperbolic language. Take descriptions of violence and conditional calls to action literally.\n4.  **Apply the \"specific and identifiable\" rule for `directed_vs_generalized`.** If the target is a broad category, this label must be 0. A group defined solely by a protected characteristic (e.g., \"autistic people\") is not \"specific\" for this category.\n\n**Output Instructions:**\n- The output must be a single, valid JSON object.\n- Do not include any other text, explanations, or formatting outside the JSON.\n\nNow, classify the following text:\n\"{user_input}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 8,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text to determine if it constitutes a targeted attack. A targeted attack is defined as language that expresses contempt, hatred, or violence towards a person or group, or which uses degrading stereotypes to insult them.\n\n**Step 1: Determine if it is an attack.** Mere profanity, rudeness, strong disagreement, factual criticism, or analytical statements (even if controversial or sweeping) are NOT considered attacks. If not an attack, output all zeros.\n\nIf it IS an attack, classify it into the specific hate speech categories below. Output ONLY a valid JSON object with the following integer (0/1) labels:\n\n- `violence`: 1 ONLY for literal, explicit threats, encouragement, or calls for physical harm/violence. NOT for non-physical harm, systemic issues, or metaphorical language.\n- `directed_vs_generalized`: 1 ONLY if the attack is targeted at a single, named individual or a very small, named group (e.g., \"my neighbor John\", \"the accounting team\"). 0 for attacks against broad, demographic categories.\n- `gender`: 1 ONLY if the attack is primarily based on gender or gender identity.\n- `race`: 1 ONLY if the attack is primarily based on perceived race, ethnicity, or skin color.\n- `national_origin`: 1 ONLY if the attack is primarily based on country, nation, or place of origin.\n- `disability`: 1 ONLY if the attack is primarily based on physical or mental disabilities.\n- `religion`: 1 ONLY if the attack is primarily based on religion or religious beliefs.\n- `sexual_orientation`: 1 ONLY if the attack is primarily based on sexual orientation.\n\n**Critical Step-by-Step Reasoning (Internal):**\n1.  **Literal Interpretation:** Discount metaphorical, hyperbolic, or ideological language. Analyze the literal meaning.\n2.  **Primary Basis Test:** For each category, ask: \"Is this characteristic the fundamental reason for the attack?\" The mere presence of a demographic keyword (e.g., \"white,\" \"gay\") does NOT warrant a 1. It must be the core basis of the hostility.\n3.  **Disentangle Characteristics:** If an attack references multiple traits (e.g., \"white supremacists\"), determine the primary target. Is the attack on a group *for being* that group, or is a characteristic used merely as a descriptor for an ideology or behavior? The latter does not count.\n4.  **Apply the \"named\" rule strictly:** `directed_vs_generalized` is 1 only for explicitly named individuals or tiny, specific groups.\n\n**Output Instructions:**\n- The output must be a single, valid JSON object.\n- Do not include any other text, explanations, or formatting outside the JSON.\n- If the text is **not** a targeted attack, output: `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}`\n\nNow, classify the following text:\n\"{user_input}\"",
                "Q": 0.07500000000000001,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 9,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text to determine if it constitutes a targeted attack. If it is an attack, classify it into the specific hate speech categories below. Output ONLY a valid JSON object with the following integer (0/1) labels:\n\n- `violence`: 1 ONLY for literal, explicit threats, encouragement, or calls for physical harm/violence. NOT for non-physical harm, systemic issues, or metaphorical language (e.g., \"kill the idea\"). Examples that are 1: \"I will shoot you\", \"Go kill yourself\".\n- `directed_vs_generalized`: 1 ONLY if the attack is targeted at a specific, identifiable individual or a very small, named group (e.g., \"my neighbor John\", \"the accounting team\", \"that woman over there\"). 0 for attacks against broad, unnamed demographic categories (e.g., \"white people\", \"women\", \"Muslims\").\n- `gender`: 1 ONLY if the attack is primarily based on gender or gender identity (e.g., attacks on women, men, or transgender people using identity-specific slurs). Not for general insults that incidentally use gendered language. Attacks focused on sexual attraction are for `sexual_orientation`.\n- `race`: 1 for attacks based on perceived race, ethnicity, or skin color.\n- `national_origin`: 1 for attacks based on country, nation, or place of origin (e.g., \"Indians\"). Distinct from race.\n- `disability`: 1 for attacks based on physical or mental disabilities.\n- `religion`: 1 for attacks based on religion or religious beliefs.\n- `sexual_orientation`: 1 for attacks based on sexual orientation (e.g., \"gay,\" \"lesbian\"). Distinct from gender identity.\n\n**Critical Step-by-Step Reasoning (Internal):**\n1.  **Is this a targeted attack?** If the text is merely profane, rude, offensive opinion, or non-targeted venting but not directed as an attack against a person or group based on protected characteristics, output all 0s.\n2.  **Interpret language literally.** Discount metaphorical or hyperbolic language (e.g., \"kill them in the debate\" is not `violence: 1`).\n3.  **Apply the \"specific and identifiable\" rule for `directed_vs_generalized`.** If the target is a broad demographic category, this label must be 0.\n4.  **For each category, ask: \"Is the attack PRIMARILY based on this characteristic?\"** Use characteristic-specific slurs as a strong indicator for the corresponding category.\n5.  **Resolve overlapping characteristics:** If an attack targets a subgroup (e.g., \"Black women\"), use the most specific applicable categories. Attacks on \"lesbians\" are primarily `sexual_orientation`, not `gender`. Attacks using transphobic slurs (e.g., \"shemale\") are `gender`.\n\n**Output Instructions:**\n- The output must be a single, valid JSON object.\n- Do not include any other text, explanations, or formatting outside the JSON.\n\nNow, classify the following text:\n\"{user_input}\"",
                "Q": 0.07500000000000001,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 10,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into specific hate speech categories. Your output must be a valid JSON object with the following integer labels (0 or 1):\n\n**Category Definitions (APPLY PRECISELY):**\n- `violence`: 1 ONLY for explicit threats, encouragement, or calls for physical harm/violence. Not for non-physical harm, hyperbole, or systemic issues.\n- `directed_vs_generalized`: 1 if the attack is aimed at a SPECIFIC, named individual or a group small enough to be individually identified (e.g., \"my coworkers,\" \"the Smith family\"). 0 for attacks against broad, protected groups (e.g., \"white people,\" \"women,\" \"gays\").\n- `race`: 1 for attacks primarily based on perceived race, ethnicity, or skin color.\n- `national_origin`: 1 for attacks primarily based on country, nation, or place of origin (e.g., \"Indians\", \"Norwegians\"). Distinct from race.\n- `religion`: 1 for attacks primarily based on religion or religious beliefs (e.g., \"all Muslims are terrorists\"). Not for insults that merely use religious words (e.g., \"you're a demon\").\n- `sexual_orientation`: 1 for attacks primarily based on sexual orientation (e.g., being gay, lesbian, bisexual).\n- `gender`: 1 for attacks primarily based on gender (e.g., misogyny, sexism against men or women).\n- `gender_identity`: 1 for attacks primarily based on gender identity or trans status (e.g., anti-transgender, non-binary, or gender non-conforming rhetoric).\n- `disability`: 1 for attacks primarily based on physical or mental disabilities.\n\n**Critical Instructions:**\n- Categories are NOT mutually exclusive. Assign 1 to ALL categories that are a primary basis for the attack.\n- Determine the *basis* of the attack, not just the words used.\n- If an attack targets transgender individuals, classify it as `gender_identity: 1`. Do not classify it as `sexual_orientation` or `gender` unless that is also a clear and separate basis.\n- Output MUST be ONLY the valid JSON object, with no additional text, explanations, or formatting.\n\nNow, classify the following text:\n\"{user_input}\"",
            "Q": 0.06666666666666667,
            "N": 1,
            "reward": 0.1,
            "children": []
          }
        ]
      },
      {
        "id": 11,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into hate speech categories using the following precise definitions.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a desire, intent, or threat of physical harm.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a specific individual or small, defined group. 0 if a generalized statement against a large, abstract group.\n- **gender (1/0)**: Hate is primarily and explicitly about gender (man, woman, non-binary). Do NOT mark for generic insults using gendered slurs unless gender is the clear primary motive.\n- **race (1/0)**: Hate is primarily and explicitly based on perceived race or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is primarily and explicitly based on country of origin, nationality, or citizenship status (e.g., immigrant, refugee). Distinct from race.\n- **disability (1/0)**: Hate is primarily and explicitly based on physical or mental disability.\n- **religion (1/0)**: Hate is primarily and explicitly based on religious affiliation.\n- **sexual_orientation (1/0)**: Hate is primarily and explicitly based on sexual orientation.\n\n### Critical Instructions:\n1. Identify the PRIMARY target and motivation. Only mark a category as 1 if the hate is explicitly and primarily about that characteristic.\n2. \"directed_vs_generalized\" applies to the targeting nature of the statement, not its content.\n3. Output MUST be a valid JSON object with the exact keys and integer values (0 or 1) shown below.\n\n### Output Format:\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
        "Q": 0.8833333333333335,
        "N": 10,
        "reward": 0.1,
        "children": [
          {
            "id": 12,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into hate speech categories using the following precise definitions.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a desire, intent, or threat of physical harm.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a *named individual* or a *small, specific, named group* (e.g., \"my neighbor John,\" \"the Smith family,\" \"the board of directors of Company X\"). 0 if targeted at a *large, demographic, or general group* (e.g., \"all women,\" \"black people,\" \"Muslims,\" \"immigrants\").\n- **gender (1/0)**: Hate is explicitly and primarily about gender identity (man, woman, non-binary, transgender). **DO NOT** mark for generic insults that use gendered slurs (e.g., \"bitch,\" \"shemale\") if the central reason for the hate is something else (e.g., sexual orientation, race).\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, or citizenship status (e.g., Mexican, immigrant, refugee). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability.\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation (e.g., Muslim, Jewish, Christian).\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n### CRITICAL RULES:\n1.  **PRIMARY MOTIVE:** Identify the SINGLE PRIMARY characteristic the hate is based on. A category is 1 *only* if that characteristic is the explicit and central reason for the attack.\n2.  **GENERIC INSULTS:** The presence of a slur does not automatically mean its category is 1. Analyze the motive. (e.g., \"Execute that gay bitch\" is primarily `sexual_orientation:1`, not `gender:1`. The word \"bitch\" is a generic insult here).\n3.  **TARGETING:** For `directed_vs_generalized`, \"small, specific, named group\" means a group that could be individually identified, not a vast demographic.\n\n### Examples for Clarity:\n- **\"All Muslims are terrorists\"** -> `directed_vs_generalized: 0` (large demographic group)\n- **\"My coworker Jane, who is a Muslim, is a terrorist\"** -> `directed_vs_generalized: 1` (specific, named individual)\n- **\"That f****t is a stupid bitch\"** -> `sexual_orientation: 1`, `gender: 0` (primary motive is sexual orientation, \"bitch\" is a generic insult)\n- **\"Women are too emotional to be leaders\"** -> `gender: 1` (primary motive is gender)\n\n### Output Format:\nYou MUST output a valid JSON object with the exact keys and integer values (0 or 1) shown below. Do not add any other text or commentary.\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
            "Q": 0.35000000000000003,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 13,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into hate speech categories using the following precise definitions.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a desire, intent, or threat of physical harm.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a *specific, identifiable individual* (e.g., using a name, \"you\" in a direct address, \"that person\") or a *small, specific, named group* (e.g., \"my neighbor John,\" \"the Smith family,\" \"the board of directors of Company X\"). 0 if targeted at a *large, demographic, or general group* (e.g., \"all women,\" \"black people,\" \"Muslims,\" \"immigrants\").\n- **gender (1/0)**: Hate is explicitly and primarily about gender identity (man, woman, non-binary). **DO NOT** mark for generic insults that use gendered slurs (e.g., \"bitch,\" \"shemale\") if the central reason for the hate is something else (e.g., sexual orientation, race).\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, or citizenship status (e.g., Mexican, immigrant, refugee). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability.\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation (e.g., Muslim, Jewish, Christian).\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation **or gender identity** (e.g., gay, lesbian, bisexual, **transgender, non-binary**).\n\n### CRITICAL RULES:\n1.  **PRIMARY MOTIVE:** Identify the SINGLE PRIMARY characteristic the hate is based on. A category is 1 *only* if that characteristic is the explicit and central reason for the attack.\n2.  **GENERIC INSULTS:** The presence of a slur does not automatically mean its category is 1. Analyze the motive. (e.g., \"Execute that gay bitch\" is primarily `sexual_orientation:1`, not `gender:1`. The word \"bitch\" is a generic insult here. The slur \"tranny\" attacks gender identity and belongs to `sexual_orientation`, not `gender`).\n3.  **TARGETING:** For `directed_vs_generalized`, a \"specific, identifiable individual\" includes those targeted by name, second-person pronoun (\"you\"), or demonstrative (\"this person\") in a context that implies a specific target.\n4.  **FULL PHRASE ANALYSIS:** Analyze the entire sentence to identify the true target. A phrase targeting \"all [group] like you\" is directed at a specific group built around \"you,\" not the general demographic.\n\n### Examples for Clarity:\n- **\"All Muslims are terrorists\"** -> `directed_vs_generalized: 0` (large demographic group)\n- **\"My coworker Jane, who is a Muslim, is a terrorist\"** -> `directed_vs_generalized: 1` (specific, named individual)\n- **\"That f****t is a stupid bitch\"** -> `sexual_orientation: 1`, `gender: 0` (primary motive is sexual orientation, \"bitch\" is a generic insult)\n- **\"Women are too emotional to be leaders\"** -> `gender: 1` (primary motive is gender)\n- **\"You are an ugly tranny\"** -> `directed_vs_generalized: 1` (targets \"you\"), `sexual_orientation: 1`, `gender: 0` (\"tranny\" attacks gender identity)\n- **\"All immigrants like you are criminals\"** -> `directed_vs_generalized: 1` (target is a specific group built around \"you\"), `national_origin: 1`\n- **\"That man is a disgusting f****t\"** -> `directed_vs_generalized: 1` (targets \"that man\"), `sexual_orientation: 1`, `gender: 0` (primary motive is sexual orientation)\n\n### Output Format:\nYou MUST output a valid JSON object with the exact keys and integer values (0 or 1) shown below. Do not add any other text or commentary.\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
                "Q": 0.07500000000000001,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 14,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into hate speech categories using the following precise definitions.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a desire, intent, or threat of physical harm.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a *specific, identifiable individual* (e.g., using \"you,\" a proper name, a username, or a very small named group like \"the board of Company X\"). 0 if targeted at a *large, demographic, or general group* (e.g., \"all women,\" \"black people,\" \"Muslims,\" \"immigrants,\" an entire country like \"Sweden\").\n- **gender (1/0)**: Hate is explicitly and primarily about gender identity (man, woman, non-binary, transgender). **DO NOT** mark for generic insults that use gendered slurs (e.g., \"bitch,\" \"shemale\") if the central reason for the hate is something else (e.g., sexual orientation, race).\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, or citizenship status (e.g., Mexican, immigrant, refugee). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability.\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation (e.g., Muslim, Jewish, Christian).\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n### CRITICAL RULES:\n1.  **PRIMARY MOTIVE:** Identify the SINGLE PRIMARY characteristic the hate is based on. A category is 1 *only* if that characteristic is the explicit and central reason for the attack. **If text seems to fit multiple categories (e.g., \"jewish race\"), choose the one that is most specific and aligns with common hate speech tropes (e.g., in this case, `religion` is preferred over `race` for antisemitic content).**\n2.  **GENERIC INSULTS:** The presence of a slur does not automatically mean its category is 1. Analyze the motive. (e.g., \"Execute that gay bitch\" is primarily `sexual_orientation:1`, not `gender:1`. The word \"bitch\" is a generic insult here).\n3.  **TARGETING:** For `directed_vs_generalized`, **a target is \"specific\" (1) if it is a single person (even if addressed as \"you\") or a group small enough that every member could be identified. A target is \"generalized\" (0) if it is a large demographic, nationality, or religious group.**\n4.  **DEMOGRAPHIC TERMS:** **If a demographic term (e.g., \"Caucasian,\" \"gay\") is used to attack a specific, directly addressed individual, mark it as `directed_vs_generalized: 1`. If it is used to attack the entire group, mark it as `directed_vs_generalized: 0`.**\n\n### Examples for Clarity:\n- **\"All Muslims are terrorists\"** -> `directed_vs_generalized: 0` (large demographic group)\n- **\"My coworker Jane, who is a Muslim, is a terrorist\"** -> `directed_vs_generalized: 1` (specific, named individual)\n- **\"You are a terrorist, Muslim\"** -> `directed_vs_generalized: 1` (specific individual addressed directly)\n- **\"That f****t is a stupid bitch\"** -> `sexual_orientation: 1`, `gender: 0` (primary motive is sexual orientation, \"bitch\" is a generic insult)\n- **\"Women are too emotional to be leaders\"** -> `gender: 1` (primary motive is gender)\n- **\"The Jewish people are a plague\"** -> `religion: 1`, `race: 0` (primary motive is religious/ethnic hatred, categorized under religion per historical context)\n- **\"Hey Sweden, your people are cowards\"** -> `national_origin: 1`, `directed_vs_generalized: 0` (targeting a whole nation)\n- **\"I wish you would cut your veins\"** -> `violence: 1`, `directed_vs_generalized: 1` (target is the specific individual \"you\")\n\n### Output Format:\nYou MUST output a valid JSON object with the exact keys and integer values (0 or 1) shown below. Do not add any other text or commentary.\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 15,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into hate speech categories using the following precise definitions.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a desire, intent, or threat of physical harm.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a *specific, identifiable individual or a very small, specific group*. This includes:\n    - A named person (e.g., \"my neighbor John\").\n    - A person identified by a pronoun in a direct address (e.g., \"you\").\n    - A small, named group (e.g., \"the Smith family\").\n    - An implied specific individual (e.g., \"that person,\" \"a [slur] like this\").\n  - 0 if targeted at a *large, demographic, or general group* (e.g., \"all women,\" \"immigrants\").\n- **gender (1/0)**: Hate is explicitly and primarily about gender identity (man, woman, non-binary). **NOTE:** Attacks focused on transgender identity are NOT `gender:1`; see `sexual_orientation`.\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, or citizenship status (e.g., Mexican, immigrant, refugee). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability. **Includes derogatory references to specific conditions (e.g., \"r*tard,\" misspelled variants like \"down syndrome\" used pejoratively)**.\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation (e.g., Muslim, Jewish, Christian).\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation (e.g., gay, lesbian, bisexual) **OR transgender identity**. This includes slurs like \"f****t\" and \"tranny\".\n\n### CRITICAL RULES:\n1.  **PRIMARY MOTIVE:** Identify the SINGLE PRIMARY characteristic the hate is based on. A category is 1 *only* if that characteristic is the explicit and central reason for the attack.\n2.  **GENERIC INSULTS vs. CHARACTERISTIC-SPECIFIC SLURS:** The presence of a slur does not automatically mean its category is 1.\n    - **Generic Insult:** A slur used as a general-purpose offensive term where the primary motive is elsewhere (e.g., \"Execute that gay **bitch**\" -> `sexual_orientation:1`, `gender:0`).\n    - **Characteristic-Specific Slur:** A slur that directly references and attacks the primary characteristic (e.g., \"That **tranny** is disgusting\" -> `sexual_orientation:1`).\n3.  **TARGETING:** For `directed_vs_generalized`, consider the context. A threat against \"you\" is directed (`1`). A comment about \"a [slur] like this\" (pointing to a specific instance) is directed (`1`).\n\n### Examples for Clarity:\n- **\"All Muslims are terrorists\"** -> `directed_vs_generalized: 0` (large group)\n- **\"My coworker Jane, who is a Muslim, is a terrorist\"** -> `directed_vs_generalized: 1` (named individual)\n- **\"You're a stupid r*tard\"** -> `directed_vs_generalized: 1` (targeted \"you\"), `disability: 1` (\"r*tard\" attacks disability)\n- **\"That f****t is a stupid bitch\"** -> `sexual_orientation: 1`, `gender: 0` (primary motive is sexual orientation, \"bitch\" is generic)\n- **\"I hate seeing trannies like this on my feed\"** -> `directed_vs_generalized: 1` (specific, implied target), `sexual_orientation: 1` (\"tranny\" attacks transgender identity)\n- **\"Incest baby that got double-down syndrome\"** -> `directed_vs_generalized: 1` (specific, hypothetical target), `disability: 1` (\"double-down syndrome\" is a derogatory ref. to Down syndrome)\n- **\"Try to talk to me about your god... thanks to my hand bitch\"** -> `directed_vs_generalized: 1` (targeted \"you\"), `violence: 1`, `religion: 1`, `gender: 0` (\"bitch\" is a generic insult)\n\n### Output Format:\nYou MUST output a valid JSON object with the exact keys and integer values (0 or 1) shown below. Do not add any other text or commentary.\n\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
                "Q": 0.07500000000000001,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 16,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Your task is to analyze the provided text and classify it into specific hate speech categories using the precise definitions below. Your analysis must focus on identifying the SINGLE PRIMARY characteristic that is the target of hatred.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a clear desire, intent, or threat of physical harm. Includes explicit words like \"kill,\" \"beat,\" \"hurt,\" but not mild metaphors or hyperbolic language.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a specific, named individual or a small, defined, identifiable group (e.g., \"my coworker John,\" \"the Smith family\"). 0 if a generalized statement against a large, abstract group (e.g., \"all women,\" \"immigrants,\" \"gay people\"). Rhetorical questions about groups are generalized (0).\n- **gender (1/0)**: Hate is explicitly and primarily about gender (e.g., misogyny, misandry). Do NOT mark for generic insults using gendered slurs (e.g., \"bitch\") unless the text's core motive is gender-based hatred.\n- **gender_identity (1/0)**: Hate is explicitly and primarily about transgender status, non-binary identity, or non-conformity to gender norms (e.g., denying someone's gender identity, anti-trans statements).\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race, ancestry, or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, immigration, or citizenship status (e.g., \"go back to your country,\" hatred against immigrants). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability.\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation, beliefs, or lack thereof (e.g., Muslim, Christian, Jewish, atheist).\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n### Critical Instructions for Accurate Classification:\n1.  **Primary Motivation Analysis:** Identify the SINGLE PRIMARY characteristic the hate is targeting. Only one category (besides violence and directed_vs_generalized) can be 1. The primary characteristic is the one most central to the argument and motivation for hatred.\n2.  **Context Over Keywords:** Analyze the full context and intent, not just the presence of individual slurs. A slur can be used in a context that is not primarily about that characteristic (e.g., a racial slur used in a statement primarily about national origin).\n3.  **Disambiguation for Overlap:** For groups with overlapping characteristics (e.g., Jewish, Roma), base your judgment on the context of the text. Prioritize the characteristic that is most clearly driving the hatred. References to religious figures or practices suggest religion; references to genetics, bloodlines, or \"race\" suggest race.\n4.  **Directed vs. Generalized is Separate:** This category describes the *targeting nature* of the statement and is independent of the content-based categories. A text can be both directed and violent, or generalized and about race, etc.\n5.  **Output MUST be a valid JSON object with the exact keys and integer values (0 or 1) shown below.**\n\n### Output Format:\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"gender_identity\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
            "Q": 0.35,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 17,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze the provided text in two distinct, mandatory steps:\n\n1.  **Targeting Analysis:** First, determine if the hatred is directed at a specific, identifiable target or a generalized group.\n2.  **Content Analysis:** Second, identify the SINGLE PRIMARY characteristic that is the core motivation for the hatred.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a clear desire, intent, or threat of physical harm. Includes explicit words like \"kill,\" \"beat,\" \"hurt,\" but not mild metaphors or hyperbolic language.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a specific, named individual OR a single, identifiable person/group referenced by a descriptor (e.g., \"my coworker John,\" \"the Smith family,\" \"this Norwegian,\" \"that woman\"). 0 if a generalized statement against a large, abstract group (e.g., \"all women,\" \"immigrants\"). Rhetorical questions about groups are generalized (0).\n- **gender (1/0)**: Hate is explicitly and primarily about gender (e.g., misogyny, misandry). Do NOT mark for generic insults using gendered slurs (e.g., \"bitch\") unless the text's core motive is gender-based hatred.\n- **gender_identity (1/0)**: Hate is explicitly and primarily about transgender status, non-binary identity, or non-conformity to gender norms (e.g., denying someone's gender identity, anti-trans statements).\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race, ancestry, or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, immigration, or citizenship status (e.g., \"go back to your country,\" hatred against immigrants). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability.\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation, beliefs, or lack thereof (e.g., Muslim, Christian, Jewish, atheist).\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n### Critical Instructions for Accurate Classification:\n1.  **Two-Step Analysis:** Always perform the Targeting Analysis (`directed_vs_generalized`) first, then the Content Analysis. These are independent; a text can be both `directed_vs_generalized: 1` and `religion: 1`.\n2.  **Primary Motivation:** For the content categories, identify the SINGLE PRIMARY characteristic the hate is targeting. Only one content category can be 1. The primary characteristic is the one most central to the argument and motivation for hatred.\n3.  **Context Over Keywords:** Analyze the full context and intent, not just the presence of individual slurs. A slur can be used in a context that is not primarily about that characteristic.\n4.  **Pronouns are Critical:** Words like \"her,\" \"him,\" \"this,\" or \"that\" are strong indicators of a specific, identifiable target (`directed_vs_generalized: 1`), even if the primary hatred is based on a group identity like religion or national origin.\n5.  **Disambiguation for Overlap:** For groups with overlapping characteristics (e.g., Jewish, Roma), base your judgment on the context. References to religious figures or practices suggest religion; references to genetics, bloodlines, or \"race\" suggest race.\n6.  **Output MUST be a valid JSON object with the exact keys and integer values (0 or 1) shown below. All keys must be present.**\n\n### Output Format:\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"gender_identity\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 18,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze the provided text and classify it into specific hate speech categories using the precise definitions below. Your analysis must focus on identifying the SINGLE PRIMARY characteristic that is the target of hatred.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a clear desire, intent, or threat of physical harm. Includes explicit words like \"kill,\" \"beat,\" \"hurt,\" but not mild metaphors or hyperbolic language.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a specific, named individual or a small, defined, identifiable group (e.g., \"my coworker John,\" \"the Smith family\"). 0 if a generalized statement against a large, abstract group (e.g., \"all women,\" \"immigrants,\" \"gay people\"). Rhetorical questions about groups are generalized (0).\n- **gender (1/0)**: Hate is explicitly and primarily about gender (e.g., misogyny, misandry). Do NOT mark for generic insults using gendered slurs (e.g., \"bitch\") unless the text's core motive is gender-based hatred. Distinct from gender identity.\n- **gender_identity (1/0)**: Hate is explicitly and primarily about transgender status, non-binary identity, or non-conformity to gender norms (e.g., denying someone's gender identity, anti-trans statements).\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race, ancestry, or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, immigration, or citizenship status (e.g., \"go back to your country,\" hatred against immigrants). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability.\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation, beliefs, or lack thereof (e.g., Muslim, Christian, Jewish, atheist).\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n### Critical Instructions for Accurate Classification:\n1.  **Primary Motivation Analysis:** Identify the SINGLE PRIMARY characteristic the hate is targeting. Only one content-based category (gender through sexual_orientation) can be 1. The primary characteristic is the one most central to the argument and motivation for hatred.\n2.  **Context Over Keywords:** Analyze the full context and intent, not just the presence of individual slurs. A slur can be used in a context that is not primarily about that characteristic.\n3.  **Disambiguation for Overlap:** For groups with overlapping characteristics (e.g., Jewish, Roma), base your judgment on the context. Prioritize the characteristic that is most clearly driving the hatred. References to religious figures or practices suggest religion; references to genetics, bloodlines, or \"race\" suggest race.\n4.  **Directed vs. Generalized is Separate:** This category describes the *targeting nature* of the statement and is independent of the content-based categories.\n5.  **Output Keys:** Your output MUST be a valid JSON object. Include **only** the following keys that are applicable:\n    - `violence` (always include, set to 1 or 0)\n    - `directed_vs_generalized` (always include, set to 1 or 0)\n    - **Only one** content-based category key (from gender to sexual_orientation), set to 1. **Do not include any other content-based categories in the output.**\n\n### Examples:\n**Text:** \"All women are terrible drivers and should be banned from the roads.\"\n**Output:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 1}`\n\n**Text:** \"That transgender person is a disgusting freak and an abomination.\"\n**Output:** `{\"violence\": 0, \"directed_vs_generalized\": 1, \"gender_identity\": 1}`\n\n**Text:** \"I'm going to find and kill you, John.\"\n**Output:** `{\"violence\": 1, \"directed_vs_generalized\": 1}`\n\nText to classify:\n\"{text}\"",
                "Q": 0.05,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 19,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderation AI. Your task is to analyze the provided text and classify it into specific hate speech categories using the precise definitions below. Your analysis must focus on identifying the SINGLE PRIMARY characteristic that is the target of hatred.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a clear desire, intent, or threat of physical harm. Includes explicit words like \"kill,\" \"beat,\" \"hurt,\" but not mild metaphors or hyperbolic language.\n- **directed_vs_generalized (1/0)**: 1 if targeted at a specific, named individual or a small, defined, identifiable group (e.g., \"my coworker John,\" \"the Smith family\"). 0 if a generalized statement against a large, abstract group (e.g., \"all women,\" \"immigrants,\" \"gay people\"). Rhetorical questions about groups are generalized (0).\n- **gender (1/0)**: Hate is explicitly and primarily about gender (e.g., misogyny, misandry). Do NOT mark for generic insults using gendered slurs (e.g., \"bitch\") unless the text's core motive is gender-based hatred.\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race, ancestry, or ethnicity (e.g., Black, White, Asian). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, immigration, or citizenship status (e.g., \"go back to your country,\" hatred against immigrants). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability.\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation, beliefs, or lack thereof (e.g., Muslim, Christian, Jewish, atheist).\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n### Critical Instructions for Accurate Classification:\n1.  **Primary Motivation Analysis:** Identify the SINGLE PRIMARY characteristic the hate is targeting. Only one content-based category (gender, race, national_origin, disability, religion, sexual_orientation) can be 1. The `violence` and `directed_vs_generalized` categories are independent and can be 1 in combination with a content category.\n2.  **Context Over Keywords:** Analyze the full context and intent, not just the presence of individual slurs. A slur can be used in a context that is not primarily about that characteristic (e.g., a racial slur used in a statement primarily about national origin).\n3.  **Disambiguation for Overlap:** For groups with overlapping characteristics (e.g., Jewish, Roma), base your judgment on the context of the text. Prioritize the characteristic that is most clearly driving the hatred. References to religious figures or practices suggest religion; references to genetics, bloodlines, or \"race\" suggest race.\n4.  **No Hate Found:** If the text does not contain hate speech against any of these protected characteristics, set all content categories to 0.\n5.  **Output MUST be a valid JSON object with the exact keys and integer values (0 or 1) shown below.**\n\n### Output Format:\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 20,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderation AI. Analyze the provided text and classify it into hate speech categories using the following precise definitions.\n\n### Category Definitions:\n- **violence (1/0)**: Text expresses a clear desire, intent, or threat of physical harm. Includes wishes like \"I hope they get hurt.\"\n- **directed_vs_generalized (1/0)**: `1` (Directed) only if the text names a specific person (e.g., \"John Smith\") or a uniquely identifiable small group (e.g., \"the Smith family\"). `0` (Generalized) if the target is a general group (e.g., \"women\") or an unspecified member of a group (e.g., \"that trans person over there\").\n- **gender (1/0)**: Hate is explicitly and primarily about gender (e.g., \"women are inferior\"). This category is for misogyny and misandry. Do NOT mark for generic insults using gendered language (e.g., \"bitch\") unless the text's core hatred is about gender.\n- **race (1/0)**: Hate is explicitly and primarily based on perceived race or ethnicity (e.g., \"Black people are criminals\"). Distinct from national origin.\n- **national_origin (1/0)**: Hate is explicitly and primarily based on country of origin, nationality, or citizenship status (e.g., \"Mexicans are stealing our jobs\"). Distinct from race.\n- **disability (1/0)**: Hate is explicitly and primarily based on physical or mental disability (e.g., \"retards are useless\").\n- **religion (1/0)**: Hate is explicitly and primarily based on religious affiliation (e.g., \"I hate Muslims\"). A text criticizing religious doctrines or accusing a religion of bad acts is not necessarily hate speech against its followers.\n- **sexual_orientation (1/0)**: Hate is explicitly and primarily based on sexual orientation or gender identity (e.g., \"fags should die\", \"trannies are disgusting\"). This includes homophobic and transphobic content. If a text contains both a homophobic/transphobic slur AND a generic gendered insult, the primary motive is sexual_orientation.\n\n### Critical Instructions:\n1.  **Analyze the intent and target.** The text must be expressing hate *towards* a group based on a protected characteristic. A text discussing hate (e.g., \"Group A is racist\") is not itself hate speech against Group A.\n2.  **Identify the PRIMARY motive.** If multiple categories could apply, choose the one that is the most specific and central to the hatred. Slurs related to sexual orientation/gender identity trump generic gendered insults.\n3.  **Beware of keyword triggers.** Base your judgment on the overall meaning, not the presence of a single charged word.\n4.  Output MUST be a valid JSON object with the exact keys and integer values (0 or 1) shown below.\n\n### Output Format:\n{\n  \"violence\": 0,\n  \"directed_vs_generalized\": 0,\n  \"gender\": 0,\n  \"race\": 0,\n  \"national_origin\": 0,\n  \"disability\": 0,\n  \"religion\": 0,\n  \"sexual_orientation\": 0\n}\n\nText to classify:\n\"{text}\"",
            "Q": 0.08333333333333333,
            "N": 1,
            "reward": 0.1,
            "children": []
          }
        ]
      },
      {
        "id": 21,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based on the explicit content of the text. Apply the following precise definitions and avoid over-generalization or inference beyond what is directly stated.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: The text explicitly threatens, encourages, or calls for physical harm, injury, or death. Do NOT mark for mere mentions of violence without a direct call to action.\n- **directed_vs_generalized (1)**: The hate specifically targets a named individual or small, identified group. Mark as 0 for large, general groups (e.g., \"all Muslims,\" \"immigrants\").\n- **gender (1)**: Based on gender or gender identity (e.g., woman, man, transgender).\n- **race (1)**: Based on phenotypic characteristics like skin color or ethnicity (e.g., Black, White). Note: \"Jewish\" is typically `religion`, not `race`.\n- **national_origin (1)**: Based on country of origin, citizenship, or nationality (e.g., Mexican, Indian, refugee, immigrant).\n- **disability (1)**: Based on physical, mental, or developmental disability.\n- **religion (1)**: Based on religious affiliation, beliefs, or practices (e.g., Muslim, Christian, Jewish, Hindu).\n- **sexual_orientation (1)**: Based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n### RULES:\n1.  Base judgments strictly on literal text and explicit meaning. Avoid inferring categories not directly supported.\n2.  Pay close attention to distinctions between `race`, `national_origin`, and `religion`.\n3.  A text can have multiple labels set to 1. If a category does not apply, set it to 0.\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys and integer values (1 or 0). No other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"I will find you and break your legs, you cheating scum.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
        "Q": 0.8624999999999999,
        "N": 10,
        "reward": 0.05,
        "children": [
          {
            "id": 22,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based strictly on the explicit and literal content of the text.\n\n### CRITICAL INSTRUCTIONS:\n1.  **STRICT LITERAL INTERPRETATION:** Avoid all inference, implication, and reading between the lines. If a category is not explicitly and unambiguously stated, mark it as 0.\n2.  **METAPHOR & SLANG:** Do NOT categorize metaphorical, hyperbolic, or slang usage of words. For example, \"demon\" used as a general insult is not `religion`. \"Crazy\" used to mean \"unbelievable\" is not `disability`.\n3.  **CORE REASON FOR HATE:** A category should only be marked 1 if the protected characteristic is the explicit core reason for the hostility. An insult that happens to use a characteristic is not sufficient.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: The text explicitly and directly threatens, encourages, or calls for physical harm, injury, or death. Exclude hyperbolic expressions and vague warnings.\n- **directed_vs_generalized (1)**: The hate specifically targets a named individual or a small, well-defined group (e.g., \"my coworkers,\" \"the Johnson family\"). Mark as 0 for large, general groups (e.g., \"all Muslims,\" \"immigrants\").\n- **gender (1)**: Explicitly based on gender or gender identity (e.g., \"because she is a woman,\" \"dumb man\").\n- **race (1)**: Explicitly based on phenotypic characteristics like skin color or ethnicity (e.g., Black, White). Note: \"Jewish\" is `religion`, not `race`.\n- **national_origin (1)**: Explicitly based on country of origin, citizenship, or nationality (e.g., \"Mexican,\" \"immigrant,\" \"refugee\").\n- **disability (1)**: Explicitly based on physical, mental, or developmental disability (e.g., \"retarded,\" \"cripple\"). Exclude medical diagnoses or general insults like \"idiot\" without a disability context.\n- **religion (1)**: Explicitly based on religious affiliation, beliefs, or practices (e.g., \"Muslim,\" \"Christian,\" \"atheist\"). Exclude general religious terminology used metaphorically (e.g., \"demon,\" \"sinner\").\n- **sexual_orientation (1)**: Explicitly based on sexual orientation (e.g., \"gay,\" \"lesbian,\" \"f****t\").\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys and integer values (1 or 0). No explanations, no other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"I will find you and break your legs, you cheating scum.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"She is heavily relying on him to turn the other cheek...tough talking demon infested woman.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
            "Q": 0.3833333333333333,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 23,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based strictly on the explicit and literal content of the text.\n\n### CRITICAL INSTRUCTIONS:\n1.  **STRICT LITERAL INTERPRETATION:** Avoid all inference, implication, and reading between the lines. If a category is not explicitly and unambiguously stated, mark it as 0.\n2.  **METAPHOR & SLANG:** Do NOT categorize metaphorical, hyperbolic, or slang usage of words. For example, \"demon\" used as a general insult is not `religion`. \"Crazy\" used to mean \"unbelievable\" is not `disability`.\n3.  **TARGET IDENTIFICATION FIRST:** First identify the target of the hostility. If the target is explicitly named using a protected characteristic (e.g., \"gay people,\" \"Jews,\" \"women\"), that corresponding category must be marked as 1. The nature of the insult does not change the category of the target.\n4.  **CORE REASON FOR HATE:** A category should only be marked 1 if the protected characteristic is the explicit core reason for the hostility. An insult that happens to use a characteristic is not sufficient.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: The text explicitly and directly threatens, encourages, or calls for physical harm, injury, or death against a person or group. Exclude hyperbolic expressions and vague warnings.\n- **directed_vs_generalized (1)**: The hate specifically targets a named individual, a person directly addressed with \"you\"/\"your,\" or a small, well-defined group (e.g., \"my coworkers,\" \"the Johnson family\"). Mark as 0 for large, general groups (e.g., \"all Muslims,\" \"immigrants\") or groups defined by universal quantifiers like \"all,\" \"every,\" \"any.\"\n- **gender (1)**: Explicitly based on gender or gender identity (e.g., \"because she is a woman,\" \"dumb man\").\n- **race (1)**: Explicitly based on phenotypic characteristics like skin color or ethnicity (e.g., Black, White). Note: \"Jewish\" is `religion`, not `race`.\n- **national_origin (1)**: Explicitly based on country of origin, citizenship, or nationality (e.g., \"Mexican,\" \"immigrant,\" \"refugee\").\n- **disability (1)**: Explicitly based on physical, mental, or developmental disability (e.g., \"retarded,\" \"cripple\"). Exclude medical diagnoses or general insults like \"idiot\" without a disability context.\n- **religion (1)**: Explicitly based on religious affiliation, beliefs, or practices (e.g., \"Muslim,\" \"Christian,\" \"atheist\"). Exclude general religious terminology used metaphorically (e.g., \"demon,\" \"sinner\").\n- **sexual_orientation (1)**: Explicitly based on sexual orientation (e.g., \"gay,\" \"lesbian,\" \"f****t\").\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys and integer values (1 or 0). No explanations, no other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"I will find you and break your legs, you cheating scum.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"She is heavily relying on him to turn the other cheek...tough talking demon infested woman.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"LGBTQ+ people are mentally ill.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"You Christians are all fools.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":1,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 24,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based strictly on the explicit and literal content of the text.\n\n### CRITICAL INSTRUCTIONS:\n1.  **STRICT LITERAL INTERPRETATION:** Avoid all inference, implication, and reading between the lines. If a category is not explicitly and unambiguously stated, mark it as 0.\n2.  **METAPHOR & SLANG:** Do NOT categorize metaphorical, hyperbolic, or slang usage of words. For example, \"demon\" used as a general insult is not `religion`. \"Crazy\" used to mean \"unbelievable\" is not `disability`.\n3.  **CORE REASON FOR HATE:** A category should only be marked 1 if the protected characteristic is the explicit core reason for the hostility. An insult that happens to use a characteristic is not sufficient.\n4.  **INSULTS VS. HATE:** An insult directed at a member of a group is not automatically hate speech. For example, \"you are stupid\" said to a woman is not `gender` unless it is explicitly because she is a woman (e.g., \"you are stupid like all women\"). The characteristic must be the root of the criticism.\n5.  **MULTIPLE CHARACTERISTICS:** If a text references multiple characteristics, classify only those that are explicitly named as the reason for animosity. Priority Rule: If a racial descriptor (e.g., \"Pale,\" \"Black\") is used, classify as `race`, not `national_origin`.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: The text explicitly and directly threatens, encourages, or calls for physical harm, injury, or death. Exclude hyperbolic expressions and vague warnings.\n- **directed_vs_generalized (1)**: The hate specifically targets a named individual or a small, well-defined group (e.g., \"my coworkers,\" \"the Johnson family\"). Mark as 0 for large, general groups (e.g., \"all Muslims,\" \"immigrants\").\n- **gender (1)**: Hostility is explicitly and solely based on gender or gender identity (e.g., \"because she is a woman,\" \"inferior because they are men\"). Exclude general insults like \"ugly\" or \"bitch\" that are directed at a gender but are not explicitly about the gender itself.\n- **race (1)**: Explicitly based on phenotypic characteristics like skin color or ethnicity (e.g., Black, White). Note: \"Jewish\" is `religion`, not `race`.\n- **national_origin (1)**: Explicitly based on country of origin, citizenship, or nationality (e.g., \"Mexican,\" \"immigrant,\" \"refugee\").\n- **disability (1)**: Explicitly based on physical, mental, or developmental disability (e.g., \"retarded,\" \"cripple\"). Exclude medical diagnoses or general insults like \"idiot\" without a disability context.\n- **religion (1)**: Explicitly based on religious affiliation, beliefs, or practices (e.g., \"Muslim,\" \"Christian,\" \"atheist\"). Exclude general religious terminology used metaphorically (e.g., \"demon,\" \"sinner\").\n- **sexual_orientation (1)**: Explicitly based on sexual orientation (e.g., \"gay,\" \"lesbian,\" \"f****t\").\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys and integer values (1 or 0). No explanations, no other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"I will find you and break your legs, you cheating scum.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"She is heavily relying on him to turn the other cheek...tough talking demon infested woman.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"You're a fat, ugly bitch.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"That Black woman from Haiti is stealing our jobs.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":1,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 25,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based strictly on the explicit and literal content of the text.\n\n### CRITICAL INSTRUCTIONS:\n1.  **STRICT LITERAL INTERPRETATION:** Avoid all inference, implication, and reading between the lines. If a category is not explicitly and unambiguously stated, mark it as 0. **However, recognize that slurs and epithets directed at protected groups (e.g., \"f****t\", \"tranny\", \"n****r\") are explicit and literal indicators of hate.**\n2.  **METAPHOR & SLANG:** Do NOT categorize metaphorical, hyperbolic, or slang usage of words. For example, \"demon\" used as a general insult is not `religion`. \"Crazy\" used to mean \"unbelievable\" is not `disability`.\n3.  **CORE REASON FOR HATE:** A category should only be marked 1 if the protected characteristic is the explicit core reason for the hostility. An insult that happens to use a characteristic is not sufficient. **Attacks on concepts or movements related to a protected characteristic (e.g., \"feminism\") are not sufficient unless the characteristic itself is explicitly named.**\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: The text explicitly and directly threatens, encourages, or calls for physical harm, injury, or death. Exclude hyperbolic expressions and vague warnings.\n- **directed_vs_generalized (1)**: The hate specifically targets a named individual or a small, well-defined group (e.g., \"my coworkers,\" \"the Johnson family\"). Mark as 0 for large, general groups (e.g., \"all Muslims,\" \"immigrants\").\n- **gender (1)**: Explicitly based on biological sex (e.g., \"because she is a woman,\" \"dumb man\"). **Does not include gender identity.**\n- **race (1)**: Explicitly based on phenotypic characteristics like skin color or ethnicity (e.g., Black, White). Note: \"Jewish\" is `religion`, not `race`.\n- **national_origin (1)**: Explicitly based on country of origin, citizenship, or nationality (e.g., \"Mexican,\" \"immigrant,\" \"refugee\").\n- **disability (1)**: Explicitly based on physical, mental, or developmental disability (e.g., \"retarded,\" \"cripple\"). Exclude medical diagnoses or general insults like \"idiot\" without a disability context.\n- **religion (1)**: Explicitly based on religious affiliation, beliefs, or practices (e.g., \"Muslim,\" \"Christian,\" \"atheist\"). Exclude general religious terminology used metaphorically (e.g., \"demon,\" \"sinner\").\n- **sexual_orientation (1)**: Explicitly based on sexual orientation (e.g., \"gay,\" \"lesbian,\" \"f****t\"). **Does not include gender identity.**\n- **gender_identity (1)**: **Explicitly based on gender identity (e.g., \"transgender,\" \"non-binary,\" \"tranny\").**\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys and integer values (1 or 0). The keys are: `violence`, `directed_vs_generalized`, `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`, `gender_identity`. No explanations, no other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1,\"gender_identity\":0}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"I will find you and break your legs, you cheating scum.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"She is heavily relying on him to turn the other cheek...tough talking demon infested woman.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"Feminism is a very terrible disease\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":0}\n\nInput: \"This is exactly how I feel like when I see a tranny like this disgusting af\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0,\"gender_identity\":1}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 26,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based on the explicit and literal content of the text. Apply the following precise definitions and avoid over-generalization or inference beyond what is directly stated.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: The text explicitly threatens, encourages, or calls for physical harm, injury, or death. Do NOT mark for mere mentions of violence without a direct call to action.\n- **directed_vs_generalized (1)**: The hate specifically targets a named individual or a group small enough to be individually identified (e.g., \"my coworkers,\" \"the Smith family,\" \"that gay guy Mark\"). Mark as 0 for large, general groups (e.g., \"all Muslims,\" \"immigrants,\" \"white people,\" \"any man\"). If the text uses a second-person address (e.g., \"you people,\" \"kneel down, white boy\"), treat it as targeting the listener and mark as 1.\n- **gender (1)**: Based on gender or gender identity (e.g., woman, man, transgender, \"bitch\").\n- **race (1)**: Based on phenotypic characteristics like skin color or ethnicity (e.g., Black, White, Asian). CRITICAL RULE: The term 'Jewish' refers to religion, not race. Classify it under `religion`.\n- **national_origin (1)**: Based on country of origin, citizenship, or nationality (e.g., Mexican, Indian, refugee, immigrant).\n- **disability (1)**: Based on physical, mental, or developmental disability.\n- **religion (1)**: Based on religious affiliation, beliefs, or practices (e.g., Muslim, Christian, Jewish, Hindu).\n- **sexual_orientation (1)**: Based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n### RULES:\n1.  Base judgments strictly on literal text and explicit meaning. The only exception is the rule for 'Jewish' which must always be classified as `religion`. Avoid inferring other categories not directly supported.\n2.  Pay close attention to distinctions between `race`, `national_origin`, and `religion`.\n3.  A text can have multiple labels set to 1. If a category does not apply, set it to 0.\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys and integer values (1 or 0). No other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"I will find you and break your legs, you cheating scum.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"kneel down, white boy. its the era of brown people now\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":1,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"I wouldn’t hire any male bitch at my shop.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"The Jewish people have a right to their land.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":1,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
            "Q": 0.06666666666666667,
            "N": 1,
            "reward": 0.1,
            "children": []
          },
          {
            "id": 27,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based strictly on the explicit and literal content. Avoid all over-generalization or inference beyond what is directly stated.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: Explicitly threatens, encourages, or calls for physical harm, injury, or death. Not for mere mentions of violence.\n- **directed_vs_generalized (1)**: Targets a named individual or a specific, identifiable person (e.g., \"that Mexican guy\"). Mark 0 for vague, large groups (e.g., \"all immigrants\"). A specific person attacked via a group identifier is directed (1).\n- **gender (1)**: Based on gender or gender identity (e.g., woman, man, transgender). Includes misogyny, misandry, and transphobic slurs.\n- **race (1)**: Based on skin color or ethnicity (e.g., Black, White). Note: \"Jewish\" is typically `religion`.\n- **national_origin (1)**: Based on country of origin, citizenship, or nationality (e.g., Mexican, Norwegian). Only mark if the hate is explicitly because of origin.\n- **disability (1)**: Based on physical, mental, or developmental disability.\n- **religion (1)**: Based on religious affiliation, beliefs, or practices (e.g., Muslim, Christian).\n- **sexual_orientation (1)**: Based on sexual orientation (e.g., gay, lesbian). Includes homophobic and biphobic slurs.\n\n### CRITICAL RULES:\n1.  The hate must be directly *because of* the category. If the link is not explicit, mark 0.\n2.  A slur is a strong indicator for its corresponding category.\n3.  Pay close attention to distinctions between `race`, `national_origin`, and `religion`.\n4.  A text can have multiple labels set to 1. If a category does not apply, set it to 0.\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys in this order: `\"violence\"`, `\"directed_vs_generalized\"`, `\"gender\"`, `\"race\"`, `\"national_origin\"`, `\"disability\"`, `\"religion\"`, `\"sexual_orientation\"`. Each key must have an integer value of 1 or 0. No other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"That Norwegian is an idiot.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Get lost, you shemale freak.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
            "Q": 0.325,
            "N": 4,
            "reward": 0.1,
            "children": [
              {
                "id": 28,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based strictly on the explicit and literal content. Your analysis must be based solely on the words present in the text. Avoid all over-generalization, inference beyond what is directly stated, or use of external knowledge.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: Explicitly threatens, encourages, or calls for physical harm, injury, death, or self-harm (e.g., \"go kill yourself,\" \"you should be hurt\"). Not for mere mentions of violence.\n- **directed_vs_generalized (1)**: The text explicitly names a specific individual (e.g., \"John Smith\") or uses a unique identifier that points to one person (e.g., \"that Mexican guy on the 3rd floor\"). If the target is a group (e.g., \"my Mexican neighbors\") or an unnamed individual, mark 0.\n- **gender (1)**: Hate is directly because of gender or gender identity (e.g., woman, man, transgender). Includes misogyny, misandry, and transphobic slurs.\n- **race (1)**: Hate is directly because of skin color or ethnicity (e.g., Black, White). Note: \"Jewish\" is typically `religion`.\n- **national_origin (1)**: Hate is directly because of country of origin, citizenship, or nationality (e.g., Mexican, Indian, Norwegian).\n- **disability (1)**: Hate is directly because of physical, mental, or developmental disability.\n- **religion (1)**: Hate is directly because of religious affiliation, beliefs, or practices (e.g., Muslim, Christian, Hindu).\n- **sexual_orientation (1)**: Hate is directly because of sexual orientation (e.g., gay, lesbian). Includes homophobic and biphobic slurs.\n\n### CRITICAL RULES:\n1.  The hate must be directly *because of* the category. If the link is not explicit, mark 0.\n2.  A slur is a strong indicator for its corresponding category.\n3.  Pay close attention to distinctions between `race`, `national_origin`, and `religion`.\n4.  A text can have multiple labels set to 1. If a category does not apply, set it to 0.\n5.  For `directed_vs_generalized`, if a name or specific, unique identifier is not present, mark it as 0. Do not infer the existence of a specific target.\n6.  The word \"racist\" or \"racism\" does not automatically mean the category is `race`. Analyze the target. If the target is a nationality (e.g., Indian) or religion (e.g., Muslim), it is likely `national_origin` or `religion`.\n7.  A single phrase may contain elements of multiple categories. Mark ALL categories that are explicitly present based on the definitions.\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys in this order: `\"violence\"`, `\"directed_vs_generalized\"`, `\"gender\"`, `\"race\"`, `\"national_origin\"`, `\"disability\"`, `\"religion\"`, `\"sexual_orientation\"`. Each key must have an integer value of 1 or 0. No other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Go bleach your skin, you ugly black bitch.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":0,\"gender\":1,\"race\":1,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Those racist Indians are destroying our country.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
                "Q": 0.1,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 29,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories. Your analysis must be grounded on the explicit and literal content, but you must also interpret common coded language and metaphors for hate (e.g., \"should be gassed\" implies violence, \"turned into a drive-in\" is a homophobic slur). Avoid over-generalization or inference about the author's intent beyond the text.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: Explicitly threatens, encourages, or calls for physical harm, injury, or death. Includes common metaphors for harm (e.g., \"should be gassed,\" \"go die\").\n- **directed_vs_generalized (1)**: Targets a named individual or a specific, identifiable person (e.g., \"that Mexican guy\"). Mark 0 for vague, large groups (e.g., \"all immigrants\"). A specific person attacked via a group identifier is directed (1).\n- **gender (1)**: Hate directed at someone *because of* their gender or gender identity (e.g., \"women are stupid\", \"tranny freak\"). (Note: Attacks on \"lesbians\" are typically `sexual_orientation`, not `gender`, unless the attack is specifically about their gender).\n- **race (1)**: Hate directed at someone *because of* their perceived race or skin color (e.g., Black, White). Note: \"Jewish\" is typically `religion`.\n- **national_origin (1)**: Hate directed at someone *because of* their country of origin, citizenship, or nationality (e.g., \"Mexicans are lazy\"). Only mark if the hate is explicitly because of origin.\n- **disability (1)**: Hate directed at someone *because of* a physical, mental, or developmental disability.\n- **religion (1)**: Hate directed at someone *because of* their religious affiliation, beliefs, or practices (e.g., \"Muslims are terrorists\").\n- **sexual_orientation (1)**: Hate directed at someone *because of* their sexual orientation (e.g., \"gays are perverts\", \"dyke\").\n\n### CRITICAL RULES:\n1.  **Target of Hate:** The negative sentiment must be directed *at* a person or group *because of* the category. If a category is merely mentioned or criticized (e.g., \"He's a white supremacist\"), it is NOT hate speech targeting that category.\n2.  **Slurs & Metaphors:** A slur is a strong indicator for its corresponding category. Understand that slurs can be metaphorical.\n3.  **Category Distinction:** Pay close attention to distinctions:\n    - `race` (skin color) vs. `national_origin` (country) vs. `religion` (beliefs).\n    - `gender` (identity) vs. `sexual_orientation` (attraction). Attack on \"lesbians\" is `sexual_orientation`. Attack on \"women\" is `gender`.\n4.  A text can have multiple labels set to 1. If a category does not apply, set it to 0.\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys in this order: `\"violence\"`, `\"directed_vs_generalized\"`, `\"gender\"`, `\"race\"`, `\"national_origin\"`, `\"disability\"`, `\"religion\"`, `\"sexual_orientation\"`. Each key must have an integer value of 1 or 0. No other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"That Norwegian is an idiot.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Get lost, you shemale freak.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":1,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"Do lesbians know what a bra is? It's always the fat ugly ones.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"He criticized white supremacy, so they called him a race traitor.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":1,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
                "Q": 0.07500000000000001,
                "N": 1,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 30,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert content moderator. Your task is to analyze a text input and classify it into multiple hate speech categories based strictly on the explicit and literal content. Avoid all over-generalization or inference beyond what is directly stated.\n\n### CATEGORY DEFINITIONS:\n- **violence (1)**: Explicitly threatens, encourages, or calls for physical harm, injury, or death. Not for mere mentions of violence.\n- **directed_vs_generalized (1)**: Targets a named individual or a specific, identifiable person (e.g., \"that Mexican guy\"). Mark 0 for vague, large groups (e.g., \"all immigrants\"). A specific person attacked via a group identifier is directed (1).\n- **gender (1)**: Based solely and explicitly on gender or gender identity (e.g., \"all men are trash\"). Note: An attack using a gendered insult (e.g., 'bitch') or transphobic slur is only marked if gender itself is the primary reason for the attack, not if it's secondary to another category like sexual orientation.\n- **race (1)**: Based on perceived race, skin color, or broad ethnicity (e.g., Black, White, Asian). Attacks on pan-ethnic groups (e.g., 'Asians') or using racial phenotypes (e.g., \"pale\") are `race`.\n- **national_origin (1)**: Based explicitly on country of origin, citizenship, or nationality (e.g., Mexican, Norwegian). Only mark if the hate is explicitly because of a specific national origin.\n- **disability (1)**: Based on physical, mental, or developmental disability. Note: Metaphorical insults like \"cancer,\" \"retard,\" or \"moron\" are not literal disability attacks unless context is explicitly about disability.\n- **religion (1)**: Based on religious affiliation, beliefs, or practices (e.g., Muslim, Christian).\n- **sexual_orientation (1)**: Based on sexual orientation (e.g., gay, lesbian). Includes homophobic and biphobic slurs.\n\n### CRITICAL RULES:\n1.  **Primary Motivation:** The hate must be directly and primarily *because of* the category. Identify the core reason for the attack. If the link is not explicit, mark 0.\n2.  **Figurative Language:** Interpret common slurs metaphorically. Terms like 'cancer,' 'rat,' or 'animal' are general insults, not literal claims about disability or species.\n3.  **Slurs:** A slur indicates its category, but consider the primary motivation (Rule #1). A secondary slur does not automatically trigger its category.\n4.  **Distinctions:** `race` targets broad groups based on physical traits, `national_origin` targets specific countries, `religion` targets beliefs.\n5.  A text can have multiple labels set to 1. If a category does not apply, set it to 0.\n\n### OUTPUT FORMAT:\nOutput ONLY a valid JSON dictionary with the exact keys in this order: `\"violence\"`, `\"directed_vs_generalized\"`, `\"gender\"`, `\"race\"`, `\"national_origin\"`, `\"disability\"`, `\"religion\"`, `\"sexual_orientation\"`. Each key must have an integer value of 1 or 0. No other text.\n\n### EXAMPLES:\nInput: \"That gay guy Mark is a freak and should be fired.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"All immigrants are criminals and should go back.\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":0,\"gender\":0,\"race\":0,\"national_origin\":1,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\nInput: \"EXECUTE OHOMO AND HIS SHEMALE BITCH MIKE.\"\nOutput: {\"violence\":1,\"directed_vs_generalized\":1,\"gender\":0,\"race\":0,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":1}\n\nInput: \"Pale cancer! Back to your west pacific ocean\"\nOutput: {\"violence\":0,\"directed_vs_generalized\":1,\"gender\":0,\"race\":1,\"national_origin\":0,\"disability\":0,\"religion\":0,\"sexual_orientation\":0}\n\n---\n\nNow, classify the following text:\nInput: ${text}\nOutput:",
                "Q": 0.05,
                "N": 1,
                "reward": 0.05,
                "children": []
              }
            ]
          }
        ]
      }
    ]
  }
}