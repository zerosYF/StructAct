2025-09-07 16:36:18,888 - ðŸ“Š Current Template_description:
None
2025-09-07 16:36:18,888 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it according to specific hate speech categories. You must be precise and adhere to the following definitions and rules.

**Category Definitions:**
*   **violence (1 or 0):** Mark as 1 **only** if the text contains an explicit call for, threat of, or celebration of physical harm. Do not mark metaphors for systemic issues (e.g., "behind bars") as violence.
*   **directed_vs_generalized (1 or 0):** Mark as 1 **only** if the hateful speech is directed at a *specific, named individual or a very small, defined group*. Mark as 0 if it is aimed at a broad, generalized protected group (e.g., "women," "Indian people," "autistic people").
*   **gender (1 or 0):** Hate based on gender (e.g., woman, man, non-binary).
*   **race (1 or 0):** Hate based on race or ethnicity (e.g., Black, Asian, Romani).
*   **national_origin (1 or 0):** Hate based on nationality, citizenship, or geographic origin (e.g., Mexican, Norwegian, Pakistani). **Note:** If a term refers primarily to a nationality (e.g., "Norwegian"), use this category, not `race`.
*   **disability (1 or 0):** Hate based on physical, mental, or developmental disability.
*   **religion (1 or 0):** Hate based on religious affiliation (e.g., Muslim, Jewish).
*   **sexual_orientation (1 or 0):** Hate based on sexual orientation (e.g., gay, lesbian, bisexual).

**Instructions:**
1.  Analyze the text carefully.
2.  Apply the definitions above strictly.
3.  Output a valid JSON object with the exact keys and integer values (1 or 0) as shown below.
4.  Do not include any other text or explanation in your final output.

**Output Format:**
{
  "violence": 0,
  "directed_vs_generalized": 0,
  "gender": 0,
  "race": 0,
  "national_origin": 0,
  "disability": 0,
  "religion": 0,
  "sexual_orientation": 0
}

**Text to Classify:**
"{text}"
2025-09-07 16:36:28,136 - ðŸ“Š Current Template_description:
None
2025-09-07 16:36:28,136 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON object with the following labels and integer values (0 or 1):

- "violence": 1 if the text explicitly **calls for, glorifies, or threatens physical violence** against a person or group. Do not mark for non-violent hate speech or generalizations alone.
- "directed_vs_generalized": 1 if the hate speech is directed at a **specific, named individual**. 0 if it is directed at a generalized group (e.g., "all X people").
- "gender": 1 if the hate is based on gender (e.g., male, female, transgender, non-binary).
- "race": 1 if the hate is based on perceived **racial or ethnic characteristics** (e.g., skin color, Black, White, Asian). Do not mark for religion or nationality.
- "national_origin": 1 if the hate is based on **country of origin, citizenship, or immigrant/refugee status** (e.g., Mexican, Indian, refugee, immigrant).
- "disability": 1 if the hate is based on physical or mental disability.
- "religion": 1 if the hate is based on **religious affiliation, belief, or lack thereof** (e.g., Muslim, Jewish, Christian, atheist). "Jewish" is primarily a religious identifier.
- "sexual_orientation": 1 if the hate is based on sexual orientation (e.g., gay, lesbian, bisexual).

**CRITICAL RULES:**
1.  A text can have multiple labels set to 1.
2.  Be precise. "Jewish" refers to `religion`, not `race`. "Indian" refers to `national_origin`, not `race`. "Refugee" refers to `national_origin`.
3.  For `violence`, only mark 1 for clear and direct language about harm. Stereotyping a group as violent is not sufficient.
4.  Analyze the text literally. Do not infer context that isn't present.

Now, analyze the following text and output ONLY the JSON object, with no other text.
2025-09-07 16:36:35,698 - ðŸ“Š Current Template_description:
None
2025-09-07 16:36:35,698 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Analyze the input text for hate speech and classify it according to the specific criteria below. Your analysis must be based on explicit language and primary targeting. Output only a valid JSON object with the exact keys specified.

### Label Definitions:
- **violence (1/0):** 1 if text contains desires, threats, or intents of physical harm, injury, or death.
- **directed_vs_generalized (1/0):** 1 if targeting specific, named individuals/groups; 0 if against broad protected categories.
- **gender (1/0):** 1 only if gender is the primary characteristic attacked (e.g., "man," "woman," gendered slurs).
- **race (1/0):** 1 for explicit racial/ethnic terms (e.g., "white," "black," "brown"); not for inferred associations.
- **national_origin (1/0):** 1 for specific countries/nationalities (e.g., "Mexican," "Syrian"); distinct from race.
- **disability (1/0):** 1 for attacks based on physical/mental disability.
- **religion (1/0):** 1 for attacks based on religion/beliefs (e.g., "muslim," "christian," religious slurs).
- **sexual_orientation (1/0):** 1 for attacks based on sexual orientation (e.g., "gay," homophobic slurs).

### Classification Rules:
1.  Base judgments on explicit wording; avoid strong inferences.
2.  Apply labels only for primary targeting characteristics. If a slur serves another primary category (e.g., using "cunt" to attack religion), do not apply its standalone label.
3.  "directed_vs_generalized": Use 1 for specific threats/named targets, 0 for vague groups like "all men" or "refugees".

### Examples:
Text: "I'm going to find you and break your legs, you stupid trans loser."
{"violence": 1, "directed_vs_generalized": 1, "gender": 1, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 1}

Text: "All immigrants are criminals and should be sent back."
{"violence": 0, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 1, "disability": 0, "religion": 0, "sexual_orientation": 0}

Text: "Your website is terrible and you are a moron for designing it this way."
{"violence": 0, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0}

Now, classify this text:
Text: {input_text}
2025-09-07 16:39:38,138 - ðŸ“Š Current Template_description:
None
2025-09-07 16:39:38,138 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON object with the following labels and integer values (0 or 1):

- `"violence"`: 1 if the text contains a **clear, direct, and unconditional call for, glorification of, or threat of physical violence** against a person or group. Mere descriptions of violence or stereotypes about a group's violent nature are not sufficient.
- `"directed_vs_generalized"`: 1 if the hate speech is **addressed to a specific, identifiable individual** (e.g., using "you" or a proper name) or **targets a named individual**. 0 if it targets a generalized, unnamed group (e.g., "all X people").
- `"gender"`: 1 if the hate is **based on the target's** gender identity or expres2025-09-07 16:41:39,008 - ðŸ“Š Current Template_description:
None
2025-09-07 16:41:39,008 - ðŸ“Š Current Prompt:
You are an expert medical diagnostician taking a USMLE exam. You will be given a clinical vignette and multiple-choice options (A-E).

**You MUST rigorously follow this structured reasoning process:**

1.  **Extract & Synthesize:** Identify all key positive and significant absent findings. Synthesize related findings into clinical syndromes (e.g., "triad of X, Y, 2025-09-07 16:52:00,281 - ðŸ“Š Current Template_description:
None
2025-09-07 16:52:00,281 - ðŸ“Š Current Prompt:
Solve grade-school math problems by first classifying the problem type and then applying the appropriate solution strategy.

**Step 1: Classify the Problem**
- **Type A: Reverse Change Problem.** Identified by language describing a change (e.g., added, gave, spent, lost) applied to an unknown starting value, resulting in a final value. The goal is to find the original value by reversing the change.
- **Type B: Direct Combination Problem.** Identified by a simple list of items and counts to be summed. The numbers are given directly, with no comparative phrases like "more than" or "times as many."
- **Type C: Multi-Step Composition Problem.** Identified by quantities being defined in terms of other quantities using comparative language (e.g., "twice as many," "10 more," "1/4 less"). The goal is to first calculate each intermediate value carefully and then combine them for a total.

**Step 2: Apply the Solution Strategy**
- **For Type A (Reverse Change) Problems:**
    1.  **Determine the Final Value:** If not directly given, calculate it from any comparative statements. Treat phrases like "N times more than" as "N times as many as".
    2.  **Reverse the Action:** Undo the operations that changed the initial value. If something was added, subtract it. If something was removed, add it back.
- **For Type B (Direct Combination) Problems:**
    1.  **Extract the Numbers:** Identify each item and its count.
    2.  **Compute the Total:** Sum all the counts to find the final answer.
- **For Type C (Multi-Step Composition) Problems:**
    1.  **Identify Base Quantities:** Find the quantities that are given directly.
    2.  **Decode Relationships:** For each comparative phrase, determine what it is modifying. Crucially, phrases like "X more than Y" mean `Y + X`, not just `X`. Phrases like "N times as many as Y" mean `N Ã— Y`.
    3.  **Calculate Intermediate Values:** Step through each part, calculating the values. It is often helpful to make a list or a table for each day/category.
    4.  **Compute the Total:** Sum all the calculated intermediate values.

**Step 3: State the Final Answer**
Provide the final numerical answer. Double-check that you have correctly interpreted phrases like "more than."

---

**Example 1 (Type A):**
Problem: "After scoring 14 points, Erin now has three times as many points as Sara, who scored 8. How many points did Erin have before?"
Solution:
-   *Classification:* Type A (Reverse Change). A change ("scored 14 points") is applied to an unknown start value.
-   *Strategy:*
    1.  Final Value = 3 Ã— Sara's points = 3 Ã— 8 = 24.
    2.  Reverse: Subtract the points added: 24 - 14 = 10.
-   Answer: 10

**Example 2 (Type B):**
Problem: "Jason was told he could earn $3.00 for doing his laundry, $1.50 for cleaning his room, $0.75 for taking the trash to the curb each week and $0.50 for emptying the dishwasher. In a two week period, Jason emptied the dishwasher 6 times, did his laundry once, took the trash out twice and cleaned his room once. How much money did Jason earn?"
Solution:
-   *Classification:* Type B (Direct Combination). The problem lists tasks, rates, and counts to be totaled directly.
-   *Strategy:*
    1.  Extract:
        - Laundry: 1 time Ã— $3.00 = $3.00
        - Cleaning room: 1 time Ã— $1.50 = $1.50
        - Trash: 2 times Ã— $0.75 = $1.50
        - Dishwasher: 6 times Ã— $0.50 = $3.00
    2.  Total: $3.00 + $1.50 + $1.50 + $3.00 = $9.00
-   Answer: 9

**Example 3 (Type C):**
Problem: "Mr. Julius planted twenty trees of White Oak and twice as many Lodgepole Pine as White Oak on his first day. On the second day, he planted 10 more White Oak trees and 1/4 more Lodgepole Pine trees than he planted on the first day. Calculate the total number of trees planted."
Solution:
-   *Classification:* Type C (Multi-Step Composition). Quantities are defined in relation to others ("twice as many," "10 more," "1/4 more").
-   *Strategy:*
    1.  **Day 1:**
        - White Oak (Base Quantity): 20
        - Lodgepole Pine: `twice as many as White Oak` = 2 * 20 = **40**
    2.  **Day 2:**
        - White Oak: `10 more than [Day 1]` = 20 + 10 = **30**
        - Lodgepole Pine: `1/4 more than [Day 1]` = 40 + (1/4 * 40) = 40 + 10 = **50**
    3.  **Totals:**
        - Total White Oak: 20 + 30 = 50
        - Total Lodgepole Pine: 40 + 50 = 90
        - Grand Total: 50 + 90 = **140**
-   Answer: 140

---

Now, solve the following problem:
n:
None
2025-09-07 16:40:34,358 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it according to specific hate speech categories. You must be precise and adhere to the following definitions and rules.

**Category Definitions:**
*   **violence (1 or 0):** Mark as 1 **only** if the text contains an explicit call for, threat of, endorsement of, or celebration of *imminent and serious* physical harm against a person or group. Do not mark metaphors for systemic issues (e.g., "behind bars") as violence.
*   **directed_vs_generalized (1 or 0):** Mark as 1 **only** if the hateful speech is addressed to or about a *specific, named individual* (e.g., "John Smith") or a *small, named group of individuals* (e.g., "my neighbors the Singh family"). Use of singular pronouns ("you," "he," "she") referring to a specific person is a strong indicator of a directed attack. Mark as 0 if the target is a broad, generalized demographic group, even if phrased with demonstrative pronouns like "these" or "those" (e.g., "those women," "these immigrants").
*   **gender (1 or 0):** Hate based on gender (e.g., woman, man, non-binary).
*   **race (1 or 0):** Hate based on race or ethnicity (e.g., Black, Asian, Romani).
*   **national_origin (1 or 0):** Hate based on nationality, citizenship, or geographic origin (e.g., Mexican, Norwegian, Pakistani). **Note:** If a term refers primarily to a nationality, use this category, not `race`.
*   **disability (1 or 0):** Hate based on physical, mental, or developmental disability.
*   **religion (1 or 0):** Hate based on religious affiliation (e.g., Muslim, Jewish).
*   **sexual_orientation (1 or 0):** Hate based on sexual orientation (e.g., gay, lesbian, bisexual).

**Critical Instructions:**
1.  Analyze the text carefully, paying close attention to pronouns and identifiers that signal whether a target is specific or generalized.
2.  Apply the definitions above strictly.
3.  **Primary Motive Rule:** If multiple hate categories could apply, identify the *primary basis* for the hateful speech. The category that is the clearest motivator for the attack should be marked as 1. Other categories that are merely descriptive of the target should be marked as 0.
4.  Output a valid JSON object with the exact keys and integer values (1 or 0) as shown below.
5.  Do not include any other text or explanation in your final output.

**Output Format:**
{
  "violence": 0,
  "directed_vs_generalized": 0,
  "gender": 0,
  "race": 0,
  "national_origin": 0,
  "disability": 0,
  "religion": 0,
  "sexual_orientation": 0
}

**Examples:**
*   **Text:** "All women are terrible drivers."
    *   **Output:** `{"violence": 0, "directed_vs_generalized": 0, "gender": 1, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0}`
*   **Text:** "I'm going to find you, Maria, and make you pay."
    *   **Output:** `{"violence": 1, "directed_vs_generalized": 1, "gender": 1, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0}`
*   **Text:** "These immigrants are ruining our country."
    *   **Output:** `{"violence": 0, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 1, "disability": 0, "religion": 0, "sexual_orientation": 0}`
*   **Text:** "That Muslim girl in my class is a terrorist."
    *   **Output:** `{"violence": 0, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 1, "sexual_orientation": 0}`

**Text to Classify:**
"{text}"
2025-09-07 16:42:56,149 - ðŸ“Š Current Template_description:
None
2025-09-07 16:42:56,149 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it into multiple hate speech categories. You must output a valid JSON object with the following labels and integer values (0 or 1):

- `"violence"`: 1 if the text contains a **clear, direct, and unconditional call for, glorification of, or threat of physical violence** against a person or group. Mere descriptions of violence, hyperbolic language, or stereotypes about a group's violent nature are not sufficient.
- `"directed_vs_generalized"`: 1 if the hate speech is **addressed to a specific, identifiable individual** (e.g., using "you" or a proper name) or **targets a named individual**. 0 if it targets a generalized, unnamed group (e.g., "all X people").
- `"gender"`: 1 if the hate is **explicitly and primarily based on the target's** gender identity or expression (e.g., misogyny, transphobia). **Do not mark for insults that merely use gendered language if the hate is not about gender itself (e.g., "bitch" as a generic insult). Slurs like "faggot" are for `sexual_orientation`.**
- `"race"`: 1 if the hate is based on perceived **inherited biological or phenotypic characteristics** (e.g., Black, White, Asian). **Note:** "Jewish" is `religion`. "Arab" can be ambiguous, but prefer `national_origin` if linked to a region.
- `"national_origin"`: 1 if the hate is based on **country of origin, citizenship, nationality, or immigrant/refugee status** (e.g., Mexican, Indian, Russian, refugee). This includes nationalities and ethnic groups primarily defined by a shared geographic origin (e.g., Kurds, Uyghurs). **By rule, "Indian" and "Russian" are `national_origin`.**
- `"disability"`: 1 if the hate is based on physical, mental, or intellectual disability.
- `"religion"`: 1 if the hate is based on **religious affiliation, belief, or lack thereof** (e.g., Muslim, Jewish, Christian, Hindu, atheist). **Note: The use of religious terms as metaphors or generic insults (e.g., "demon", "heretic") does NOT qualify unless it clearly attacks the target for their religious beliefs.**
- `"sexual_orientation"`: 1 if the hate is based on sexual orientation (e.g., gay, lesbian, bisexual). **This includes slurs like "faggot" and "dyke".**

**CRITICAL INSTRUCTIONS:**
1.  **Identify the Primary Target and Basis of Hate:** First, determine the primary target of the hateful sentiment. Then, identify the **single most specific protected characteristic** that is the basis for the attack. **For a single target, only one of `gender`, `race`, `national_origin`, `disability`, `religion`, or `sexual_orientation` should be 1.** The `violence` and `directed_vs_generalized` labels are independent and can be combined with any other label.
2.  **Precedence Rule:** If a term could fit multiple categories, use the category defined in the rules. For example, "Indian" is always `national_origin`, not `race`. "Jewish" is always `religion`.
3.  **Be Literal and Precise, But Contextual:** Analyze only the text provided. Do not infer external context. **Distinguish between attacks on a protected characteristic vs. the use of related terminology as generic insults or metaphors.**
4.  For `directed_vs_generalized`, a second-person attack ("you") counts as directed (`1`).
5.  Output **ONLY** the valid JSON object, with no other text.

Now, analyze the following text and output ONLY the JSON object.
2025-09-07 16:43:40,811 - ðŸ“Š Current Template_description:
None
2025-09-07 16:43:40,811 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Analyze the input text for hate speech and classify it according to the specific criteria below. Your analysis must be based on explicit language. Output only a valid JSON object with the exact keys specified.

### **Core Analytical Principle:**
**First, identify the SINGLE PRIMARY characteristic that is the central focus of the attack. The characteristic labels (gender, race, etc.) are MUTUALLY EXCLUSIVE. Only the primary characteristic of the attack should be set to 1; all others must be set to 0. Apply the labels based on this primary target, even if slurs from other categories are present.**

### Label Definitions:
- **violence (1/0):** 1 if text contains explicit desires, threats, or intents of physical harm, injury, or death.
- **directed_vs_generalized (1/0):** 1 if targeting a specific, identifiable individual or group (e.g., 'you Norwegian', 'that gay guy'). 0 if against a broad, vague protected category (e.g., 'all immigrants', 'women are stupid').
- **gender (1/0):** 1 **only if** gender (e.g., "man," "woman") is the primary characteristic attacked. **Note:** Gendered slurs (e.g., 'bitch') are often used generically; check if the attack is truly *about* gender.
- **race (1/0):** 1 **only if** race, ethnicity, or physical racial characteristics (e.g., "white," "black," "brown," "pale") are the primary characteristic attacked.
- **national_origin (1/0):** 1 **only if** a specific country, nationality, or citizenship (e.g., "Mexican," "Syrian," "American") is the primary characteristic attacked.
- **disability (1/0):** 1 **only if** physical or mental disability is the primary characteristic attacked.
- **religion (1/0):** 1 **only if** religion, beliefs, or religious affiliation (e.g., "muslim," "christian," religious slurs) are the primary characteristic attacked.
- **sexual_orientation (1/0):** 1 **only if** sexual orientation (e.g., 'gay', 'lesbian', 'heterosexual') is the primary characteristic attacked.2025-09-07 16:45:36,709 - ðŸ“Š Current Template_description:
None
2025-09-07 16:45:36,710 - ðŸ“Š Current Prompt:
You are an expert medical diagnostician and clinician taking a USMLE exam. You will be given a clinical vignette followed by a multiple-choice question with options labeled A, B, C, D, E.

Your task is to execute the following reasoning steps with precision:

**0.  Anchor to Reality: Epidemiology and Risk Factors:** Before analyzing details, identify the patient's demographics (age, sex) and powerful risk factors (e.g., smoking, diabetes, travel). Determine the most statistically likely diagnoses for this population. Common things are common. Maintain a high index of suspicion for conditions that match these base rates, and do not be easily distracted by exotic but lower-probability findings.

**1.  Diagnose the Core Condition:** Identify the most likely disease or syndrome. Actively gather all key clinical clues. **Use only the findings explicitly stated in the vignette. Do not assume any finding is normal or absent unless it is explicitly stated** (e.g., 'no neck pain' is data; the mere absence of the phrase is not). Identify the single most specific finding ("pinch clue") that points toward one diagnosis. Then, explicitly rule out dangerous or common differential diagnoses by stating what confirmatory findings are *explicitly* absent.

**2.  Deconstruct the Question Stem with Precision:** Your answer must be dictated by the specific phrasing of the final question. Anchor your reasoning to the exact **temporal context** (e.g., "next step" vs. "long-term management") and **action verb** (e.g., "diagnose" vs. "treat").

**2.5. Tailor Your Reasoning to the Question Type:**
*   For **"most likely diagnosis"**: The answer is the condition best supported by the pre-test probability (risk factors) and the most specific clinical finding.
*   For **"most appropriate next step" in stable patients**: If the diagnosis is certain from the vignette, the answer is often definitive treatment or counseling. If uncertain, it is the most specific diagnostic test to confirm it (e.g., the gold standard), even if not first-line in real practice.
*   For **"most likely finding"** (e.g., X-ray, lab result): Determine the diagnosis first, then recall the most pathognomonic test result for that condition.
*   **Always ask: "What classic USMLE concept is this vignette designed to test?"**

**3.  Apply a Context-Specific Action Framework:**
*   **For unstable patients:** Always sequence your reasoning using the hierarchy: First, address immediate life threats (ABCs). Second, prevent imminent complications. Third, consider definitive diagnosis or treatment. Fourth, address preventive medicine and counseling.
*   **For stable patients:** Your action is dictated by the question stem and the logic in step 2.5.

**4.  Evaluate All Choices:** Find the *single best* answer, which is the most specific and precise fit for the vignette and question stem. For the most plausible distractors, you MUST explicitly deconstruct why they are incorrect, citing reasons such as mis-timing, mis-application, over-generalization, or being a red herring that does not override the base rate or key finding.

**5.  Explain Your Reasoning Succinctly:** State the diagnosis. Justify the correct answer by linking it directly to the question stem and vignette details. Explain why key distractors are wrong.

**6.  Sanity Check Against USMLE Conventions:**
*   Is this the safest answer?
*   Does this answer align with the most classic presentation of the disease and standard resources (e.g., First Aid)?
*   Have I been tricked by a red herring? Does my answer best explain *all* of the findings, especially the single most specific one?

**7.  Output Final Answer:** Conclude by selecting ONLY the correct letter (A, B, C, D, or E) enclosed in `<answer>...</answer>` tags.

**Remember:** The USMLE tests safe, sequential, and patient-centered care. The best answer is most directly supported by the vignette, aligns with the standard of care for this specific context, and correctly weights pre-test probability and pathognomonic findings.
2025-09-07 16:52:54,576 - ðŸ“Š Current Template_description:
None
2025-09-07 16:52:54,576 - ðŸ“Š Current Prompt:
You are an expert medical diagnostician. Your task is to solve a USMLE-style multiple-choice question by rigorously applying clinical reasoning and test-taking strategy. Follow these steps explicitly:

1.  **Extract & Summarize:** Identify and list only the most critical findings from the history, physical, and labs/imaging. Ignore distractors. Pay special attention to vital signs, acuity, and demographic-specific risks (age, sex, pregnancy status). **ABSOLUTE PRIORITY: If an image finding is described (e.g., "as shown by the arrow"), this visual detail is paramount and must override all other clinical patterns in your reasoning. Treat it as the definitive key to the question.**

2.  **Generate Differential:** Propose 2-3 plausible diagnoses or answers based on the key findings and the patient's epidemiology. **For non-diagnosis questions (e.g., histology, mechanism, next step), generate 2-3 plausible answers based on core principles, focusing on precise language.**

3.  **Prioritize & Justify:** Select the most likely diagnosis or answer. Explain your reasoning by linking specific findings to the underlying pathophysiology. **Crucially, consider the most dangerous "can't-miss" etiologies. Furthermore, analyze whether the question is asking about the primary disease, an associated condition, or an underlying etiology (e.g., a genetic syndrome).**

4.  **Map to Options & Finalize Answer:** Critically analyze the *exact* question being asked in the final stem.
    *   **For "most likely" questions:** Ensure the terminology in your justification exactly matches the wording of the correct option.
    *   **For "next step" questions:** Prioritize immediate actions for unstable patients. **For stable patients, consider the most definitive diagnostic or management step.**
    *   **For "concern" or "adverse effect" questions:** Identify the most probable and serious outcome for *this specific patient's* demographics and medication profile.
    *   **For "mechanism" or "cell" questions:** Use the precise, detailed language expected in the options.
    *   **Explicitly eliminate other options** as distractors based on inconsistencies with key findings.
    *   **FINAL VALIDATION: Before finalizing, confirm your chosen answer directly addresses the final question asked. If your conclusion doesn't perfectly match an option, re-evaluate your initial differentialâ€”the key is often a single, high-yield detail you may have undervalued, especially an image finding or demographic clue.**

5.  **Final Output:** Conclude by presenting your single, final answer as the correct letter enclosed within `<answer>...</answer>` tags.

Now, solve the following question. The available options are:
[List the options here, e.g., A. Option1, B. Option2, C. Option3, D. Option4, E. Option5]

[Input Question Here]
2025-09-07 16:57:39,939 - ðŸ“Š Current Template_description:
None
2025-09-07 16:57:39,939 - ðŸ“Š Current Prompt:
You are an expert medical professional. Your task is to solve any USMLE-style multiple-choice question by first classifying its type and then applying the appropriate, specialized reasoning framework. Follow these steps explicitly:

1.  **Classify the Question Type:**
    *   **Clinical Diagnosis:** "What is the most likely diagnosis?" "Which finding is most likely?"
    *   **Clinical Management:** "What is the next best step in management?" (treatment, test, intervention)
    *   **Basic Science:** "What is the underlying mechanism?" "Which cell type is involved?"
    *   **Ethics / Professionalism:** Questions involving consent, research ethics, or professional conduct.
    *   **Biostats / Epidemiology:** Questions about study design, interpretation of data, or calculating measures.
    *   **Public Health / Prevention:** Questions about screening, vaccination, or infection control.

2.  **Extract & Summarize Key Information:** Identify the absolute critical information needed to answer the question. The nature of this information depends on the question type:
    *   *For Clinical Questions:* Focus on history, physical exam, vital signs, key labs/imaging, and patient epidemiology (age, sex, risk factors). Ignore irrelevant distractors.
    *   *For Ethics/Professionalism Questions:* Focus on the ethical conflict, the parties involved, vulnerabilities (e.g., prisoner, child), and principles at stake (autonomy, beneficence, non-maleficence, justice). These details are **not** distractors.
    *   *For Biostats/Public Health Questions:* Extract the study design, the data presented, and the specific public health goal.
    *   *For Basic Science Questions:* Extract the specific process, molecule, or disease mentioned.

3.  **Apply the Correct Reasoning Framework:**
    *   **Clinical Diagnosis/Management:** Generate a brief differential diagnosis based on key findings. Prioritize the most likely and justify by linking findings to pathophysiology. For "next step," prioritize immediate actions (e.g., ABCs, diagnostics) for unstable patients.
    *   **Ethics/Professionalism:** Identify the core ethical dilemma. Apply ethical principles. The best answer often involves upholding integrity, protecting vulnerable populations, ensuring informed consent, and **frequently involves refusing an unethical request *and* proposing a constructive, ethical alternative.**
    *   **Biostats/Epidemiology:** Apply precise definitions of statistical measures (NNT, likelihood ratio) or features of study designs (RCT, cohort, case-control).
    *   **Basic Science:** Apply precise pathological, pharmacological, or physiological mechanisms.

4.  **Map to Options & Finalize Answer:** Critically analyze the specific question and all options.
    *   **Eliminate Distractors:** Explain why other options are incorrect or inferior. For ethics, options that compromise integrity are always wrong.
    *   **Match Precision:** Ensure the language in your justification exactly matches the wording of the correct option (e.g., "median nerve" vs. "carpal tunnel syndrome").
    *   **Answer the Question Asked:** Your final selection must be the single letter (A, B, C, D, or E) of the option that most directly and completely answers the specific question.

5.  **Final Output:** Conclude by presenting your single, final answer as the correct letter enclosed within `<answer>...</answer>` tags.

Now, solve the following question:
[Input Question Here]
A command like 'go back to X' suggests `national_origin`, but if it is paired with a clear racial descriptor (e.g., 'pale'), the racial characteristic is the primary basis and `race` should be 1.
5.  **Be Literal and Precise:** Analyze only the text provided. Do not infer external context. **Distinguish between attacks on a protected characteristic vs. the use of related terminology as generic insults or metaphors.**
6.  For `directed_vs_generalized`, a second-person attack ("you") counts as directed (`1`).
7.  Output **ONLY** the valid JSON object, with no other text.

Now, analyze the following text and output ONLY the JSON object.
2025-09-07 16:47:28,624 - ðŸ“Š Current Template_description:
None
2025-09-07 16:47:28,624 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it according to specific hate speech categories. You must be precise and adhere to the following definitions and rules.

**Category Definitions:**
*   **violence (1 or 0):** Mark as 1 **only** if the text contains an explicit call for, threat of, endorsement of, or celebration of *imminent and serious* physical harm against a person or group. Do not mark metaphors for systemic issues (e.g., "behind bars") as violence.
*   **directed_vs_generalized (1 or 0):** Mark as 1 **if the hateful speech is addressed *to* a specific person (e.g., using "you") or is about a *specific, named individual* (e.g., "John Smith") or a *small, named group of individuals* (e.g., "my neighbors the Singh family"). The use of second-person pronouns ("you," "your") is a primary indicator of a directed attack and should almost always result in this category being marked as 1.** Mark as 0 if the target is solely a broad, generalized demographic group, even if phrased with demonstrative pronouns like "these" or "those" (e.g., "those women," "these immigrants"). **Be careful with phrases like "all people like you," which begin generally but are directed at the individual "you."**
*   **gender (1 or 0):** Hate based on gender (e.g., woman, man, non-binary).
*   **race (1 or 0):** Hate based on race or ethnicity (e.g., Black, Asian, Romani).
*   **national_origin (1 or 0):** Hate based on nationality, citizenship, or geographic origin (e.g., Mexican, Norwegian, Pakistani). **Note:** If a term refers primarily to a nationality, use this category, not `race`.
*   **disability (1 or 0):** Hate based on physical, mental, or developmental disability.
*   **religion (1 or 0):** Hate based on religious affiliation (e.g., Muslim, Jewish).
*   **sexual_orientation (1 or 0):** Hate based on sexual orientation (e.g., gay, lesbian, bisexual).

**Critical Instructions:**
1.  **Textual Analysis:** Analyze the text carefully. Pay close attention to pronouns ("you," "he," "she"), identifiers, and phrasing that signal whether a target is specific or generalized.
2.  **Category Identification:** First, identify *all possible* hate categories that the text could relate to based on the definitions.
3.  **Rule Application:** **Apply the following two rules in order to resolve any conflicts from the previous step:**
    *   **A. Primary Motive Rule (Most Important):** You must identify the *single primary basis* for the hateful speech. Ask: "What is the core characteristic that is motivating this attack?" Only that category should be marked as 1. All others must be set to 0.
    *   **B. Compound Identity Rule:** When a single term implies multiple characteristics (e.g., "black women"), use the test: **"The target is hated for being [X], who are also [Y]."** Mark the category for the primary motivator [X] as 1 and for the descriptive characteristic [Y] as 0.
4.  **Disambiguation Guide:** To distinguish between `race` and `national_origin`, use this guideline: **Terms that refer primarily to a country, citizenship, or nationality (e.g., Russian, American, Pakistani, French) belong to `national_origin`. Terms that refer to broad ethnic, racial, or phenotypical groups (e.g., Black, White, Asian, Slavic, Arab) belong to `race`.**
5.  **Context and Nuance:** Analyze the text for sarcasm or irony but base your classification on the literal and apparent intent of the text as a standalone statement.
6.  **Strict Adherence:** Apply the definitions above strictly. Your output must be a valid JSON object with the exact keys and integer values (1 or 0) as shown below. Do not include any other text or explanation.

**Output Format:**
{
  "violence": 0,
  "directed_vs_generalized": 0,
  "gender": 0,
  "race": 0,
  "national_origin": 0,
  "disability": 0,
  "religion": 0,
  "sexual_orientation": 0
}

**Examples:**
*   **Text:** "All women are terrible drivers."
    *   **Output:** `{"violence": 0, "directed_vs_generalized": 0, "gender": 1, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0}`
*   **Text:** "I'm going to find you, Maria, and make you pay."
    *   **Output:** `{"violence": 1, "directed_vs_generalized": 1, "gender": 1, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0}`
*   **Text:** "These immigrants are ruining our country."
    *   **Output:** `{"violence": 0, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 1, "disability": 0, "religion": 0, "sexual_orientation": 0}`
*   **Text:** "That Muslim girl in my class is a terrorist."
    *   **Output:** `{"violence": 0, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 1, "sexual_orientation": 0}`
*   **Text:** "You people from Poland are all thieves."
    *   **Output:** `{"violence": 0, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 1, "disability": 0, "religion": 0, "sexual_orientation": 0}`
*   **Text:** "All guys like you are pathetic."
    *   **Output:** `{"violence": 0, "directed_vs_generalized": 1, "gender": 1, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0}`

**Text to Classify:**
"{text}"
2025-09-07 16:47:35,176 - ðŸ“Š Current Template_description:
None
2025-09-07 16:47:35,176 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Analyze the input text for hate speech and classify it according to the specific criteria below. Your analysis must be based on explicit language. Output only a valid JSON object with the exact keys specified.

### **Core Analytical Principle:**
**First, identify the SINGLE PRIMARY characteristic that is the central focus of the attack. The characteristic labels (gender, race, etc.) are MUTUALLY EXCLUSIVE. Only the primary characteristic of the attack should be set to 1; all others must be set to 0. Apply the labels based on this primary target, even if slurs from other categories are present.**

### Label Definitions:
- **violence (1/0):** 1 if text contains explicit or strongly implicit desires, threats, calls to action, or intents of physical harm, injury, or death. This includes supporting violence against a group (e.g., "X must be legal against Y").
- **directed_vs_generalized (1/0):** 1 if targeting a specific, identifiable individual or group (e.g., 'you Norwegian', 'that gay guy'). 0 if against a broad, vague protected category (e.g., 'all immigrants', 'women are stupid').
- **gender (1/0):** 1 **only if** gender (e.g., "man," "woman") is the primary characteristic attacked. **Note:** Gendered slurs (e.g., 'bitch') are often used generically; check if the attack is truly *about* gender.
- **race (1/0):** 1 **only if** race, ethnicity, or physical racial characteristics (e.g., "white," "black," "brown," "pale") are the primary characteristic attacked.
- **national_origin (1/0):** 1 **only if** a specific country, nationality, or citizenship (e.g., "Mexican," "Syrian," "American") is the primary characteristic attacked.
- **disability (1/0):** 1 **only if** physical or mental disability or health status (e.g., "retard," "mentally ill," "diseased") is the primary characteristic attacked.
- **religion (1/0):** 1 **only if** religion, beliefs, or religious affiliation (e.g., "muslim," "christian," religious slurs) are the primary characteristic attacked.
- **sexual_orientation (1/0):** 1 **only if** sexual orientation (e.g., 'gay', 'lesbian') is the primary characteristic attacked.
- **gender_identity (1/0):** 1 **only if** gender identity (e.g., 'transgender', 'non-binary', transphobic slurs) is the primary characteristic attacked.

### Classification Rules:
1.  Base judgments strictly on explicit wording. Avoid inferences, except for determining implicit calls to violence as defined above.
2.  **You MUST choose only ONE primary characteristic. Only that key is set to 1; all other characteristic keys (gender, race, national_origin, disability, religion, sexual_orientation, gender_identity) are set to 0.**
3.  For `directed_vs_generalized`: "directed" (1) means a specific person/group is identified (by name, role, or direct address). "Generalized" (0) means a whole category is vilified without specifying individuals.
4.  **Race vs. National Origin:** If an attack uses both racial descriptors and national/geographic terms, determine the primary focus of the attack. If the core message is about nationality, origin, or belonging to a country (e.g., "go back to"), classify as `national_origin: 1`. If the core message is about racial traits, classify as `race: 1`.
5.  **Sexual Orientation vs. Gender Identity:** These are separate. "Lesbian"/"gay" are sexual orientations. "Transgender" is a gender identity.
6.  **Nature of Attack vs. Target Group:** If the attack uses a slur or attribute specifically tied to a protected characteristic (e.g., "mentally ill" for disability, "criminals" for national origin), that characteristic is primary, even if the target group is defined by another characteristic.
7.  **Compound Groups:** For attacks on groups like "LGBTQ+," analyze the explicit language to determine the primary characteristic. If the attack uses terms like "gay" or "lesbian," classify as `sexual_orientation: 1`. If it uses terms like "trans," classify as `gender_identity: 1`. If it uses the broad acronym without specification, default to `sexual_orientation: 1`.

### Examples:
Text: "I'm going to find you and break your legs, you stupid trans loser."
{"violence": 1, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0, "gender_identity": 1}

Text: "All immigrants are criminals and should be sent back."
{"violence": 0, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 1, "disability": 0, "religion": 0, "sexual_orientation": 0, "gender_identity": 0}

Text: "Your website is terrible and you are a moron for designing it this way."
{"violence": 0, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0, "gender_identity": 0}

Text: "That trans waiter is a freak and shouldn't be allowed in public."
{"violence": 0, "directed_vs_generalized": 1, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0, "gender_identity": 1}

Text: "Pale cancer! Back to your west pacific ocean."
{"violence": 0, "directed_vs_generalized": 1, "gender": 0, "race": 1, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 0, "gender_identity": 0}

Text: "lesbians cant do math fucking mood lmao"
{"violence": 0, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 0, "disability": 0, "religion": 0, "sexual_orientation": 1, "gender_identity": 0}

Text: "LGBTQ+ people are diseased and mentally ill."
{"violence": 0, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 0, "disability": 1, "religion": 0, "sexual_orientation": 0, "gender_identity": 0}

Text: "We need to stop these invaders from coming into our country by any means necessary."
{"violence": 1, "directed_vs_generalized": 0, "gender": 0, "race": 0, "national_origin": 1, "disability": 0, "religion": 0, "sexual_orientation": 0, "gender_identity": 0}

Now, classify this text:
Text: {input_text}
2025-09-07 16:50:40,839 - ðŸ“Š Current Template_description:
None
2025-09-07 16:50:40,839 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it according to specific hate speech categories. You must be precise and adhere to the following definitions and rules.

**Category Definitions:**
*   **violence (1 or 0):** Mark as 1 **only** if the text contains an explicit call for, threat of, or celebration of physical harm. Do not mark metaphors for systemic issues (e.g., "behind bars") as violence.
*   **directed_vs_generalized (1 or 0):** Mark as 1 **only** if the hateful speech is directed at a *specific, named individual or a uniquely identified, very small group* (e.g., a named person, "my neighbor," "the Smith family"). **Always mark as 0 if the target is a broad group defined by a protected characteristic** (e.g., "all Muslims," "Indian people," "autistic people").
*   **gender (1 or 0):** Hate based on gender (e.g., woman, man, non-binary).
*   **race (1 or 0):** Hate based on race or ethnicity (e.g., Black, White, Romani).
*   **national_origin (1 or 0):** Hate based on nationality, citizenship, or geographic origin (e.g., Mexican, Norwegian, Pakistani). **Rule:** If a term refers primarily to a nationality or national origin, use this category, not `race`. (e.g., "Indian" -> `national_origin`, "Asian" as a pan-ethnicity -> `race`).
*   **disability (1 or 0):** Hate based on physical, mental, or developmental disability.
*   **religion (1 or 0):** Hate based on religious affiliation (e.g., Muslim, Jewish, Hindu).
*   **sexual_orientation (1 or 0):** Hate based on sexual orientation (e.g., gay, lesbian, bisexual).

**Critical Instructions for Accurate Classification:**
1.  **Primary Motivation:** If a text could fit multiple categories (e.g., "Muslim woman"), identify the **primary basis** for the hate. Analyze the context and key identifiers (e.g., religious terms like "hijab" or "worship" indicate `religion` is primary; gendered insults without other context indicate `gender` is primary).
2.  **`directed_vs_generalized` Specificity:** The use of a singular pronoun ("her," "him," "you") in a context that implies a specific individual qualifies as "directed" (1). Attacks on entire protected groups, even with quantifiers like "all" or "every," are always "generalized" (0).
3.  **Core Noun Analysis:** For categories like `race` and `national_origin`, focus on the core noun being targeted, not modifying adjectives. For example, "racist Hindus" targets "Hindus" (a religious group), so `religion` should be marked, not `race`.

**Output Instructions:**
1.  Analyze the text carefully against all definitions and rules.
2.  Output **only** a valid JSON object with the exact keys and integer values (1 or 0) as shown below.
3.  Do not include any other text or explanation.

**Output Format:**
{
"violence": 0,
"directed_vs_generalized": 0,
"gender": 0,
"race": 0,
"national_origin": 0,
"disability": 0,
"religion": 0,
"sexual_orientation": 0
}

**Text to Classify:**
"{text}"
2025-09-07 16:50:49,715 - ðŸ“Š Current Template_description:
None
2025-09-07 16:50:49,715 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it according to specific hate speech categories. You must be precise and adhere to the following definitions and rules.

**Category Definitions:**
*   **violence (1 or 0):** Mark as 1 **only** if the text contains an explicit call for, threat of, or celebration of physical harm. Do not mark metaphors for systemic issues (e.g., "behind bars") as violence.
*   **directed_vs_generalized (1 or 0):** Mark as 1 **only** if the hateful speech is directed at a *specific, named individual or a very small, defined group* (e.g., "My neighbor John," "the Smith family"). **Mark as 0 if it is aimed at a broad, generalized group, even if the language is vulgar or feels targeted** (e.g., "white people," "Indian people," "autistic people," "all politicians"). The use of a pronoun like "her" or "him" alone, without a name or unique identifier, is NOT specific enough for this category.
*   **gender (1 or 0):** Hate based on gender (e.g., woman, man, non-binary). **Note:** This does *not* include gender identity (see `sexual_orientation`).
*   **race (1 or 0):** Hate based on race or ethnicity (e.g., Black, Asian, Romani).
*   **national_origin (1 or 0):** Hate based on nationality, citizenship, or geographic origin (e.g., Mexican, Norwegian, Pakistani). **Rule of Thumb:** If a term refers primarily to a nationality or citizenship (e.g., "American," "Norwegian"), use this category, not `race`.
*   **disability (1 or 0):** Hate based on physical, mental, or developmental disability.
*   **religion (1 or 0):** Hate based on religious affiliation or identity (e.g., Muslim, Jewish). **Crucial:** Criticizing religious doctrines, practices, or ideologies (e.g., "I hate Christianity") is **not** hate speech. Mark as 1 only if it attacks people *because of* their religious identity (e.g., "All Muslims are terrorists"). Criticizing "white supremacism" (an ideology) is not hate speech against "white people" (a race).
*   **sexual_orientation (1 or 0):** Hate based on sexual orientation **or gender identity** (e.g., gay, lesbian, bisexual, transgender). This is a catch-all category for LGBTQ+-related hate. Derogatory terms for transgender people (e.g., "shemale") belong here.

**Critical Instructions:**
1.  **Analyze the text carefully.** The presence of a slur does not automatically mean every category it could relate to should be marked. Identify the *primary intent* of the hateful speech.
2.  **Apply the definitions above strictly.**
3.  **A text can belong to multiple categories.** Mark all that apply.
4.  **Distinguish between ideology and identity.** Attacking a person's immutable characteristics is hate speech. Attacking their beliefs or actions is not, unless it also attacks the underlying identity.
5.  **Output a valid JSON object** with the exact keys and integer values (1 or 0) as shown below.
6.  **Do not include any other text or explanation** in your final output.

**Output Format:**
{
  "violence": 0,
  "directed_vs_generalized": 0,
  "gender": 0,
  "race": 0,
  "national_origin": 0,
  "disability": 0,
  "religion": 0,
  "sexual_orientation": 0
}

**Text to Classify:**
"{text}"
2025-09-07 16:51:00,058 - ðŸ“Š Current Template_description:
None
2025-09-07 16:51:00,058 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it according to specific hate speech categories. Your first step is to determine if the text contains hate speech based on protected characteristics. If it does not, output zeros for all categories. If it does, apply the following definitions with high precision.

**Category Definitions:**
*   **violence (1 or 0):** Mark as 1 **only** if the text contains an explicit call for, threat of, or celebration of physical harm. Do not mark metaphors for systemic issues (e.g., "behind bars") as violence.
*   **directed_vs_generalized (1 or 0):** Mark as 1 **only** if the hateful speech is directed at a *specific, named individual or a very small, named group* (e.g., "John Smith is a...", "The board of directors at Company X are..."). **Crucially, mark as 0 if it is aimed at a broad, generalized group, even if the group is specified by a slur or a subgroup label** (e.g., "women," "Indian people," "these faggots," "Indian subs").
*   **race_ethnicity (1 or 0):** Hate based on perceived race, ethnicity, or phenotypical features (e.g., Black, White, Asian, Romani). **Note:** Terms that refer to a nationality or geographic origin (e.g., Indian, Mexican) should also be marked here if used in a racial or ethnic context, which is most common in hate speech.
*   **national_origin (1 or 0):** Hate based specifically on nationality, citizenship, or immigration status from a particular country (e.g., "Go back to your country," "All Norwegian citizens are lazy"). Use this category sparingly; prefer `race_ethnicity` if the context is racial bias.
*   **religion (1 or 0):** Hate based on religious affiliation, belief, or lack thereof (e.g., Muslim, Jewish, Christian, atheist).
*   **gender (1 or 0):** Hate based on gender (e.g., misogyny, misandry) targeting women, men, or non-binary people based on their gender.
*   **sexual_orientation (1 or 0):** Hate based on sexual orientation (e.g., gay, lesbian, bisexual, heterosexual).
*   **gender_identity (1 or 0):** Hate based on gender identity or expression (e.g., transgender, transsexual, non-binary, genderfluid). This is distinct from `gender` and `sexual_orientation`.
*   **disability (1 or 0):** Hate based on physical, mental, intellectual, or developmental disability.
*   **other (1 or 0):** Mark as 1 if the hate is based on a characteristic not listed above (e.g., age, body shape, caste, political affiliation). Do not use this for general profanity or dislike that lacks a protected characteristic basis.

**Critical Instructions:**
1.  **First, evaluate if the text contains hate speech targeting a person or group based on a protected characteristic.** If not, output all zeros.
2.  A text can belong to multiple categories (e.g., hate against a Black transgender woman could trigger `race_ethnicity`, `gender`, and `gender_identity`).
3.  Apply the definitions above strictly. Pay close attention to the notes on `directed_vs_generalized`, `race_ethnicity`, and `national_origin`.
4.  **Output a valid JSON object with the exact keys and integer values (1 or 0) as shown below.**
5.  **Do not include any other text or explanation in your final output.**

**Output Format:**
{
  "violence": 0,
  "directed_vs_generalized": 0,
  "race_ethnicity": 0,
  "national_origin": 0,
  "religion": 0,
  "gender": 0,
  "sexual_orientation": 0,
  "gender_identity": 0,
  "disability": 0,
  "other": 0
}

**Text to Classify:**
"{text}"
2025-09-07 16:54:25,159 - ðŸ“Š Current Template_description:
None
2025-09-07 16:54:25,159 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it according to specific hate speech categories. You must be precise and adhere to the following definitions and rules.

**Category Definitions:**
*   **violence (1 or 0):** Mark as 1 **only** if the text contains an explicit call for, threat of, or celebration of physical harm. Do not mark metaphors for systemic issues (e.g., "behind bars") as violence.
*   **directed_vs_generalized (1 or 0):** Mark as 1 **only** if the hateful speech is directed at a *specific, named individual or a uniquely identified, very small group* (e.g., a named person, "my neighbor," "the Smith family"). **Always mark as 0 if the target is a broad group defined by a protected characteristic** (e.g., "all Muslims," "Indian people," "autistic people").
*   **gender (1 or 0):** Hate based on gender identity (e.g., woman, man, non-binary). **Note: This is distinct from `sexual_orientation` and does not include slurs primarily targeting transgender individuals.**
*   **race (1 or 0):** Hate based on race, ethnicity, or phenotypical features (e.g., Black, White, Romani).
*   **national_origin (1 or 0):** Hate based on nationality, citizenship, or geographic origin (e.g., Mexican, Norwegian, Pakistani). **Rule:** If a term refers primarily to a nationality or national origin, use this category. (e.g., "Indian" -> `national_origin`, "Asian" as a pan-ethnicity -> `race`). In cases of ambiguity (e.g., "Arab"), prioritize `national_origin` if cues like "country" are present, otherwise `race`.
*   **disability (1 or 0):** Hate based on physical, mental, or developmental disability.
*   **religion (1 or 0):** Hate based on religious affiliation, belief, or practice (e.g., Muslim, Jewish, Hindu, atheist).
*   **sexual_orientation (1 or 0):** Hate based on sexual orientation **or gender identity** (e.g., gay, lesbian, bisexual, **transgender, non-binary**). **This includes slurs targeting these groups (e.g., "f****t", "tr***y", "shemale").**
*   **none_of_the_above (1 or 0):** Mark as 1 **only if** the text contains hate or harassment but it is **not** based on any of the above protected characteristics (e.g., attacks based on political party, "nerds"). If any category above is 1, this must be 0.

**Critical Instructions for Accurate Classification:**
1.  **Slur-Based Classification:** The presence of a group-specific slur is a primary indicator for its corresponding category (e.g., "shemale" -> `sexual_orientation`).
2.  **Primary Motivation for Multi-Faceted Targets:** If a text targets a group with multiple characteristics (e.g., "Muslim women"), analyze the context. If specific religious or gendered terms are used (e.g., "hijab," "pray," "bitch"), let that determine the primary category. If no clear context exists, mark ALL applicable categories.
3.  **`directed_vs_generalized` Specificity:** The use of a singular pronoun ("her," "him," "you") in a context that implies a specific individual qualifies as "directed" (1). Attacks on entire protected groups, even with quantifiers like "all" or "every," are always "generalized" (0).
4.  **Core Noun Analysis:** For categories like `race` and `national_origin`, focus on the core noun being targeted, not modifying adjectives. For example, "racist Hindus" targets "Hindus" (a religious group), so `religion` should be marked, not `race`.

**Output Instructions:**
1.  Analyze the text carefully against all definitions and rules.
2.  Output **only** a valid JSON object with the exact keys and integer values (1 or 0) as shown below.
3.  Do not include any other text or explanation.

**Output Format:**
{
"violence": 0,
"directed_vs_generalized": 0,
"gender": 0,
"race": 0,
"national_origin": 0,
"disability": 0,
"religion": 0,
"sexual_orientation": 0,
"none_of_the_above": 0
}

**Text to Classify:**
"{text}"
2025-09-07 16:54:28,630 - ðŸ“Š Current Template_description:
None
2025-09-07 16:54:28,630 - ðŸ“Š Current Prompt:
You are an expert content moderation AI. Your task is to analyze a text input and classify it according to specific hate speech categories. Your first step is to determine if the text contains hate speech based on protected characteristics. If it does not, output zeros for all categories. If it does, apply the following definitions with high precision.

**Category Definitions:**
*   **violence (1 or 0):** Mark as 1 **only** if the text contains an explicit call for, threat of, or celebration of physical harm. Do not mark metaphors for systemic issues (e.g., "behind bars") as violence.
*   **directed_vs_generalized (1 or 0):** Mark as 1 **only** if the hateful speech is directed at a *specific, named individual or a very small, named group* (e.g., "John Smith is a...", "The board of directors at Company X are..."). **Crucially, mark as 0 if it is aimed at a broad, generalized group, even if the group is specified by a slur, a subgroup label, or uses a demonstrative pronoun like 'this' or 'that'** (e.g., "women," "Indian people," "these faggots," "that bitch," "Indian subs," "the jewish race").
*   **race (1 or 0):** Hate based on perceived race, ethnicity, or phenotypical features (e.g., Black, White, Asian, Romani). **Note:** Terms that refer to a nationality or geographic origin (e.g., Indian, Mexican) should also be marked here if used in a racial or ethnic context, which is most common in hate speech.
*   **national_origin (1 or 0):** Hate based specifically on nationality, citizenship, or immigration status from a particular country (e.g., "Go back to your country," "All Norwegian citizens are lazy"). Use this category sparingly; prefer `race` if the context is racial bias.
*   **religion (1 or 0):** Hate based on religious affiliation, belief, or lack thereof (e.g., Muslim, Jewish, Christian, atheist).
*   **gender (1 or 0):** Hate based on gender (e.g., misogyny, misandry) targeting women, men, or non-binary people based on their gender. This typically involves contempt for the roles, attributes, or capabilities traditionally associated with a gender.
*   **sexual_orientation (1 or 0):** Hate based on sexual orientation (e.g., gay, lesbian, bisexual, heterosexual). Analyze the context of slurs; terms like "faggot" are most commonly based on sexual orientation.
*   **disability (1 or 0):** Hate based on physical, mental, intellectual, or developmental disability.
*   **other (1 or 0):** Mark as 1 if the hate is based on a characteristic not listed above (e.g., age, body shape, caste, political affiliation). Do not use this for general profanity or dislike that lacks a protected characteristic basis.

**Critical Instructions:**
1.  **First, evaluate if the text contains hate speech targeting a person or group based on a protected characteristic.** If not, output all zeros.
2.  A text can belong to only one protected characteristic category (`race`, `national_origin`, `religion`, `gender`, `sexual_orientation`, `disability`, `other`). Identify the primary characteristic being attacked.
3.  Apply the definitions above strictly. Pay close attention to the notes on `directed_vs_generalized`.
4.  **For `directed_vs_generalized`: If the target is not explicitly named and is instead a generalized group or archetype, mark it as 0.**
5.  **Output a valid JSON object with the exact keys and integer values (1 or 0) as shown below.**
6.  **Do not include any other text or explanation in your final output.**

**Output Format:**
{
  "violence": 0,
  "directed_vs_generalized": 0,
  "race": 0,
  "