{
  "config": {
    "mcts_iters": 10,
    "depth_threshold": 5,
    "width_threshold": 3
  },
  "best_node": {
    "action_sequence": [
      "FailureDrivenAction",
      "SuccessDrivenAction"
    ],
    "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and rules:\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit calls for, glorification of, or celebration of physical harm, injury, or death (e.g., kill, murder, genocide, massacre, harm, beat, exterminate). Systemic issues (e.g., incarceration) alone do not qualify.\n- **directed_vs_generalized**: 1 = targeted at specific individual(s) using \"you\" or names; 0 = targeted at broad group\n- **gender (1)**: Attacks based on gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on race or perceived race\n- **national_origin (1)**: Attacks based on origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on physical/mental disabilities\n- **religion (1)**: Attacks based on religious beliefs\n- **sexual_orientation (1)**: Attacks based on sexual orientation\n\n**CRITICAL RULES:**\n1. Be conservative: Only assign 1 if text unambiguously matches definition\n2. Explicit mention required: Only label categories with explicitly mentioned identifiers\n3. \"Behind bars\" refers to incarceration, not physical violence\n4. Criticism of religion ≠ automatic violence classification\n5. A single phrase can trigger multiple categories simultaneously\n6. Evaluate each category independently using explicit evidence\n\n**LEARN FROM EXAMPLES:**\n\nExample: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\n→ {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Incarceration references ≠ violence; explicit race mention\n\nExample: \"With a face like that I bet you could scare your HIV into remission.\"\n→ {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Targeted attack (\"you\") with explicit disability mention\n\nExample: \"MASS BLACK WORLDWIDE GENOCIDE IS OUR ONLY HOPE\"\n→ {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Explicit genocide call (violence) targeting racial group (generalized)\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: “[INSERT TEXT HERE]”",
    "depth": 2,
    "Q": 1.3602011709276574,
    "N": 4
  },
  "search_tree": {
    "id": 0,
    "depth": 0,
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "Q": 9.506766455554512,
    "N": 30,
    "uct_value": 0.0,
    "reward": 0.1,
    "children": [
      {
        "id": 1,
        "depth": 1,
        "action_sequence": [
          "SuccessDrivenAction"
        ],
        "prompt": "Classify the text for hate speech by following this structured process:\n\n1.  **ANALYZE:** First, describe the text's tone, intent, and any targeted groups or individuals. Identify the basis for hostility (e.g., national origin, religion).\n2.  **JUSTIFY:** For each label, provide a brief justification. If a label is 0, explicitly state why (e.g., \"no threat of violence,\" \"no mention of gender\").\n3.  **CLASSIFY:** Output your final scores in the exact JSON format.\n\n**Label Definitions:**\n- **violence (1):** Direct threat, call for, or strong implication of physical harm.\n- **directed_vs_generalized (1):** Targets a specific, identifiable individual or well-defined group (e.g., \"Norwegians\"). (0) for vague, generalized statements.\n- **gender/race/national_origin/disability/religion/sexual_orientation (1):** Hostility is explicitly based on membership in this protected class.\n\n**Text to Classify:**",
        "Q": 3.24821995903111,
        "N": 10,
        "uct_value": 0.32938555481342385,
        "reward": 0.05,
        "children": [
          {
            "id": 2,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "Classify the text for hate speech by following this structured process. Use the examples below to understand common pitfalls and apply the definitions strictly.\n\n**CRITICAL DEFINITIONS (APPLY STRICTLY):**\n\n- **`violence` (1):** Only for direct threats, calls for, or clear incitement of **physical harm**. Do NOT score 1 for metaphors, hyperbole, or non-physical degradation (e.g., \"plague,\" sexual insults).\n- **`directed_vs_generalized` (1):** Only for a **specific, named individual** or a **protected class group** (race, religion, national origin, sexual orientation, gender, disability). Score 0 for non-protected groups (e.g., \"refugees,\" \"Sweden\" as a nation) or vague statements.\n- **Protected Class Labels (1):** Score 1 **only** if hostility is explicitly based on membership in that class.\n  - **`religion`:** Primary label for antisemitism targeting \"Jews\" unless explicit racial terminology is used.\n  - **`national_origin`:** Based on country of origin, nationality, or citizenship.\n  - (Other definitions remain precise as in the previous prompt)\n\n**LEARN FROM THESE EXAMPLES:**\n\nExample: \"I fucking hate gay and lesbians they are all animals\"\n- Gold: `{\"directed_vs_generalized\": 0, \"sexual_orientation\": 1}`  \n*Why: \"gay and lesbians\" is a protected class, so `sexual_orientation=1`, but it is not a *specific* named group/individual, so `directed_vs_generalized=0`.*\n\nExample: \"This fcking jew wants money? I’ll give him only my dick if he wants\"\n- Gold: `{\"violence\": 0, \"directed_vs_generalized\": 1, \"religion\": 1}`  \n*Why: The sexual insult is degrading but not a threat of physical harm. It targets a specific individual based on religion.*\n\nExample: \"i like dead refugees on my shore\"\n- Gold: `{\"directed_vs_generalized\": 0, \"national_origin\": 1, \"violence\": 1}`  \n*Why: \"Refugees\" is a non-protected, generalized group. Hostility is based on national origin. Expressing pleasure about death is a direct endorsement of violence.*\n\n**PROCESS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and targets. Identify the precise basis for hostility.\n2.  **JUSTIFY:** For each label, provide a strict justification based on the definitions above. If 0, state why (e.g., \"no physical threat,\" \"targets a non-protected group\").\n3.  **CLASSIFY:** Output your final scores in the exact JSON format: `{\"violence\": ..., \"directed_vs_generalized\": ..., \"gender\": ..., \"race\": ..., \"national_origin\": ..., \"disability\": ..., \"religion\": ..., \"sexual_orientation\": ...}`\n\n**Text to Classify:**  \n`[INSERT TEXT HERE]`",
            "Q": 1.2958907886016673,
            "N": 4,
            "uct_value": 0.39463418803790334,
            "reward": 0.05,
            "children": [
              {
                "id": 3,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by following this structured process. Apply the definitions strictly and independently for each label.\n\n**CRITICAL DEFINITIONS (STRICT BINARY RULES):**\n\n- **`violence` (1):** Only for direct threats, calls for, or clear incitement of **physical harm**. Metaphors, hyperbole, jokes, and non-physical degradation (e.g., sexual insults) score 0.\n- **`directed_vs_generalized` (1):** Only for a **specific, named individual** or a statement targeting a **protected class as a whole** (e.g., \"All [protected class]\"). Score 0 for generalized statements, non-protected groups, or vague targets.\n- **Protected Class Labels (1):** Score 1 **only** if hostility is explicitly and primarily based on membership in that class.\n  - **`religion`:** Primary label for antisemitism targeting \"Jews\" unless explicit racial terminology is used.\n  - **`national_origin`:** Based on country of origin, nationality, or citizenship.\n  - (Other definitions remain precise)\n\n**GUIDING PRINCIPLES (MUST FOLLOW):**\n1.  **Definitions Over Intuition:** Apply the definitions as absolute rules. Ignore overall offensiveness.\n2.  **Independent Evaluation:** Score each label separately. A 1 in one category does not imply a 1 in another.\n3.  **Literal Interpretation:** For `violence`, consider only the literal meaning. Figurative language scores 0.\n4.  **Specificity for Targeting:** `directed_vs_generalized=1` requires a named individual or a protected class named *as a whole*.\n5.  **Explicit Basis for Hostility:** A protected class scores 1 only if the text's hostility is explicitly because of that class identity.\n6.  **Handle Ambiguity Conservatively:** If a definition is not explicitly met, the score is 0.\n\n**PROCESS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and targets. Identify the precise basis for any hostility.\n2.  **JUSTIFY:** For each label, provide a strict justification based on the definitions above. If 0, state why (e.g., \"no physical threat,\" \"targets a non-protected group\").\n3.  **CLASSIFY:** Output your final scores in the exact JSON format: `{\"violence\": ..., \"directed_vs_generalized\": ..., \"gender\": ..., \"race\": ..., \"national_origin\": ..., \"disability\": ..., \"religion\": ..., \"sexual_orientation\": ...}`\n\n**Text to Classify:**\n`[INSERT TEXT HERE]`",
                "Q": 0.443287221992578,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.15,
                "children": []
              },
              {
                "id": 4,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by following this structured process. Use the examples below to understand common pitfalls and apply the definitions strictly.\n\n**CRITICAL DEFINITIONS (APPLY STRICTLY):**\n\n- **`violence` (1):** Only for direct threats, calls for, or clear incitement of **physical harm**. Do NOT score 1 for metaphors, hyperbole, or non-physical degradation (e.g., \"plague,\" sexual insults).\n- **`directed` (1):** Score 1 **only** if the text targets a **specific, named individual** (e.g., \"John Doe\"), a **specific, named organization**, or a **very specific, well-defined subgroup** (e.g., \"the Johnson family,\" \"the board of Acme Corp\"). Score 0 for vague, generalized, or statistical groups (e.g., \"white people,\" \"refugees,\" \"men,\" \"the French,\" \"all politicians\").\n- **`protected_class` (1):** Score 1 if the hostility is explicitly based on membership in a protected class: **race, religion, national origin, sexual orientation, gender, disability.** Score 0 if the target is a non-protected group (e.g., \"ceos,\" \"refugees\") or if no protected class is referenced.\n- **Protected Class Labels (1):** Score 1 **only** if hostility is explicitly based on membership in that specific class.\n  - **`religion`:** Primary label for antisemitism targeting \"Jews\" unless explicit racial terminology is used.\n  - **`national_origin`:** Based on country of origin, nationality, or citizenship.\n  - (Other definitions remain precise)\n\n**LEARN FROM THESE EXAMPLES (CRITICAL FOR AVOIDING MISTAKES):**\n\nExample: \"I fucking hate gay and lesbians they are all animals\"\n- Gold: `{\"directed\": 0, \"protected_class\": 1, \"sexual_orientation\": 1}`\n*Why: \"gay and lesbians\" is a generalized protected class group, so `directed=0`. The hostility is based on sexual orientation, so `protected_class=1` and `sexual_orientation=1`.*\n\nExample: \"This fcking jew wants money? I’ll give him only my dick if he wants\"\n- Gold: `{\"violence\": 0, \"directed\": 1, \"protected_class\": 1, \"religion\": 1}`\n*Why: The sexual insult is not a physical threat. It targets a specific individual (\"him\") based on religion (\"jew\"), so `directed=1` and `protected_class=1`.*\n\nExample: \"i like dead refugees on my shore\"\n- Gold: `{\"violence\": 1, \"directed\": 0, \"protected_class\": 1, \"national_origin\": 1}`\n*Why: Expressing pleasure about death is endorsing violence. \"Refugees\" is a generalized, non-protected group based on national origin, so `directed=0` but `protected_class=1`.*\n\n**FAILURE EXAMPLES (LEARN WHAT NOT TO DO):**\n\nFailure Input: \"Only black lives matter. fuck white people the cancer of the earth\"\n- **Incorrect Output:** `{\"directed\": 1, ...}`\n- **Correct Gold:** `{\"violence\": 1, \"directed\": 0, \"protected_class\": 1, \"race\": 1}`\n*Why Failure: \"white people\" is a generalized group, not a specific named target. It should be `directed=0`. \"cancer of the earth\" is a metaphor for societal harm, not a direct physical threat, so `violence=0` in this context.*\n\nFailure Input: \"I don’t like this gender at all. I definitely wouldn’t hire any male bitch at ma shop\"\n- **Incorrect Output:** `{\"directed\": 1, ...}`\n- **Correct Gold:** `{\"directed\": 0, \"protected_class\": 1, \"gender\": 1}`\n*Why Failure: \"any male\" refers to a generalized group, not a specific individual. It should be `directed=0`.*\n\n**PROCESS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and targets. Identify the precise basis for hostility.\n2.  **ASSESS & JUSTIFY:** For each label, provide a strict justification based on the definitions above.\n    - **First, assess `directed`:** Is the target a *specific, named* entity? Justify your score.\n    - **Second, assess `protected_class`:** Is the hostility based on a *protected class*? Justify your score.\n    - **Third, assess `violence`:** Is there a *direct threat of physical harm*? Justify your score.\n    - **Finally, assess specific class labels (e.g., `race`):** Justify each score based on the explicit basis of hostility.\n3.  **CLASSIFY:** Output your final scores in the exact JSON format: `{\"violence\": ..., \"directed\": ..., \"protected_class\": ..., \"gender\": ..., \"race\": ..., \"national_origin\": ..., \"disability\": ..., \"religion\": ..., \"sexual_orientation\": ...}`\n\n**Text to Classify:**\n`[INSERT TEXT HERE]`",
                "Q": 0.368287221992578,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Of course. Here is the rewritten prompt, designed to address the identified failure modes by incorporating stricter definitions, critical clarifications, and new, targeted examples.\n\n***\n\n### New Prompt\n\nClassify the text for hate speech by following this structured process. Apply the definitions and clarifications with extreme strictness. The provided examples are critical for understanding common pitfalls.\n\n**CRITICAL DEFINITIONS (APPLY STRICTLY):**\n\n-   **`violence` (1):** Only for direct threats, calls for, or clear incitement of **physical harm**. Do NOT score 1 for metaphors, hyperbole, or non-physical degradation (e.g., \"plague,\" sexual insults) unless they unambiguously advocate for physical harm.\n-   **`directed_vs_generalized` (1):** Score 1 **only** if the text targets a **specific, named individual** (e.g., \"John Smith\") OR a **specific, named protected group** (e.g., \"the Jews in this town,\" \"Black people\"). Score 0 for **generalized, non-specific references** to a protected class (e.g., \"all men,\" \"gay people\" as a whole) or for non-protected groups (e.g., \"refugees,\" \"politicians\").\n-   **Protected Class Labels (1):** Score 1 **only** if hostility is explicitly or implicitly based on membership in that class.\n    -   **`religion`:** Hostility based on religion. Includes attacks on core religious symbols, practices, or institutions (e.g., a holiday, holy book).\n    -   **`national_origin`:** Based on country of origin, nationality, or citizenship.\n    -   **`disability`:** Includes using disability-related terms as pejorative insults, even metaphorically or without clinical accuracy (e.g., \"retard,\" \"spaz\").\n    -   (Other definitions remain precise)\n\n**CLARIFICATIONS & EDGE CASES (NON-NEGOTIABLE):**\n\n1.  **Attacks on Symbols:** Hostility towards a core symbol, practice, or institution of a protected group (e.g., a religious holiday) **is considered** hostility based on membership in that protected class.\n2.  **Derogatory Metaphors:** Score 1 for a protected class label if a term related to that class is used as a **pejorative insult**. The intent to degrade based on that characteristic is key, regardless of clinical accuracy.\n3.  **Hypothetical Individuals:** If a text attacks a hypothetical person defined primarily by a protected characteristic (e.g., \"an incest baby,\" \"a retard\"), treat it as targeting a **specific individual** for `directed_vs_generalized` (score 1).\n\n**LEARN FROM THESE EXAMPLES (ANALYZE THEM CAREFULLY):**\n\nExample: \"I fucking hate gay and lesbians they are all animals\"\n-   Gold: `{\"directed_vs_generalized\": 0, \"sexual_orientation\": 1}`\n-   *Why: \"gay and lesbians\" is a protected class (`sexual_orientation=1`), but it is a generalized, non-specific reference, so `directed_vs_generalized=0`.*\n\nExample: \"This fcking jew wants money? I’ll give him only my dick if he wants\"\n-   Gold: `{\"violence\": 0, \"directed_vs_generalized\": 1, \"religion\": 1}`\n-   *Why: The sexual insult is degrading but not a threat of physical harm (`violence=0`). It targets a specific individual based on religion (`religion=1`, `directed_vs_generalized=1`).*\n\nExample: \"i like dead refugees on my shore\"\n-   Gold: `{\"directed_vs_generalized\": 0, \"national_origin\": 1, \"violence\": 1}`\n-   *Why: \"Refugees\" is a non-protected, generalized group (`directed_vs_generalized=0`). Hostility is based on national origin (`national_origin=1`). Expressing pleasure about death is a direct endorsement of violence (`violence=1`).*\n\n**--- NEW CRITICAL EXAMPLES ---**\n\nExample: \"What about a global massacre of men? Pussiezzz only girls\"\n-   Gold: `{\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 1}`\n-   *Why: \"men\" is a protected class (`gender=1`), but it is a generalized reference, so `directed_vs_generalized=0`. \"Massacre\" is an unambiguous call for large-scale physical harm (`violence=1`).*\n\nExample: \"Incest baby that got double-down syndrome\"\n-   Gold: `{\"violence\": 0, \"directed_vs_generalized\": 1, \"disability\": 1}`\n-   *Why: Targets a hypothetical individual defined by a disability slur (`directed_vs_generalized=1`). \"Double-down syndrome\" is a pejorative mocking disability (`disability=1`). No threat of physical harm exists (`violence=0`).*\n\nExample: \"Easter is the most silly holiday ever existed. It must be stopped.\"\n-   Gold: `{\"violence\": 0, \"directed_vs_generalized\": 0, \"religion\": 1}`\n-   *Why: Attacking a core religious holiday constitutes hostility based on religion (`religion=1`). It is not directed at a specific group or individual (`directed_vs_generalized=0`). \"Stopped\" is a rhetorical demand, not a threat of violence (`violence=0`).*\n\n**PROCESS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and targets. Precisely identify the basis for hostility, checking against the clarifications above.\n2.  **JUSTIFY:** For each label, provide a strict justification based on the definitions and clarifications. If 0, state why explicitly (e.g., \"no physical threat,\" \"targets a non-protected group,\" \"generalized protected class\").\n3.  **CLASSIFY:** Output your final scores in the exact JSON format: `{\"violence\": ..., \"directed_vs_generalized\": ..., \"gender\": ..., \"race\": ..., \"national_origin\": ..., \"disability\": ..., \"religion\": ..., \"sexual_orientation\": ...}`\n\n**Text to Classify:**\n`[INSERT TEXT HERE]`",
                "Q": 0.343287221992578,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              }
            ]
          },
          {
            "id": 3,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "Classify the text for hate speech by following this structured process. Use the examples below to understand common misclassifications and avoid similar errors.\n\n**COMMON MISCLASSIFICATIONS TO AVOID:**\n- **directed_vs_generalized=1** is ONLY for specific, named individuals or small groups (e.g., \"my neighbor John\"), NOT for large demographic groups (e.g., \"women,\" \"Black people\").\n- **violence=1** requires a direct threat or explicit call for physical harm, NOT descriptions of violence, vulgar insults, or non-violent actions.\n- Choose the MOST SPECIFIC protected class. \"Jew\" is typically `religion`; attacks on transgender people are `gender`; attacks on gay people are `sexual_orientation`.\n\n**PROCESS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and any targeted entities. Identify the precise basis for hostility. Strictly avoid over-interpretation.\n2.  **JUSTIFY:** For each label, provide a concise justification based strictly on the definitions. If a label is 0, explicitly state why (e.g., \"no direct threat,\" \"targets a large demographic group\").\n3.  **CLASSIFY:** Output your final scores in the exact JSON format.\n\n**Label Definitions:**\n- **violence (1):** Direct, explicit threat or call for physical harm. Not for insults or descriptions of violence.\n- **directed_vs_generalized (1):** Targets a specific, named individual or small, delineated group. Statements about large demographic groups (e.g., based on gender, nationality, religion) are generalized (0).\n- **gender/race/national_origin/disability/religion/sexual_orientation (1):** Hostility is explicitly and primarily based on membership in this protected class.\n\n**EXAMPLES:**\nText: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\nAnalysis: Targets a large demographic group (Black people) based on race. No direct threat of violence.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nText: \"This fcking jew wants money? I’ll give him only my dick if he wants\"\nAnalysis: Targets a specific individual based on religion (Jewish). Vulgar insult but no direct threat of physical harm.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\nText: \"Women are made for making babies and cooking dinner and nothing else!!!\"\nAnalysis: Targets a large demographic group (women) based on gender. No violence.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n**Text to Classify:**",
            "Q": 1.400214894459889,
            "N": 4,
            "uct_value": 0.3536587836213616,
            "reward": 0.1,
            "children": [
              {
                "id": 4,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by following this structured process exactly.\n\n**CRITICAL REASONING RULES:**\n*   **Be Literal:** Base analysis only on explicit words. Avoid inference and emotional reasoning.\n*   **Apply Definitions Strictly:** If the text does not meet a definition exactly, the label is 0.\n*   **Most Specific Class:** Identify the single, most specific protected class that is the explicit basis for hostility.\n\n**PROCESS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and the exact target of hostility (e.g., \"a specific 'you'\", \"the group 'Black people'\").\n2.  **JUSTIFY:** For each label, provide a concise justification. You MUST use this phrasing for zeros:\n    *   `violence`: \"Direct threat of harm? [Yes/No]. Therefore, violence = [0/1].\"\n    *   `directed_vs_generalized`: \"Target is a specific individual/small group? [Yes/No]. Therefore, directed_vs_generalized = [0/1].\"\n    *   For protected classes: \"Explicit hostility based on [class]? [Yes/No]. Therefore, [class] = [0/1].\"\n3.  **CLASSIFY:** Output your final scores in the exact JSON format.\n\n**LABEL DEFINITIONS (STRICT):**\n-   **violence (1):** ONLY for a direct, explicit threat or call for physical harm (e.g., \"I will stab you,\" \"kill them\"). Insults, vulgarity, or descriptions of violence are 0.\n-   **directed_vs_generalized (1):** ONLY if the target is a specific, named individual or a small, named group (e.g., \"my neighbor John\"). Attacks on large demographic groups (e.g., \"women,\" \"Black people\") are ALWAYS 0.\n-   **Protected Classes (1):** ONLY if hostility is explicitly and primarily based on this class. Choose the MOST SPECIFIC one (e.g., \"Jew\" -> `religion`; \"transgender\" -> `gender`; \"refugee\" -> `national_origin`; \"HIV\" -> `disability`).\n\n**Text to Classify:**",
                "Q": 0.42250636838976297,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by following this structured process. Use the examples below to understand common misclassifications and avoid similar errors.\n\n**COMMON MISCLASSIFICATIONS TO AVOID:**\n- **directed_vs_generalized=1** is ONLY for specific, named individuals (e.g., \"my neighbor John\") OR direct, second-person addresses (e.g., \"you are a stupid trans\"). It is NOT for large demographic groups (e.g., \"women,\" \"Black people\").\n- **CRITICAL:** A statement using a singular term (e.g., \"a Jew,\" \"a white boy\") as a stand-in for an entire demographic group is GENERALIZED (0), not directed.\n- **violence=1** requires a direct, explicit threat or call for physical harm. It is NOT for descriptions of violence, vulgar insults, or non-violent actions.\n- Choose the MOST SPECIFIC protected class. \"Jew\" is typically `religion`; attacks on transgender people are `gender`; attacks on gay people are `sexual_orientation`.\n\n**PROCESS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and any targeted entities. Determine if the target is a named individual, a direct address (\"you\"), or if a singular term is being used to represent a large group. Identify the precise basis for hostility. Strictly avoid over-interpretation.\n2.  **JUSTIFY:** For each label, provide a concise justification based strictly on the definitions. If a label is 0, explicitly state why (e.g., \"no direct threat,\" \"uses a singular term to represent a large group\").\n3.  **CLASSIFY:** Output your final scores in the exact JSON format.\n\n**Label Definitions:**\n- **violence (1):** Direct, explicit threat or call for physical harm. Not for insults or descriptions of violence.\n- **directed_vs_generalized (1):** Targets a specific, named individual (e.g., \"my neighbor John\") OR is a direct, second-person address (e.g., \"you are a stupid trans\"). Statements about large, unnamed demographic groups are generalized (0). A statement using a singular noun as a stand-in for the entire group is GENERALIZED (0).\n- **gender/race/national_origin/disability/religion/sexual_orientation (1):** Hostility is explicitly and primarily based on membership in this protected class.\n\n**EXAMPLES:**\nText: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\nAnalysis: Targets a large demographic group (Black people) based on race. No direct threat of violence.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nText: \"This fcking jew wants money? I’ll give him only my dick if he wants\"\nAnalysis: Targets a specific individual based on religion (Jewish). Vulgar insult but no direct threat of physical harm.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\nText: \"Women are made for making babies and cooking dinner and nothing else!!!\"\nAnalysis: Targets a large demographic group (women) based on gender. No violence.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n**FAILURE CASE EXAMPLE TO LEARN FROM:**\nText: \"kneel down, white boy. its the era of brown people now\"\nIncorrect Analysis: (Previous error: Misclassified as directed_vs_generalized=0)\nCorrect Analysis: The command \"kneel down\" uses the singular \"white boy\" as a stand-in for the entire white race, representing a generalized power shift. It is not a personal address to a specific individual. No direct threat of violence is made.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\n**Text to Classify:**",
                "Q": 0.347506368389763,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 6,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by following this structured process. Use the examples below to understand common misclassifications and avoid similar errors.\n\n**COMMON MISCLASSIFICATIONS TO AVOID:**\n- **directed_vs_generalized=1** is ONLY for specific, named individuals or small, delineated groups (e.g., \"my neighbor John\"), NOT for large demographic groups (e.g., \"women,\" \"Black people\").\n- **violence=1** requires a direct threat, explicit call for physical harm, or clear endorsement/glorification of violence. Not for mere insults, non-violent actions, or neutral descriptions of violence.\n- A text can be labeled for **multiple protected classes** if hostility is based on multiple factors (e.g., both `national_origin` and `religion`).\n- Attacks on Jewish people are typically `religion`; attacks on transgender people are `gender`; attacks on gay people are `sexual_orientation`.\n\n**KEY CONSIDERATIONS:**\n- **Implied Targets:** The target may not be explicitly named. Infer from context clues (e.g., \"they are changing our culture\" often implies immigrants or religious minorities).\n- **Dehumanizing Language:** Terms like \"parasites,\" \"vermin,\" \"plague,\" or \"animals\" are strong indicators of hate speech and may imply calls for violence.\n- **Calls to Action:** Phrases like \"we must stop them\" or \"fight back\" can be implicit calls for violence when paired with dehumanizing language.\n\n**PROCESS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and any targeted entities. Identify the precise basis for hostility. Pay close attention to dehumanizing language, implied calls to action, and whether multiple protected classes are targeted. Make reasonable inferences based on context but avoid over-interpretation.\n2.  **JUSTIFY:** For each label, provide a concise justification based strictly on the definitions. If a label is 0, explicitly state why (e.g., \"no direct threat,\" \"targets a large demographic group\").\n3.  **CLASSIFY:** Output your final scores in the exact JSON format.\n\n**Label Definitions:**\n- **violence (1):** Direct threat, explicit call for physical harm, or clear endorsement/glorification of violence against a target. Includes expressing a desire for violence to happen.\n- **directed_vs_generalized (1):** Targets a specific, named individual or small, delineated group. Statements about large demographic groups are generalized (0).\n- **gender/race/national_origin/disability/religion/sexual_orientation (1):** Hostility is explicitly or implicitly based on membership in this protected class.\n\n**EXAMPLES:**\nText: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\nAnalysis: Targets a large demographic group (Black people) based on race. No direct threat or endorsement of violence.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nText: \"This fcking jew wants money? I'll give him only my dick if he wants\"\nAnalysis: Targets a specific individual based on religion (Jewish). Vulgar insult but no direct threat or endorsement of physical harm.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\nText: \"We need to get rid of these parasites. They swarm in, steal our jobs, and bring crime. Stop them at the border before it's too late!\"\nAnalysis: Uses dehumanizing language (\"parasites\") and implies a group based on national origin/immigrant status. The phrase \"stop them before it's too late\" is an implicit call to action that suggests violence.\nJSON: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\n\nText: \"Dirty Hindu immigrants from India are taking over our tech jobs and undercutting our wages.\"\nAnalysis: Targets a large group based on both religion (\"Hindu\") and national origin (\"from India\", \"immigrants\"). No violence.\nJSON: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\n\n**Text to Classify:**",
                "Q": 0.42250636838976297,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              }
            ]
          },
          {
            "id": 4,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "Classify the text for hate speech by following this structured process. Carefully study the examples to avoid common mistakes.\n\n**CRITICAL GUIDELINES:**\n- **For `directed_vs_generalized`:** Score 1 **only** if the text targets a **specific, named individual** (e.g., \"John Smith\") or a **small, named group** (e.g., \"the managers at Tesla\"). Score 0 for all statements about large, demographic groups (e.g., \"women,\" \"white people,\" \"lesbians\").\n- **Apply the Rule of Specificity:** If hostility is based on a specific protected class (e.g., `sexual_orientation`), do **not** mark a broader class (e.g., `gender`) unless it is independently and explicitly targeted.\n- **`gender_identity` is distinct:** Hostility toward transgender or non-binary individuals is classified under `gender_identity`, not `gender` or `sexual_orientation`.\n\n**REASONING STEPS:**\n1.  **ANALYZE:** Describe the text's tone, intent, and the target of the hostility. Identify the precise basis for the hostility (e.g., national origin, religion, gender identity).\n2.  **JUSTIFY:** For each label, provide a brief justification based on the definitions below. If a label is 0, explicitly state why (e.g., \"no threat of violence,\" \"target is a generalized group\").\n3.  **CLASSIFY:** Output your final scores in the exact JSON format.\n\n**LEARN FROM THESE EXAMPLES:**\n\n**Example 1:**\n**Text:** \"Any lesbian that doesn't look like a complete hog actually wants to be with a man.\"\n**Gold JSON:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1, \"gender_identity\": 0}`\n**Why:** The target (\"lesbians\") is a large demographic group, so `directed_vs_generalized=0`. The attack is based on sexual orientation, not gender, so `sexual_orientation=1`, `gender=0`.\n\n**Example 3:**\n**Text:** \"Racist ass white ass people\"\n**Gold JSON:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0, \"gender_identity\": 0}`\n**Why:** The target (\"white people\") is a large demographic group, so `directed_vs_generalized=0`. The attack is based on race, so `race=1`.\n\n**Example 5:**\n**Text:** \"An ugly man in a wig and dress, is still just an ugly man in a wig and dress... NOT A WOMAN!!!\"\n**Gold JSON:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0, \"gender_identity\": 1}`\n**Why:** The target is a generalized concept, not a named individual, so `directed_vs_generalized=0`. The attack is on gender identity, not sexual orientation or gender, so `gender_identity=1`.\n\n**Label Definitions:**\n- **`violence` (1):** Direct threat, call for, or strong implication of physical harm or self-harm.\n- **`directed_vs_generalized` (1):** Targets a **specific, named individual** or a **small, named group**. (0) for all large, demographic groups.\n- **`gender` (1):** Hostility based on **sex or gender roles** (e.g., \"women are bad drivers\").\n- **`race` (1):** Hostility based on **race or ethnicity** (e.g., \"Black,\" \"Asian\").\n- **`national_origin` (1):** Hostility based on **country of origin, citizenship, or nationality** (e.g., \"Mexicans\").\n- **`disability` (1):** Hostility based on **physical, mental, or developmental disability**.\n- **`religion` (1):** Hostility based on **religious affiliation, beliefs, or practices**.\n- **`sexual_orientation` (1):** Hostility based on **romantic or sexual attraction** (e.g., \"gay,\" \"lesbian\").\n- **`gender_identity` (1):** Hostility based on **gender identity or expression** (e.g., \"transgender,\" misgendering).\n\n**Text to Classify:**",
            "Q": 0.3743624559572667,
            "N": 1,
            "uct_value": 0.3779675213712366,
            "reward": 0.05,
            "children": []
          }
        ]
      },
      {
        "id": 2,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert hate speech classifier. Analyze the text strictly according to the following guidelines and output ONLY a valid JSON object.\n\n**CRITICAL GUIDELINES:**\n- **Violence (violence):** Only mark as 1 for explicit calls for physical harm, injury, death, or direct threats. Do NOT mark for aggressive tone, anger, insults, or harmful metaphors (e.g., \"parasite,\" \"plague\").\n- **Directed vs Generalized (directed_vs_generalized):** Mark as 1 ONLY if the attack is targeted at a specific, identifiable individual. Mark as 0 for attacks against groups or non-specific persons.\n- **Gender (gender):** Mark as 1 only if the primary attack is based on gender. A gendered slur (e.g., \"cunt\") used as a general insult may not be sufficient if the comment's main focus is not gender.\n- **Race (race):** Mark as 1 for attacks based on race/ethnicity or use of racial slurs.\n- **National Origin (national_origin):** Mark as 1 for attacks based on nationality, citizenship, or immigrant status. If combined with racism (e.g., \"go back to Africa\"), prioritize the `race` label unless the national origin is the distinct focus.\n- **Religion (religion):** For attacks on Jewish people, prioritize this label unless the text explicitly uses racial terminology.\n- **Base decisions strictly on explicit evidence in the text. Do not infer context.**\n\n**FAILURE EXAMPLES TO AVOID:**\n- Input: \"Racist ass white ass people\" → INCORRECT: `\"violence\":1` → CORRECT: `\"violence\":0` (no explicit threat)\n- Input: \"Fat lesbians\" → INCORRECT: `\"gender\":1, \"directed_vs_generalized\":1` → CORRECT: `\"gender\":0, \"directed_vs_generalized\":0` (group insult, not specifically gender-focused)\n- Input: \"FUCKING NIGGER GO BACK TO AFRICA\" → INCORRECT: `\"national_origin\":1` → CORRECT: `\"national_origin\":0` (prioritize `race` for this racist trope)\n\n**Output Format:** ONLY a valid JSON object with keys: \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
        "Q": 3.149514553705642,
        "N": 10,
        "uct_value": 0.32939775942307126,
        "reward": 0.05,
        "children": [
          {
            "id": 3,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "You are an expert hate speech classifier. Analyze the text strictly and output ONLY a valid JSON object. Follow this reasoning process step-by-step:\n\n**1. VIOLENCE (violence=1 ONLY for explicit calls to physical harm, injury, death, or direct threats). Ignore aggressive tone, insults, or metaphors.**\n**2. TARGET (directed_vs_generalized=1 ONLY if attacking a specific, identifiable individual).**\n**3. PRIMARY CATEGORY (Choose ONLY ONE main category in this priority order):**\n   - `religion`: For attacks on Jewish people or religious groups\n   - `race`: For racial slurs or race/ethnicity-based attacks\n   - `national_origin`: For nationality/immigrant status attacks (prioritize `race` if combined)\n   - `gender`: ONLY if gender is the primary focus of attack\n   - `sexual_orientation`/`disability`: For attacks based on these characteristics\n**4. EVIDENCE: Base decisions strictly on explicit text. Do not infer context.**\n\n**Output Format:** ONLY a valid JSON object with keys: \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
            "Q": 1.242041148472524,
            "N": 4,
            "uct_value": 0.36997101585316255,
            "reward": 0.05,
            "children": [
              {
                "id": 4,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classifier. Analyze the text strictly and output ONLY a valid JSON object. Follow this reasoning process step-by-step:\n\n**Step 1: VIOLENCE (violence=1 ONLY for explicit calls to physical harm, injury, death, or direct threats). Ignore aggressive tone, insults, or metaphors.**\n**Step 2: TARGET (directed_vs_generalized=1 ONLY if attacking a specific, identifiable individual).**\n**Step 3: PRIMARY CATEGORY (Choose ONLY ONE main category in this priority order):**\n   - `religion`: For attacks on Jewish people or religious groups\n   - `race`: For racial slurs or race/ethnicity-based attacks\n   - `national_origin`: For nationality/immigrant status attacks (prioritize `race` if combined)\n   - `gender`: ONLY if gender is the primary focus of attack\n   - `sexual_orientation`/`disability`: For attacks based on these characteristics\n**Step 4: EVIDENCE: Base decisions strictly on explicit text. Do not infer context. Ignore speaker identity - focus on the target of attack.**\n\n**CRITICAL: If no protected characteristic is explicitly attacked, ALL category values must be 0.**\n\n**Output Format:** ONLY a valid JSON object with keys: \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
                "Q": 0.34189173267777706,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are a hate speech classification algorithm. Analyze the text and output ONLY a valid JSON object. Follow this process strictly and in order:\n\n**STEP 1: VIOLENCE (violence=1 ONLY for explicit calls to physical harm, injury, death, or direct threats). Ignore aggressive tone, insults, or metaphors.**\n**STEP 2: TARGET (directed_vs_generalized=1 ONLY if attacking a specific, identifiable individual).**\n**STEP 3: PRIMARY CATEGORY (Choose the SINGLE most applicable category based on this priority list):**\n   1. `religion`: Attacks on Jewish people or religious groups\n   2. `race`: Racial slurs or race/ethnicity-based attacks\n   3. `national_origin`: Nationality/immigrant status attacks (prioritize `race` if combined)\n   4. `gender`: ONLY if gender is the primary focus\n   5. `sexual_orientation`/`disability`: Attacks based on these characteristics\n   - Set the chosen category to `1` and all others to `0`.\n\n**CRITICAL: Base decisions STRICTLY on explicit text. IGNORE sarcasm, irony, tone, and inferred context.**\n\n**Output Format:** {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: {{INPUT_TEXT}}",
                "Q": 0.3918917326777771,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 6,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classifier. Analyze the text strictly and output ONLY a valid JSON object. Follow this reasoning process **step-by-step**:\n\n**1. VIOLENCE (violence=1 ONLY for explicit calls to physical harm, injury, death, or direct threats).** Ignore metaphors (\"virus,\" \"cancer\"), hyperbolic language (\"I could kill you\"), aggressive tone, or insults.\n\n**2. TARGET (directed_vs_generalized=1 ONLY if attacking a specific, identifiable individual).** Set to 0 for groups or non-specific persons.\n\n**3. PRIMARY CATEGORY (Choose ONLY ONE main category in this strict priority order):**\n   - `religion`: Attacks on religious groups (e.g., \"Jews,\" \"Muslims\")\n   - `race`: Racial slurs or race/ethnicity-based attacks\n   - `national_origin`: Nationality/immigrant status attacks (prioritize `race` if combined)\n   - `gender`: ONLY if gender is the primary and explicit focus of attack\n   - `sexual_orientation`: Attacks based on sexual orientation (e.g., \"faggots,\" \"lgbt\")\n   - `disability`: Attacks based on disability (e.g., \"autistic people,\" \"retards\")\n   \n**4. EVIDENCE: Base decisions strictly on explicit text. Do not infer context, intent, or unstated meanings.**\n\n**Output Format:** ONLY a valid JSON object with keys: \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
                "Q": 0.34189173267777706,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              }
            ]
          },
          {
            "id": 4,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert hate speech classifier. Analyze the text strictly according to the following guidelines and output ONLY a valid JSON object with no additional text or formatting.\n\n**CRITICAL GUIDELINES:**\n- **Violence (violence):** Mark as 1 for explicit calls for physical harm, injury, death, or direct threats. Also mark as 1 for imminent calls for exclusion or action that strongly implies physical prevention or confrontation (e.g., \"keep them out,\" \"stop them now\"). Do NOT mark for aggressive tone, anger, insults, or harmful metaphors (e.g., \"parasite,\" \"plague\").\n- **Directed vs Generalized (directed_vs_generalized):** Mark as 1 if the attack uses a singular pronoun (\"you,\" \"he,\" \"she\") or is addressed to a specific, named individual. Mark as 0 for attacks against groups (e.g., \"women,\" \"immigrants,\" \"they\") or non-specific persons.\n- **Gender (gender):** Mark as 1 if the core reason for the attack is based on gender or if a gendered slur is used. A gendered slur (e.g., \"cunt\") used as a general insult may not be sufficient if the comment's main focus is not gender.\n- **Race (race):** Mark as 1 for attacks based on race/ethnicity or use of racial slurs.\n- **National Origin (national_origin):** Mark as 1 for attacks based on nationality, citizenship, or immigrant status.\n- **Disability (disability):** Mark as 1 for attacks based on physical, cognitive, or mental disabilities, or the use of related slurs.\n- **Religion (religion):** Mark as 1 for attacks based on religion or use of religious slurs.\n- **Sexual Orientation (sexual_orientation):** Mark as 1 for attacks based on sexual orientation or use of related slurs.\n\n**PRIORITIZATION RULE:** If text qualifies for multiple labels, assign 1 to ALL that apply. Do not try to choose only one.\n\n**STRICT EVIDENCE RULE:** Base decisions strictly on explicit evidence in the text. Do not infer context.\n\n**FAILURE EXAMPLES TO AVOID:**\n1. Input: \"Racist ass white ass people\" → INCORRECT: `\"violence\":1` → CORRECT: `\"violence\":0` (no explicit threat)\n2. Input: \"Fat lesbians\" → INCORRECT: `\"gender\":1, \"directed_vs_generalized\":1` → CORRECT: `\"gender\":0, \"directed_vs_generalized\":0` (group insult)\n3. Input: \"FUCKING NIGGER GO BACK TO AFRICA\" → INCORRECT: `\"national_origin\":1` → CORRECT: `\"race\":1, \"national_origin\":0` (prioritize race for racist trope)\n4. Input: \"All girls like you should have a pimp...\" → INCORRECT: `\"directed_vs_generalized\":0` → CORRECT: `\"directed_vs_generalized\":1` (\"you\" indicates specific target)\n5. Input: \"Wish you cut your veins. Don't shout out you have mental problems.\" → INCORRECT: `\"disability\":0` → CORRECT: `\"disability\":1` (\"mental problems\" is disability-based attack)\n6. Input: \"Stop them now while you still can.\" → INCORRECT: `\"violence\":0` → CORRECT: `\"violence\":1` (imminent call to action implying physical confrontation)\n\n**Output Format:** ONLY a valid JSON object with keys: \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
            "Q": 0.3663659504391926,
            "N": 1,
            "uct_value": 0.36997101585316255,
            "reward": 0.05,
            "children": []
          },
          {
            "id": 5,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert hate speech classifier. Analyze the text strictly according to the following guidelines and output ONLY a valid JSON object.\n\n**CRITICAL GUIDELINES:**\n- **Base decisions strictly on explicit evidence in the text. Do not infer context.**\n- **A text can be marked for multiple categories (e.g., 1 for both `gender` and `sexual_orientation`).**\n\n**Category Definitions:**\n- **Violence (violence):** Mark as 1 ONLY for explicit, literal calls for physical harm, injury, death, or direct threats. Mark as 0 for aggressive tone, anger, insults, or harmful metaphors (e.g., \"parasite,\" \"plague,\" \"kill yourself\").\n- **Directed vs Generalized (directed_vs_generalized):** Mark as 1 ONLY if the attack is targeted at a specific, identifiable individual (e.g., using a name, @handle). Mark as 0 for attacks against groups, non-specific persons, or \"you\" when it's a generic insult.\n- **Gender (gender):** Mark as 1 for attacks based on gender, including:\n    - Misogyny (hatred of women) or misandry (hatred of men).\n    - Transphobia (attacks on transgender or non-binary people, e.g., questioning their identity, deadnaming, \"it's a man in a dress\").\n    - Use of gendered slurs (e.g., \"bitch,\" \"cunt\") **when the attack is fundamentally about gender**.\n- **Sexual Orientation (sexual_orientation):** Mark as 1 for attacks based on real or perceived sexual orientation (e.g., gay, lesbian, bisexual), including the use of homophobic slurs.\n- **Race (race):** Mark as 1 for attacks based on race/ethnicity or use of racial slurs.\n- **National Origin (national_origin):** Mark as 1 for attacks based on nationality, citizenship, or immigrant status (e.g., \"go back to your country\"). If combined with racism (e.g., \"go back to Africa\"), mark BOTH `race` and `national_origin` as 1.\n- **Religion (religion):** Mark as 1 for attacks based on religion or use of religious slurs. For attacks on Jewish people, mark this as 1 unless the text explicitly uses racial terminology (e.g., \"black Jews\"), then mark `race`.\n- **Disability (disability):** Mark as 1 for attacks based on physical or mental disability.\n\n**FAILURE EXAMPLES TO AVOID:**\n- Input: \"Two things are apparent: one, not a male and two, your ass is indeed sorry that it's been turned into a drive-in.\"\n  - INCORRECT: `\"gender\":1, \"sexual_orientation\":0`\n  - CORRECT: `\"gender\":1, \"sexual_orientation\":1` (targets both gender identity and implies homosexual acts)\n- Input: \"Fuck that shit this bitch ugly and she has a penis,oh hell nawww!!!\"\n  - INCORRECT: `\"gender\":1, \"sexual_orientation\":1`\n  - CORRECT: `\"gender\":1, \"sexual_orientation\":0` (\"bitch\" is generic insult; primary attack is transphobic \"has a penis\")\n- Input: \"Ugly dyke\"\n  - INCORRECT: `\"gender\":1`\n  - CORRECT: `\"gender\":0, \"sexual_orientation\":1` (slur targets sexual orientation, not gender itself)\n\n**Output Format:** Output MUST be a valid JSON object with the following keys and integer values (0 or 1): \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
            "Q": 1.2966889681149736,
            "N": 4,
            "uct_value": 0.3277773020351328,
            "reward": 0.05,
            "children": [
              {
                "id": 6,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classifier. Analyze the text strictly according to the following guidelines and output ONLY a valid JSON object.\n\n**CRITICAL GUIDELINES:**\n- **Base decisions strictly on explicit evidence in the text. Do not infer context.**\n- **A text can be marked for multiple categories (e.g., 1 for both `race` and `national_origin`).**\n\n**Category Definitions:**\n- **Violence (violence):** Mark as 1 ONLY for explicit, literal calls for physical harm, injury, death, or direct threats. Mark as 0 for aggressive tone, anger, insults, or harmful metaphors (e.g., \"kill yourself\" is a metaphor, not literal violence).\n- **Directed vs Generalized (directed_vs_generalized):** Mark as 1 ONLY if the attack is targeted at a specific, identifiable individual (e.g., using a name, @handle). Mark as 0 for attacks against groups, non-specific persons, or generic \"you\".\n- **Gender (gender):** Mark as 1 for attacks based on gender, including:\n    - Misogyny (hatred of women) or misandry (hatred of men).\n    - Transphobia (attacks on transgender or non-binary people, e.g., questioning their identity, deadnaming).\n    - Use of gendered slurs (e.g., \"bitch,\" \"cunt\") **only when the attack is fundamentally about gender**.\n- **Sexual Orientation (sexual_orientation):** Mark as 1 for attacks based on real or perceived sexual orientation (e.g., gay, lesbian, bisexual) or use of homophobic slurs (e.g., \"dyke\").\n- **Race (race):** Mark as 1 for attacks based on race/ethnicity or use of racial slurs.\n- **National Origin (national_origin):** Mark as 1 for attacks based on nationality, citizenship, or immigrant status (e.g., \"go back to your country\"). If combined with racism (e.g., \"go back to Africa\"), mark BOTH `race` and `national_origin` as 1.\n- **Religion (religion):** Mark as 1 for attacks based on religion or use of religious slurs.\n- **Disability (disability):** Mark as 1 for attacks based on physical or mental disability or use of slurs (e.g., \"retard\").\n\n**KEY DISTINCTIONS:**\n- **\"dyke\" → `sexual_orientation`:1, `gender`:0**\n- **\"retard\" → `disability`:1**\n- **\"kill yourself\" → `violence`:0 (metaphor)**\n- **Attack on \"lesbians\" → `sexual_orientation`:1, `directed_vs_generalized`:0**\n\n**Output Format:** Output MUST be a valid JSON object with the following keys and integer values (0 or 1): \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
                "Q": 0.4517743392252603,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.15,
                "children": []
              },
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classifier. Analyze the text strictly and output ONLY a valid JSON object. Follow these steps:\n\n**CORE RULES:**\n1.  **EVIDENCE-BASED:** Base decisions ONLY on explicit, literal evidence. DO NOT INFER context or intent.\n2.  **CATEGORY INDEPENDENCE:** Mark multiple categories as 1 if evidence exists for each. Categories are independent.\n3.  **GENERALIZED TARGET:** `directed_vs_generalized:1` ONLY for specific, named individuals (@handle, name). Otherwise `0`.\n\n**CATEGORY DEFINITIONS (Mark 1 ONLY if evidence matches):**\n-   **`violence`:** Explicit calls for physical harm, injury, death, or direct threats. Mark 0 for insults, anger, or metaphors (e.g., \"virus\", \"plague\").\n-   **`gender`:** Attacks based on gender (misogyny, misandry, transphobia) or gendered slurs **if the attack is about gender**.\n-   **`sexual_orientation`:** Attacks based on sexual orientation or homophobic slurs.\n-   **`race`:** Attacks based on race/ethnicity or racial slurs.\n-   **`national_origin`:** Attacks based on nationality, citizenship, or immigrant status.\n-   **`religion`:** Attacks based on religion or religious slurs.\n-   **`disability`:** Attacks based on physical or mental disability.\n\n**DECISION PROCESS:**\n1.  Check for explicit calls for harm → `violence`\n2.  Identify if target is a specific individual → `directed_vs_generalized`\n3.  Scan for explicit evidence matching each category definition → set `1` or `0`\n\n**Output MUST be valid JSON with integer values (0/1) for these keys:** \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
                "Q": 0.35177433922526025,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 8,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classifier. Analyze the text strictly according to the following guidelines and output ONLY a valid JSON object.\n\n**CRITICAL GUIDELINES:**\n- **Base decisions strictly on explicit evidence in the text. Do not infer context.**\n- **A text can be marked for multiple categories (e.g., 1 for both `race` and `national_origin`).**\n\n**CATEGORY DEFINITIONS:**\n- **Violence (violence):** Mark as 1 ONLY for explicit, literal calls for physical harm, injury, death, or direct threats. Mark as 0 for aggressive tone, anger, insults, or harmful metaphors (e.g., \"parasite,\" \"plague,\" \"kill yourself\").\n- **Directed vs Generalized (directed_vs_generalized):** Mark as 1 ONLY if the attack is targeted at a specific, identifiable individual (e.g., using a name, @handle). Mark as 0 for attacks against groups, non-specific persons, or \"you\" when it's a generic insult.\n- **Gender (gender):** Mark as 1 for attacks based on gender, including:\n    - Misogyny, misandry, or transphobia (e.g., questioning identity, deadnaming).\n    - Use of gendered slurs (e.g., \"bitch,\" \"cunt\") **only when the attack is fundamentally about gender**.\n- **Sexual Orientation (sexual_orientation):** Mark as 1 for attacks based on real or perceived sexual orientation or use of homophobic slurs.\n- **Race (race):** Mark as 1 for attacks based on race/ethnicity or use of racial slurs.\n- **National Origin (national_origin):** Mark as 1 for attacks based on nationality, citizenship, or immigrant status (e.g., \"go back to your country\"). If combined with racism, mark BOTH `race` and `national_origin` as 1.\n- **Religion (religion):** Mark as 1 for attacks based on religion or use of religious slurs.\n- **Disability (disability):** Mark as 1 for attacks based on physical or mental disability.\n\n**REASONING STEPS:**\n1.  **Scan for Violence:** Identify explicit, literal commands or desires for physical harm.\n2.  **Identify the Target:** Determine if the attack is against a specific, named individual (`directed_vs_generalized:1`) or a group/non-specific entity (`directed_vs_generalized:0`).\n3.  **Map Target to Category:** Based on explicit evidence, map the target to the correct category/ies (e.g., attack on religion -> `religion:1`).\n4.  **Analyze Slurs in Context:** For slurs, determine if the attack is fundamentally about the protected characteristic (mark category) or just a generic insult (do not mark category).\n5.  **Default to 0:** If no explicit evidence exists for a category, its value is `0`.\n\n**OUTPUT FORMAT:** Output MUST be a valid JSON object with the following keys and integer values (0 or 1): \"violence\", \"directed_vs_generalized\", \"gender\", \"race\", \"national_origin\", \"disability\", \"religion\", \"sexual_orientation\".\n\nText to classify: {{INPUT_TEXT}}",
                "Q": 0.35177433922526025,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              }
            ]
          }
        ]
      },
      {
        "id": 3,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and examples:\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit calls for, glorification of, or celebration of physical harm, injury, or death. Systemic issues (e.g., incarceration) alone do not qualify.\n- **directed_vs_generalized**: 1 = targeted at specific individual(s); 0 = targeted at broad group\n- **gender (1)**: Attacks based on gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on race or perceived race\n- **national_origin (1)**: Attacks based on origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on physical/mental disabilities\n- **religion (1)**: Attacks based on religious beliefs\n- **sexual_orientation (1)**: Attacks based on sexual orientation\n\n**CRITICAL GUIDELINES:**\n1. Be conservative: Only assign 1 if text unambiguously matches definition\n2. No inference: Only label explicitly mentioned or unambiguously implied categories\n3. \"Behind bars\" refers to incarceration, not physical violence\n4. Criticism of religion ≠ automatic violence classification\n5. Generalized group threats are directed_vs_generalized: 0\n\n**LEARN FROM THESE EXAMPLES:**\n\nExample: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Incarceration references are not physical violence\n\nExample: \"Islam is a religion of hate. These propaganda type videos are dishonest.\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: Religious criticism without violent calls ≠ violence\n\nExample: \"What about a global massacre of men? Pussiezzz only girls\"\nCorrect: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"Global massacre\" targets entire gender group (generalized)\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: “[INSERT TEXT HERE]”",
        "Q": 3.2090319428177585,
        "N": 10,
        "uct_value": 0.3254667531920887,
        "reward": 0.1,
        "children": [
          {
            "id": 4,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and rules:\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit calls for, glorification of, or celebration of physical harm, injury, or death (e.g., kill, murder, genocide, massacre, harm, beat, exterminate). Systemic issues (e.g., incarceration) alone do not qualify.\n- **directed_vs_generalized**: 1 = targeted at specific individual(s) using \"you\" or names; 0 = targeted at broad group\n- **gender (1)**: Attacks based on gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on race or perceived race\n- **national_origin (1)**: Attacks based on origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on physical/mental disabilities\n- **religion (1)**: Attacks based on religious beliefs\n- **sexual_orientation (1)**: Attacks based on sexual orientation\n\n**CRITICAL RULES:**\n1. Be conservative: Only assign 1 if text unambiguously matches definition\n2. Explicit mention required: Only label categories with explicitly mentioned identifiers\n3. \"Behind bars\" refers to incarceration, not physical violence\n4. Criticism of religion ≠ automatic violence classification\n5. A single phrase can trigger multiple categories simultaneously\n6. Evaluate each category independently using explicit evidence\n\n**LEARN FROM EXAMPLES:**\n\nExample: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\n→ {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Incarceration references ≠ violence; explicit race mention\n\nExample: \"With a face like that I bet you could scare your HIV into remission.\"\n→ {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Targeted attack (\"you\") with explicit disability mention\n\nExample: \"MASS BLACK WORLDWIDE GENOCIDE IS OUR ONLY HOPE\"\n→ {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Explicit genocide call (violence) targeting racial group (generalized)\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: “[INSERT TEXT HERE]”",
            "Q": 1.3602011709276574,
            "N": 4,
            "uct_value": 0.3436553527383037,
            "reward": 0.15,
            "children": [
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and rules by following this exact decision process:\n\n**DECISION PROCESS:**\n1.  **Violence First:** Check ONLY for explicit keywords: kill, murder, genocide, massacre, harm, beat, exterminate. If present, `\"violence\": 1`. Otherwise, `0`. \"Behind bars\" means incarceration, not violence.\n2.  **Target Second:** Is the attack aimed at a specific person using \"you\" or a name? If yes, `\"directed_vs_generalized\": 1`. If at a broad group, `0`.\n3.  **Categories Last:** Independently check for explicit mentions of: `gender, race, national_origin, disability, religion, sexual_orientation`. Only set to `1` if the identifier is literally in the text.\n\n**CRITICAL RULES:**\n- **Be Conservative:** Default is 0. Only assign 1 for unambiguous, explicit matches.\n- **No Inference:** Rely solely on explicit keywords. Ignore tone, sarcasm, and implication.\n- **Independent Evaluation:** A single phrase can trigger multiple categories. Evaluate each one separately.\n\n**CONFIRM WITH EXAMPLES:**\n\nExample: \"MASS BLACK WORLDWIDE GENOCIDE IS OUR ONLY HOPE\"\n→ {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"GENOCIDE\" = violence; \"BLACK\" = race; targets a broad group.\n\nExample: \"kneel down, white boy. its the era of brown people now\"\n→ {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: No violent keywords; \"white boy\" targets a specific individual (\"boy\") and mentions race.\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: “[INSERT TEXT HERE]”",
                "Q": 0.3791664313351385,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 6,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and rules:\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: ONLY explicit calls for, glorification of, or celebration of physical harm, injury, or death (e.g., kill, murder, genocide, massacre, harm, beat, exterminate). Systemic issues (e.g., incarceration, \"behind bars\") or non-physical insults do NOT qualify.\n- **directed_vs_generalized**: 1 = targeted at specific individual(s) using \"you\" or names; 0 = targeted at broad group\n- **gender (1)**: Attacks based on explicitly mentioned gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on explicitly mentioned race or perceived race\n- **national_origin (1)**: Attacks based on explicitly mentioned origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on explicitly mentioned physical/mental disabilities\n- **religion (1)**: Attacks based on explicitly mentioned religious beliefs\n- **sexual_orientation (1)**: Attacks based on explicitly mentioned sexual orientation\n\n**CRITICAL RULES:**\n1. **Be conservative**: Only assign 1 if text unambiguously matches definition\n2. **Explicit mention required**: Never infer identifiers. If not explicitly mentioned, category is 0\n3. **Literal violence interpretation**: Only physical harm qualifies. \"Behind bars\" = incarceration ≠ violence\n4. **Independent evaluation**: Check each category separately based on its own evidence\n5. **Directed check**: Look for \"you/your\" or names for directed_vs_generalized: 1\n6. **Attack definition**: Insult + explicit identifier = attack for that category\n\n**LEARN FROM EXAMPLES:**\n\nExample: \"Ima fucking stab u up bruv. Fucking white prick bruv\"\n→ {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"stab\" = physical violence (1); \"u\" = directed (1); \"white\" = explicit race mention (1)\n\nExample: \"MASS BLACK WORLDWIDE GENOCIDE IS OUR ONLY HOPE\"\n→ {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"genocide\" = violence (1); targets group (0); \"black\" = explicit race mention (1)\n\nExample: \"With a face like that I bet you could scare your HIV into remission.\"\n→ {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}\nReason: No violence words (0); \"you/your\" = directed (1); \"HIV\" = explicit disability mention (1)\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: \"[INSERT TEXT HERE]\"",
                "Q": 0.3041664313351385,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and rules:\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit calls for, glorification of, or celebration of physical harm, injury, or death (e.g., kill, murder, die, dead, genocide, harm, beat, exterminate). Systemic issues (e.g., incarceration) alone do not qualify.\n- **directed_vs_generalized**: 1 = targeted at specific individual(s) using \"you\" or names; 0 = targeted at broad group\n- **gender (1)**: Attacks based on explicitly mentioned gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on explicitly mentioned race or perceived race\n- **national_origin (1)**: Attacks based on explicitly mentioned origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on explicitly mentioned physical/mental disabilities\n- **religion (1)**: Attacks based on explicitly mentioned religious beliefs\n- **sexual_orientation (1)**: Attacks based on explicitly mentioned sexual orientation\n\n**CRITICAL RULES:**\n1. Be conservative: Only assign 1 if text unambiguously matches definition\n2. Explicit mention required: Only label categories with explicitly mentioned identifiers\n3. \"Behind bars\" refers to incarceration, not physical violence\n4. Criticism of religion ≠ automatic violence classification\n5. Evaluate each category independently using explicit evidence\n6. Use \"you\" or names → directed_vs_generalized: 1; group references → directed_vs_generalized: 0\n\n**DECISION PROCESS:**\n1. Scan for violence keywords (kill, murder, die, dead, genocide, harm, beat, exterminate) - if found: violence=1\n2. Check for \"you\" or names - if present: directed=1; if attacking group: directed=0\n3. Check for explicit protected category mentions - assign 1 only if identifier is explicitly used\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: \"[INSERT TEXT HERE]\"",
                "Q": 0.4041664313351385,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.1,
                "children": []
              }
            ]
          },
          {
            "id": 5,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and guidelines.\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit calls for, glorification of, or celebration of physical harm, injury, or death. Systemic issues (e.g., incarceration) alone do not qualify.\n- **directed_vs_generalized**: 1 = targeted at specific individual(s); 0 = targeted at broad group\n- **gender (1)**: Attacks based on gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on race or perceived race\n- **national_origin (1)**: Attacks based on origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on physical/mental disabilities\n- **religion (1)**: Attacks based on religious beliefs\n- **sexual_orientation (1)**: Attacks based on sexual orientation\n\n**CRITICAL GUIDELINES:**\n1. **Be conservative:** Only assign 1 if the text unambiguously matches the definition.\n2. **No inference:** Only label explicitly mentioned or unambiguously implied categories.\n3. **Literal interpretation:** \"Behind bars\" refers to incarceration, not physical violence. Distinguish metaphorical commands (e.g., \"kneel down\") from literal calls for harm (e.g., \"kill them\").\n4. **Disambiguate content:** Criticism of religion ≠ automatic violence classification.\n5. **Granular target analysis:** Identify if the attack is directed (specific individual/group) or generalized (broad category).\n\n**REASONING FRAMEWORK:**\nFollow these steps:\n1.  **Analyze Violence First:** Scan for explicit, literal words promoting physical harm (kill, massacre, harm, die, etc.). If none, violence=0.\n2.  **Identify the Target:** Is the attack aimed at a specific person/group (directed_vs_generalized=1) or a broad category (directed_vs_generalized=0)?\n3.  **Check for Protected Categories:** For each category, ask: \"Is this category **explicitly mentioned** as the basis for the attack?\" If yes, set to 1. If not, set to 0.\n\n**LEARN FROM EXAMPLES:**\nExample: \"kneel down, white boy. its the era of brown people now\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"kneel down\" is metaphorical, not violent. Targets a specific individual (\"white boy\") based on race.\n\nExample: \"There is clearly written in Quran... Kill them you will get 72 hours Islam Is Terrorism\"\nCorrect: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: \"Kill them\" is an explicit call for violence against a generalized group (\"kafir\") based on religion.\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: “[INSERT TEXT HERE]”",
            "Q": 0.37270187692224177,
            "N": 1,
            "uct_value": 0.3763069423362117,
            "reward": 0.05,
            "children": []
          },
          {
            "id": 6,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and examples.\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit calls for, glorification of, or celebration of physical harm, injury, or death. *Describing past atrocities in a way that glorifies them for an in-group qualifies.* Systemic issues (e.g., incarceration) alone do not qualify.\n- **target_specific_individual (1)**: The attack is targeted at specific, named or otherwise clearly identified individual(s). If the target is a broad, unnamed group, this is 0.\n- **gender (1)**: Attacks based on gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on race or perceived race\n- **national_origin (1)**: Attacks based on origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on physical/mental disabilities\n- **religion (1)**: Attacks based on religious beliefs, figures, symbols, or practices (e.g., holidays).\n- **sexual_orientation (1)**: Attacks based on sexual orientation\n\n**CRITICAL GUIDELINES:**\n1.  **Be conservative:** Only assign 1 if the text unambiguously matches the definition.\n2.  **No inference:** Only label explicitly mentioned or unambiguously implied categories.\n3.  **\"Behind bars\"** refers to incarceration, not physical violence.\n4.  **Criticism ≠ Attack:** Criticizing religious tenets or political ideas is not an automatic `religion:1` or `violence:1` classification. It must be an attack on the people who hold those beliefs.\n5.  **Generalized Groups:** Attacks on broad groups (e.g., \"all Muslims,\" \"white people,\" \"gays\") are `target_specific_individual: 0`.\n6.  **Specific Individuals:** Attacks on named individuals or clearly identified small groups (e.g., \"our neighbors the Khans\") are `target_specific_individual: 1`.\n7.  **Glorification of Violence:** Describing violent acts (e.g., \"lynching,\" \"massacre\") as a justified response or as a point of pride for a group qualifies as `violence: 1`.\n\n**LEARN FROM THESE EXAMPLES:**\n\nExample: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\nCorrect: {\"violence\": 0, \"target_specific_individual\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Targets a specific \"he\". Incarceration references are not physical violence.\n\nExample: \"Islam is a religion of hate. These propaganda type videos are dishonest.\"\nCorrect: {\"violence\": 0, \"target_specific_individual\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: Attack on a religion (`religion:1`), but no violence and target is the generalized group.\n\nExample: \"What about a global massacre of men? Pussiezzz only girls\"\nCorrect: {\"violence\": 1, \"target_specific_individual\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"Global massacre\" is a call for violence against a generalized gender group.\n\n--- **FAILURE ANALYSIS EXAMPLES** ---\n\nExample: \"The reason you see Indians bashing Muslims... cow lynching of Muslims, gang rapes... Let all Muslims be united and boycott Indian products...\"\nCorrect: {\"violence\": 1, \"target_specific_individual\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"religion\": 1, \"disability\": 0, \"sexual_orientation\": 0}\nReason: Glorifies past \"lynching\" and \"rapes\" as a reason for action (`violence:1`). Targets \"Muslims\" and \"Indians\" as generalized groups (`target_specific_individual:0`). \"Indians\" is `national_origin:1`.\n\nExample: \"I fucking hate gay and lesbians they are all animals\"\nCorrect: {\"violence\": 0, \"target_specific_individual\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\nReason: Attack on a generalized group (\"gay and lesbians\"). No explicit call for violence. `target_specific_individual:0`.\n\nExample: \"Whites smoke to much ciggssss and drugs shot those co. Down..\"\nCorrect: {\"violence\": 0, \"target_specific_individual\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"Whites\" is a generalized racial group (`race:1`, `target_specific_individual:0`). \"shot those co. down\" is a malformed phrase likely targeting companies, not people.\n\nExample: \"Easter is the most silly holiday ever. It must be stopped.\"\nCorrect: {\"violence\": 0, \"target_specific_individual\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: Attack on a core religious holiday qualifies as `religion:1`. \"Stopped\" is not a call for physical harm.\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"target_specific_individual\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: “[INSERT TEXT HERE]”",
            "Q": 1.348377074955573,
            "N": 4,
            "uct_value": 0.42630694233621175,
            "reward": 0.1,
            "children": [
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and examples.\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit calls for, glorification of, or celebration of physical harm, injury, or death. *Describing past atrocities in a way that glorifies them for an in-group qualifies.* Systemic issues (e.g., incarceration) alone do not qualify.\n- **directed_vs_generalized (1)**: The attack is **directed at the reader or a specific audience** (e.g., using \"you,\" \"your,\" or naming the recipient). If the attack is about a broad, unnamed third-party group (e.g., \"they,\" \"all X\"), this is 0.\n- **gender (1)**: Attacks based on gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on race or perceived race\n- **national_origin (1)**: Attacks based on origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on physical/mental disabilities\n- **religion (1)**: Attacks based on religious beliefs, figures, symbols, or practices (e.g., holidays).\n- **sexual_orientation (1)**: Attacks based on sexual orientation\n\n**CRITICAL GUIDELINES:**\n1.  **Be conservative:** Only assign 1 if the text unambiguously matches the definition.\n2.  **No inference:** Only label explicitly mentioned or unambiguously implied categories.\n3.  **\"Behind bars\"** refers to incarceration, not physical violence.\n4.  **Criticism ≠ Attack:** Criticizing religious tenets or political ideas is not an automatic `religion:1` or `violence:1` classification. It must be an attack on the people who hold those beliefs.\n5.  **Generalized Groups:** Attacks on broad third-party groups (e.g., \"all Muslims,\" \"white people,\" \"gays\") are `directed_vs_generalized: 0`.\n6.  **Directed Attacks:** Attacks directed at the reader or a specific audience (e.g., \"you people are vile,\" \"go back to your country\") are `directed_vs_generalized: 1`.\n7.  **Glorification of Violence:** Describing violent acts (e.g., \"lynching,\" \"massacre\") as a justified response or as a point of pride for a group qualifies as `violence: 1`.\n8.  **Race vs. National Origin:** Prefer `race` for attacks based on physical characteristics (e.g., \"pale,\" \"black skin\"). Prefer `national_origin` for attacks based on geography, nationality, or citizenship (e.g., \"Pacific Islanders,\" \"go back to Mexico\").\n9.  **\"You\" and \"Your\":** The use of second-person pronouns (\"you,\" \"your\") typically indicates a **directed attack** (`directed_vs_generalized: 1`), unless it's clearly part of a hypothetical or general statement.\n\n**LEARN FROM THESE EXAMPLES:**\n\nExample: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Targets a specific \"he\" (directed). Incarceration references are not physical violence.\n\nExample: \"Islam is a religion of hate. These propaganda type videos are dishonest.\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: Attack on a religion (`religion:1`), but no violence and target is a generalized third-party group.\n\nExample: \"What about a global massacre of men? Pussiezzz only girls\"\nCorrect: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"Global massacre\" is a call for violence against a generalized gender group.\n\n--- **FAILURE ANALYSIS EXAMPLES** ---\n\nExample: \"Pale cancer! Back to your west pacific ocean\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"Your\" makes this a directed attack. \"Pale\" is a racial characteristic, not national origin.\n\nExample: \"Try to talk to me about your senseless, inane god and you will meet him again as soon as possible thanks to my hand bitch\"\nCorrect: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: \"You\" indicates a directed threat of violence. Attack on religious concept (\"god\").\n\nExample: \"I would love to watch tearing apart every christian or muslim\"\nCorrect: {\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: Violence against \"every\" member of a religious group (generalized third-party).\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: “[INSERT TEXT HERE]”",
                "Q": 0.34189173267777706,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 8,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to these definitions and examples.\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit calls for, glorification of, or celebration of physical harm, injury, or death. *Describing past atrocities in a way that glorifies them for an in-group qualifies.* Systemic issues (e.g., incarceration) alone do not qualify.\n- **directed_vs_generalized (1)**: The attack is targeted at specific, named or otherwise clearly identified individual(s). If the target is a broad, unnamed group, this is 0.\n- **gender (1)**: Attacks based on gender identity (man, woman, non-binary)\n- **race (1)**: Attacks based on race or perceived race\n- **national_origin (1)**: Attacks based on origin, nationality, or citizenship status\n- **disability (1)**: Attacks based on physical/mental disabilities\n- **religion (1)**: Attacks based on religious beliefs, figures, symbols, or practices (e.g., holidays).\n- **sexual_orientation (1)**: Attacks based on sexual orientation (includes attacks on transgender individuals)\n\n**CRITICAL GUIDELINES:**\n1.  **Be conservative:** Only assign 1 if the text unambiguously matches the definition.\n2.  **No inference:** Only label explicitly mentioned or unambiguously implied categories. **Do not infer categories from pronouns (he/she) alone.**\n3.  **\"Behind bars\"** refers to incarceration, not physical violence.\n4.  **Criticism ≠ Attack:** Criticizing religious tenets, political ideas, or practices is not an automatic `religion:1` classification. It must be an attack on the *people* who hold those beliefs. (e.g., \"Islam is oppressive\" is 0; \"Muslims are oppressors\" is 1).\n5.  **Generalized Groups:** Attacks on broad groups (e.g., \"all Muslims,\" \"white people,\" \"transgenders\") are `directed_vs_generalized: 0`.\n6.  **Specific Individuals:** Attacks on named individuals or clearly identified small groups (e.g., \"our neighbors the Khans\") are `directed_vs_generalized: 1`.\n7.  **Glorification of Violence:** Describing violent acts (e.g., \"lynching,\" \"massacre\") as a justified response or as a point of pride for a group qualifies as `violence: 1`.\n8.  **Ethnoreligious Groups:** For groups like Jewish, Sikh, or Roma people, prioritize `religion:1` if the attack uses religious identifiers (e.g., \"jew\"). Only use `race:1` or `national_origin:1` if the attack explicitly uses racial or origin-based slurs.\n9.  **\"Based On\" Logic:** An attack is \"based on\" a category if that category is the central reason for the insult. An attack that mentions a characteristic (e.g., using pronouns) is not automatically classified under that category unless the attack is clearly motivated by it.\n\n**LEARN FROM THESE EXAMPLES:**\n\nExample: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Targets a specific \"he\". Incarceration is not violence. Attack is based on race.\n\nExample: \"Islam is a religion of hate. These propaganda type videos are dishonest.\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: Attack on a religion (`religion:1`), target is generalized.\n\n--- **FAILURE ANALYSIS & CORRECTIONS** ---\n\nExample: \"This fcking jew wants money? I’ll give him only my dick if he wants\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: Targets a specific individual. \"Jew\" is an ethnoreligious identifier (`religion:1`). Gender is mentioned but is not the basis of the attack (`gender:0`).\n\nExample: \"EEEWWWWWWW TRANSGENDERS ARE FUCKING DISGUSTING\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\nReason: Attack on a broad group (`directed_vs_generalized:0`). Transgender identity is classified under `sexual_orientation:1`.\n\nExample: \"Fuck that shit this bitch ugly and she has a penis,oh hell nawww!!!\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\nReason: Targets a specific individual (\"this bitch\"). The attack is based on the mention of a penis (a sexual characteristic), not gender, so `sexual_orientation:1`, `gender:0`.\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: “[INSERT TEXT HERE]”",
                "Q": 0.3918917326777771,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 9,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the provided text strictly according to these definitions and examples.\n\n**CATEGORY DEFINITIONS:**\n- **violence (1)**: Explicit, unambiguous calls for, glorification of, or celebration of physical harm, injury, or death towards people. Descriptions of past atrocities that glorify them for an in-group qualify. References to incarceration, deportation, or other non-physical outcomes do NOT qualify.\n- **directed_vs_generalized (1)**: The attack is directed at a specific, named individual OR a specific, non-generalized group (e.g., \"illegal aliens,\" \"these whiny black crybabies\"). Attacks on broad, generalized groups (e.g., \"all Muslims,\" \"white people,\" \"gays\") are 0.\n- **gender (1)**: Attacks specifically targeting individuals or groups for being men, women, non-binary, or transgender. Criticism of gender roles or cultural masculinity is not an automatic 1.\n- **race (1)**: Attacks based on race or perceived race.\n- **national_origin (1)**: Attacks based on origin, nationality, or citizenship status.\n- **disability (1)**: Attacks based on physical or mental disabilities.\n- **religion (1)**: Attacks on people based on their religious beliefs, figures, symbols, or practices. Criticism of religious tenets or ideas is not an attack on people.\n- **sexual_orientation (1)**: Attacks based on sexual orientation (e.g., gay, lesbian, bisexual).\n\n**CRITICAL GUIDELINES:**\n1.  **Ultra-Conservatism:** Only assign 1 if the text is an unambiguous, explicit match for the definition. When in doubt, assign 0.\n2.  **No Inference:** Base classifications only on explicitly mentioned or unambiguously implied language.\n3.  **Primary Target Rule:** If text contains multiple targets, base `directed_vs_generalized` on the primary target of the hate speech.\n4.  **Independent Evaluation:** Evaluate each category separately. A text can have multiple 1s.\n5.  **Slur Interpretation:** Derogatory terms targeting protected groups (e.g., \"shemale\") should be mapped to their appropriate category based on the group being attacked.\n\n**LEARN FROM THESE EXAMPLES:**\n\nExample: \"If only he was black....He would be literally be behind bars faster than you can even imagine\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Targets specific \"he\" (directed:1) based on race (race:1). Incarceration ≠ violence.\n\nExample: \"Islam is a religion of hate. These propaganda type videos are dishonest.\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}\nReason: Attack on religion (religion:1) targeting generalized group (directed:0).\n\n--- **FAILURE PREVENTION EXAMPLES** ---\n\nExample: \"EXECUTE OHOMO AND HIS SHEMALE BITCH MIKE.\"\nCorrect: {\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\nReason: \"EXECUTE\" = violence (1). Targets named individuals (directed:1). \"Shemale\" attacks sexual orientation (1), not gender.\n\nExample: \"Hey Sweden do you remember when your men were vikings and acted like they had a dick...\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: Targets \"Sweden\" (national_origin:1). Gender comment is cultural criticism, not gender attack.\n\nExample: \"Any lesbian that doesn't look like a complete hog actually wants to be with a man.\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}\nReason: Attacks lesbians as generalized group (sexual_orientation:1). Gender stereotype doesn't qualify as gender attack.\n\nExample: \"NO TO ILLEGAL ALIENS THEY DO NOT BELONG HERE. Deport today\"\nCorrect: {\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}\nReason: \"Illegal aliens\" = specific but generalized group (directed:0). \"Deport\" ≠ violence. Citizenship attack (national_origin:1).\n\n**OUTPUT REQUIREMENT:**\nOutput ONLY valid JSON: {\"violence\": 0|1, \"directed_vs_generalized\": 0|1, \"gender\": 0|1, \"race\": 0|1, \"national_origin\": 0|1, \"disability\": 0|1, \"religion\": 0|1, \"sexual_orientation\": 0|1}\n\nText to classify: \"[INSERT TEXT HERE]\"",
                "Q": 0.34189173267777706,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              }
            ]
          }
        ]
      }
    ]
  }
}