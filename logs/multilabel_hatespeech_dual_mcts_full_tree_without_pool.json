{
  "config": {
    "mcts_iters": 10,
    "depth_threshold": 5,
    "width_threshold": 3
  },
  "best_node": {
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "depth": 0,
    "Q": 1.941666666666667,
    "N": 28
  },
  "search_tree": {
    "id": 0,
    "depth": 0,
    "action_sequence": [],
    "prompt": "Classify text into multiple hate speech labels.",
    "Q": 1.941666666666667,
    "N": 28,
    "uct_value": 0.0,
    "reward": 0.1,
    "children": [
      {
        "id": 1,
        "depth": 1,
        "action_sequence": [
          "SuccessDrivenAction"
        ],
        "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps:\n\n1.  **Decompose the text** into its key parts.\n2.  **Evaluate each label independently** using strict definitions:\n    *   `violence`: Promotes, threatens, or glorifies physical harm.\n    *   `directed_vs_generalized`: `1` for a specific individual, `0` for a generalized group.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` only for clear attacks on that characteristic.\n3.  **Default to `0`** for any label without unambiguous evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\nExample output: `{\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}`\n\nText to classify:",
        "Q": 1.1,
        "N": 14,
        "uct_value": 0.08308394652520763,
        "reward": 0.05,
        "children": [
          {
            "id": 2,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps:\n\n1.  **Decompose the text** into its key parts and their targets.\n2.  **Evaluate each label independently** using strict definitions:\n    *   `violence`: Promotes, threatens, or glorifies physical harm. Requires explicit content.\n    *   `directed_vs_generalized`: `1` for a specific individual, `0` for a generalized group.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` only for clear, unambiguous attacks on that specific characteristic.\n3.  **Apply critical reasoning**:\n    *   Metaphorical language can constitute an attack if it clearly targets a protected characteristic.\n    *   Distinguish between race (physical traits) and national origin (country/citizenship).\n    *   Distinguish between religion (belief systems) and ideology (social/political movements).\n    *   Default to `0` for any label without explicit, unambiguous evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\n**Example Output:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}`\n\nText to classify:",
            "Q": 0.4083333333333333,
            "N": 4,
            "uct_value": 0.10614461913144285,
            "reward": 0.05,
            "children": [
              {
                "id": 3,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by following these steps rigorously:\n\n1.  **Determine Overall Intent**: Assess if the text is derogatory, mocking, hostile, or belittling. Recognize that short, stereotypical, or dehumanizing phrases can constitute attacks even without explicit violent language.\n\n2.  **Decompose and Identify Targets**: Break down the text into key phrases and identify the primary target(s) of any hostile language (e.g., a national group, gender, or sexual orientation).\n\n3.  **Evaluate Each Label Independently**:\n    Use the following definitions. An \"attack\" includes negative stereotyping, slurring, dehumanizing, mocking, or excluding based on a protected characteristic:\n    - `violence`: `1` only for explicit promotion, threat, or glorification of physical harm.\n    - `directed_vs_generalized`: `1` if targeting a specific, named individual; `0` for a generalized group.\n    - `gender`: `1` for attacks based on gender identity.\n    - `race`: `1` for attacks based on perceived physical racial traits.\n    - `national_origin`: `1` for attacks based on country, citizenship, ethnicity, or national/ethnic stereotypes.\n    - `disability`: `1` for attacks based on physical, mental, or developmental disabilities.\n    - `religion`: `1` for attacks based on religious beliefs (distinguish from non-religious ideologies).\n    - `sexual_orientation`: `1` for attacks based on romantic or sexual attraction.\n\n4.  **Apply Critical Reasoning**:\n    - A single derogatory term or label targeting a protected group is sufficient for `1`.\n    - Metaphorical language can be an attack.\n    - If the text is ambiguous but has a likely derogatory meaning based on common usage, classify it as an attack (`1`).\n    - Only default to `0` if there is absolutely no evidence of an attack.\n\n5.  **Output a valid JSON object** with the exact keys and integer values (1 or 0). Do not include any other text.\n\n**Examples of Correct Classification**:\n- Input: \"Hey Sweden do you remember when your men were vikings...\"  \n  Output: `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}`  \n  *Reasoning: Attacks Swedes using a national stereotype, not gender.*\n\n- Input: \"Fat lesbians\"  \n  Output: `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}`  \n  *Reasoning: Derogatory labeling based on sexual orientation.*\n\nText to classify:",
                "Q": 0.25,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.15,
                "children": []
              },
              {
                "id": 4,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps:\n\n1.  **Decompose the text** into its key phrases and identify the primary target(s) of each phrase.\n\n2.  **Evaluate each label independently** using strict, literal definitions. Default to `0` for any label without explicit, unambiguous evidence.\n    *   `violence`: **Literal and explicit** promotion, threat, or glorification of **physical harm**. Figurative language (e.g., \"go to hell\") or ideological warnings do not qualify.\n    *   `directed_vs_generalized`: `1` for a specific, named individual, `0` for a generalized group or unnamed individuals.\n    *   `gender`: `1` only for clear attacks based on gender identity or expression (e.g., \"women are inferior,\" anti-trans slurs). Not triggered by attacks on umbrella groups unless gender is the specific focus.\n    *   `race`: `1` for attacks based on physical traits, ancestry, or racial categories (e.g., slurs, \"go back to\" tropes primarily targeting race).\n    *   `national_origin`: `1` for attacks based on country of origin, citizenship, or immigration status (e.g., \"illegal aliens,\" \"go back to [specific country]\").\n    *   `disability`: `1` for attacks based on physical or mental disability. Do not conflate pejorative use of medical terms (e.g., \"mentally ill\") attacking another group with an attack on disabled people.\n    *   `religion`: `1` for attacks on religious beliefs, practices, or adherents. Distinguish from ideology (e.g., \"fascists\").\n    *   `sexual_orientation`: `1` for attacks based on sexual orientation (e.g., homophobic slurs, \"gays are predators\").\n\n3.  **Apply Critical Reasoning:**\n    *   **Primary Target:** If an attack uses a trope associated with multiple characteristics (e.g., \"go back to Africa\"), assign `1` only to the **most specific and primary characteristic** (typically `race` in this case).\n    *   **Umbrella Terms:** For groups like \"LGBTQ+,\" assign `1` only to the characteristic(s) **explicitly mentioned or unambiguously targeted**. An attack on the group is not automatically an attack on every characteristic within it.\n    *   **Literal Interpretation:** For `violence`, prioritize a literal interpretation of words. Assume non-literal, metaphorical meaning unless the text is explicitly and directly threatening physical harm.\n\n4.  **Learn from these examples to avoid common errors:**\n    *   **Example A:** Text: \"Send all these whiny black crybabies back to central Apefrica...\"\n      *   **Incorrect:** `\"national_origin\": 1` (The \"go back to\" trope primarily targets race, not national origin).\n      *   **Correct:** `\"race\": 1, \"national_origin\": 0`\n    *   **Example B:** Text: \"LGBTQ+ are mentally ill. Period.\"\n      *   **Incorrect:** `\"gender\": 1` (The attack is on the group's sanity, not specifically gender identity. The term \"LGBTQ+\" explicitly includes sexual orientation).\n      *   **Correct:** `\"sexual_orientation\": 1, \"gender\": 0`\n    *   **Example C:** Text: \"Beware LBGQ community Shari Law is coming you guys will have to go back in closet to save your heads.\"\n      *   **Incorrect:** `\"violence\": 1` (\"save your heads\" is a metaphorical/ideological warning, not an explicit threat of physical harm).\n      *   **Correct:** `\"violence\": 0, \"sexual_orientation\": 1`\n\n5.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\n**Example Output:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}`\n\nText to classify:",
                "Q": 0.17500000000000002,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps:\n\n1.  **Decompose the text** into its key phrases and identify their targets.\n2.  **Evaluate each label independently** using strict definitions:\n    *   `violence`: Promotes, threatens, or glorifies physical harm. Requires explicit language or clear intent.\n    *   `directed_vs_generalized`: `1` for a specific, named individual, `0` for a generalized group or unnamed individual.\n    *   `gender`: `1` for clear attacks on people *because they are men or women* (e.g., misogyny, misandry).\n    *   `gender_identity`: `1` for clear attacks on a person's transgender or non-binary identity (e.g., denying their gender, deadnaming, mockery of transition).\n    *   `sexual_orientation`: `1` for clear attacks on a person's romantic or sexual attraction (e.g., homophobia, biphobia).\n    *   `race`: `1` for attacks based on perceived physical traits or ancestry.\n    *   `national_origin`: `1` for attacks based on country of origin, citizenship, or ethnicity.\n    *   `disability`: `1` for attacks based on physical or mental disability.\n    *   `religion`: `1` for attacks based on religious beliefs or affiliation.\n    *   `ideology`: `1` for attacks based on political or social beliefs (e.g., communism, fascism).\n3.  **Apply critical reasoning**:\n    *   Metaphorical language can constitute an attack if it clearly targets a protected characteristic.\n    *   **Crucially distinguish:** `gender` (men/women) vs. `gender_identity` (trans/cis) vs. `sexual_orientation` (gay/straight/bi). Denying a person's stated gender is an attack on `gender_identity`.\n    *   Distinguish between race (physical traits) and national origin (country/citizenship).\n    *   Distinguish between religion (belief systems) and ideology (social/political movements).\n    *   Default to `0` for any label without explicit, unambiguous evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\n**Examples:**\n**Text:** \"Women are too emotional to be good leaders.\"\n**Output:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 1, \"gender_identity\": 0, \"sexual_orientation\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"ideology\": 0}`\n\n**Text:** \"An ugly man in a wig and dress, is still just an ugly man in a wig and dress... NOT A WOMAN!!!\"\n**Output:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"gender_identity\": 1, \"sexual_orientation\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"ideology\": 0}`\n*Explanation: This attacks gender identity by denying and mocking transgender identity, not gender (men/women) or sexual orientation.*\n\n**Text:** \"All immigrants from CountryX are criminals and should be deported.\"\n**Output:** `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"gender_identity\": 0, \"sexual_orientation\": 0, \"race\": 0, \"national_origin\": 1, \"disability\": 0, \"religion\": 0, \"ideology\": 0}`\n\nText to classify:",
                "Q": 0.25,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.15,
                "children": []
              }
            ]
          },
          {
            "id": 3,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps rigidly:\n\n1.  **Decompose the text** into its key phrases.\n2.  **Evaluate each label independently** using strict, literal definitions. You must be able to point to a specific phrase as unambiguous evidence for any label scored `1`.\n    *   `violence`: `1` **only** for explicit promotion, threat, or glorification of physical harm. General hatred or offensive language is **not sufficient**.\n    *   `directed_vs_generalized`: `1` for a specific, named individual. `0` for a generalized group (even a specific one) or no target.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` **only** for clear, direct attacks on that specific characteristic. The use of a slur is clear evidence for its associated characteristic.\n3.  **Default to `0`** for any label without direct, phrase-level evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\nExample output: `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}`\n\nText to classify:",
            "Q": 0.5,
            "N": 5,
            "uct_value": 0.11656128579810951,
            "reward": 0.1,
            "children": [
              {
                "id": 4,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps rigidly:\n\n1.  **Decompose the text** into its key phrases.\n2.  **Evaluate each label independently** using strict, literal definitions. You must be able to point to a specific phrase as unambiguous evidence for any label scored `1`.\n    *   `violence`: `1` **only** for explicit promotion, threat, or glorification of physical harm. General hatred, offensive language, or **metaphorical language (e.g., \"like a virus\") is not sufficient**.\n    *   `directed_vs_generalized`: `1` **only** for a specific, named individual or a very small, uniquely identifiable group. `0` for a generalized group (even a specific one like \"all politicians\") or no target.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` **only** for clear, direct attacks on that specific characteristic. **Identify the core characteristic that is the primary basis for the attack.**\n        *   The use of a slur is clear evidence for its associated characteristic.\n        *   **Dehumanizing language (e.g., comparing people to diseases, animals, or objects) is also clear evidence.**\n3.  **Default to `0`** for any label without direct, phrase-level evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\nExample output: `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}`\n\nText to classify:",
                "Q": 0.1,
                "N": 1,
                "uct_value": 0.10317158901715469,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text for hate speech. Your analysis must be rigid, literal, and evidence-based. Follow these steps:\n\n1.  **Decompose the text** into its key phrases.\n2.  **Evaluate each label independently** using these strict definitions. **For any label scored `1`, you MUST be able to point to a specific phrase as unambiguous evidence.**\n    *   `violence`: `1` **only** for explicit promotion, threat, or glorification of physical harm.\n    *   `directed_vs_generalized`: `1` **only** for a specific, named individual. `0` for a generalized group or no target.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` **only** for clear, direct attacks on that specific characteristic. The use of a slur is clear evidence for its associated characteristic.\n3.  **Default to `0`** for any label without direct, phrase-level evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\nExample output: `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}`\n\nText to classify:",
                "Q": 0.15000000000000002,
                "N": 1,
                "uct_value": 0.15317158901715472,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 6,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps rigidly:\n\n1.  **Decompose the text** into its key phrases.\n2.  **Evaluate each label independently** using strict definitions. You must be able to point to a specific phrase as unambiguous evidence for any label scored `1`.\n    *   `violence`: `1` **only** for explicit promotion, threat, or glorification of physical harm. General hatred or offensive language is **not sufficient**.\n    *   `directed_vs_generalized`: `1` for a specific, named, **or directly addressed individual** (e.g., using \"you\" or a pronoun that implies a specific person). `0` for a generalized group (even a specific one) or no target.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` **only** for clear, direct attacks on that specific characteristic. **This includes the use of slurs, malicious mockery of the characteristic, and fabricated terms intended to evoke and attack that characteristic.** The use of a slur is clear evidence for its associated characteristic.\n        *   **Note:** The slur \"tranny\" is classified under `sexual_orientation`, not `gender`.\n        *   **Note:** The term \"bitch\" is only `gender=1` if clearly attacking someone based on gender; otherwise it's a generic insult and scores `0`.\n3.  **Default to `0`** for any label without direct, phrase-level evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\n**Examples of correct classification:**\n\nExample 1: \"Incest baby that got double-down syndrome\"\n→ `{\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}`\n*Rationale: Directed at specific \"you\"; \"double-down syndrome\" mocks disability.*\n\nExample 2: \"This is exactly how I feel like when I see a tranny like this disgusting af\"\n→ `{\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}`\n*Rationale: \"a tranny like this\" targets specific person; \"tranny\" is sexual_orientation slur.*\n\nExample 3: \"Try to talk to me about your senseless, inane god and you will meet him again as soon as possible thanks to my hand bitch\"\n→ `{\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 1, \"sexual_orientation\": 0}`\n*Rationale: Violence threat; directed at \"you\"; attacks religion; \"bitch\" is generic insult here.*\n\nText to classify:",
                "Q": 0.22500000000000003,
                "N": 2,
                "uct_value": 0.1781715890171547,
                "reward": 0.1,
                "children": [
                  {
                    "id": 7,
                    "depth": 4,
                    "action_sequence": [
                      "SuccessDrivenAction",
                      "SuccessDrivenAction",
                      "FailureDrivenAction",
                      "SuccessDrivenAction"
                    ],
                    "prompt": "Classify the text for hate speech by analyzing its constituent phrases against each label. Follow these steps rigidly:\n\n1.  **Decompose the text** into its key phrases. Your final classification must be based solely on evidence from these discrete phrases.\n2.  **Evaluate each label independently** using the strict definitions below. For any label scored `1`, you must be able to cite the exact phrase that serves as unambiguous evidence.\n    *   `violence`: `1` **only** for explicit promotion, threat, or glorification of physical harm. General offensive language is **not sufficient**.\n    *   `directed_vs_generalized`: `1` for a specific, named, **or directly addressed individual** (e.g., using \"you\"). `0` for a generalized group (even a specific one).\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` **only** for clear, direct attacks on that specific characteristic. **This includes the use of slurs and malicious mockery of the characteristic.**\n        *   **Note:** \"tranny\" → `sexual_orientation`; \"bitch\" → `gender=1` only if clearly attacking someone based on gender.\n3.  **Default to `0`** for any label without direct, phrase-level evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\nText to classify:",
                    "Q": 0.15000000000000002,
                    "N": 1,
                    "uct_value": 0.0,
                    "reward": 0.1,
                    "children": []
                  },
                  {
                    "id": 8,
                    "depth": 4,
                    "action_sequence": [
                      "SuccessDrivenAction",
                      "SuccessDrivenAction",
                      "FailureDrivenAction",
                      "FailureDrivenAction"
                    ],
                    "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps rigidly:\n\n1.  **Decompose the text** into its key phrases.\n2.  **Evaluate each label independently** using strict definitions. You must be able to point to a specific phrase as unambiguous evidence for any label scored `1`.\n    *   `violence`: `1` **only** for explicit promotion, threat, or glorification of physical harm. General hatred or offensive language is **not sufficient**.\n    *   `directed_vs_generalized`: `1` for a specific, named, **or directly addressed individual** (e.g., using \"you\" in a context that implies a specific person). `0` for a generalized group (even a specific one) or no target. **Generic terms like \"kid\" or \"dude\" without other identifying context are NOT sufficient for `1`.**\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` **only** for clear, direct attacks on that specific characteristic. **This includes:**\n        *   **The use of slurs.**\n        *   **Malicious mockery of the characteristic.**\n        *   **Fabricated terms intended to evoke and attack that characteristic.**\n        *   **Pejorative statements that demean the characteristic or link it to inherently negative outcomes (e.g., suffering, worthlessness).**\n        *   **The promotion of negative, dehumanizing, or harmful stereotypes about the group.**\n        *   **Note:** The slur \"tranny\" is classified under `sexual_orientation`, not `gender`.\n        *   **Note:** The term \"bitch\" is only `gender=1` if clearly attacking someone based on gender (e.g., \"because you're a woman\"); otherwise it's a generic insult and scores `0`.\n        *   **CRITICAL: An insult must be DIRECTLY about the protected characteristic. Generic insults about appearance (e.g., \"ugly\"), intelligence (e.g., \"idiot\"), or behavior (e.g., \"lazy\") do NOT score for a characteristic unless they are inherently tied to a stereotype of that group (e.g., a racist stereotype).**\n3.  **Default to `0`** for any label without direct, phrase-level evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\n**Examples of correct classification:**\n\nExample 1: \"Incest baby that got double-down syndrome\"\n→ `{\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}`\n*Rationale: Directed at specific \"you\"; \"double-down syndrome\" mocks disability.*\n\nExample 2: \"Being blind would make me want suicide. Go kill yourself kid\"\n→ `{\"violence\": 1, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}`\n*Rationale: \"Go kill yourself\" is a violent threat; \"kid\" implies a specific target; \"Being blind would make me want suicide\" is a pejorative statement that demeans disability by linking it to suffering.*\n\nExample 3: \"Do lesbians know what a bra is? It's always the fat ugly ones who pick a fight.\"\n→ `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}`\n*Rationale: Targets \"lesbians\" (generalized group); promotes a negative stereotype (`sexual_orientation=1`); \"fat\" and \"ugly\" are generic appearance insults, not direct attacks on gender or sexual orientation.*\n\nText to classify:",
                    "Q": 0.05,
                    "N": 0,
                    "uct_value": 0.0,
                    "reward": 0.05,
                    "children": []
                  },
                  {
                    "id": 9,
                    "depth": 4,
                    "action_sequence": [
                      "SuccessDrivenAction",
                      "SuccessDrivenAction",
                      "FailureDrivenAction",
                      "FailureDrivenAction"
                    ],
                    "prompt": "Classify the text for hate speech by analyzing its phrases and overall meaning against each label. Follow these steps rigidly:\n\n1.  **Decompose the text** into its key phrases and consider the overall statement.\n2.  **Evaluate each label independently** using strict definitions. You must be able to point to specific language as unambiguous evidence for any label scored `1`.\n    *   `violence`: `1` for explicit promotion, threat, or glorification of physical harm. **This includes language that dehumanizes a group (e.g., comparing them to diseases, parasites, or vermin) as it implicitly promotes violence.** General hatred or offensive language without this component is **not sufficient**.\n    *   `directed_vs_generalized`: `1` for a specific, named, **or directly addressed individual** (e.g., using \"you\" or a pronoun that implies a specific person). `0` for a generalized group (even a specific one) or no target.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: `1` for clear, direct attacks on that specific characteristic. **This includes:**\n        *   **The use of slurs.**\n        *   **Malicious mockery of the characteristic.**\n        *   **Fabricated terms intended to evoke and attack that characteristic.**\n        *   **Statements of discrimination, exclusion, or inferiority based on that characteristic (e.g., \"I wouldn't hire them because they are X\").**\n        *   **Note:** The slur \"tranny\" is classified under `sexual_orientation`.\n        *   **Note:** The term \"bitch\" is only `gender=1` if clearly attacking someone based on gender; otherwise it's a generic insult and scores `0`.\n        *   **Note:** `disability` includes physical, cognitive, and mental disabilities, **as well as chronic health conditions like HIV/AIDS.**\n3.  **Default to `0`** for any label without direct, unambiguous evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0). **Do not output any other text.**\n\n**Examples of correct classification:**\n\nExample 1: \"Incest baby that got double-down syndrome\"\n→ `{\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}`\n*Rationale: Directed at specific \"you\"; \"double-down syndrome\" mocks disability.*\n\nExample 2: \"This is exactly how I feel like when I see a tranny like this disgusting af\"\n→ `{\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 1}`\n*Rationale: \"a tranny like this\" targets specific person; \"tranny\" is sexual_orientation slur.*\n\nExample 3: \"Only black lives matter. fuck white people the cancer of the earth\"\n→ `{\"violence\": 1, \"directed_vs_generalized\": 0, \"gender\": 0, \"race\": 1, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}`\n*Rationale: \"the cancer of the earth\" dehumanizes a racial group, implying a need for eradication (violence). \"white people\" is a racial target.*\n\nExample 4: \"With a face like that I bet you could scare your HIV into remission.\"\n→ `{\"violence\": 0, \"directed_vs_generalized\": 1, \"gender\": 0, \"race\": 0, \"national_origin\": 0, \"disability\": 1, \"religion\": 0, \"sexual_orientation\": 0}`\n*Rationale: Directed at \"you\"; mocks HIV, a chronic health condition (disability).*\n\nExample 5: \"I don’t like this gender at all. I definitely wouldn’t hire any male bitch at ma shop\"\n→ `{\"violence\": 0, \"directed_vs_generalized\": 0, \"gender\": 1, \"race\": 0, \"national_origin\": 0, \"disability\": 0, \"religion\": 0, \"sexual_orientation\": 0}`\n*Rationale: Statement expresses discriminatory hiring intent based on gender (\"wouldn’t hire any male\").*\n\nText to classify:",
                    "Q": 0.0,
                    "N": 0,
                    "uct_value": 0.0,
                    "reward": 0.0,
                    "children": []
                  }
                ]
              }
            ]
          },
          {
            "id": 4,
            "depth": 2,
            "action_sequence": [
              "SuccessDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps precisely:\n\n1.  **Decompose the Text:** First, break the text into its key constituent phrases or ideas.\n2.  **Evaluate Each Label Independently:** Apply the strict definitions below. Your evaluation for one label must not influence another.\n    *   `violence`: **1** only for promoting, threatening, or glorifying *physical harm*.\n    *   `directed_vs_generalized`: **1** only if targeting a *specific, identifiable individual*; **0** for a *generalized group*.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: **1** only for a clear attack, including harmful stereotypes, pejorative generalizations, or bias-based predictions.\n3.  **Default to `0`:** Assign `0` for any label without explicit and unambiguous evidence.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\nText to classify:",
            "Q": 0.25833333333333336,
            "N": 4,
            "uct_value": 0.06864461913144285,
            "reward": 0.05,
            "children": [
              {
                "id": 5,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps precisely:\n\n1.  **Decompose the Text:** Break the text into its key constituent phrases or ideas.\n2.  **Evaluate Each Label Independently:** Apply the strict definitions below. Your evaluation for one label must not influence another. Treat each label as a separate, atomic question.\n    *   `violence`: **1** only for explicit promotion, threat, or glorification of *physical harm*. Do not infer violence from offensive language alone.\n    *   `directed_vs_generalized`: **1** only if targeting a *specific, identifiable individual* (e.g., by name); **0** for any *generalized group*.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: **1** only for a clear attack, including harmful stereotypes, pejorative generalizations, bias-based predictions, or slurs. Neutral mentions or personal anecdotes do not qualify.\n3.  **Default to `0`:** Assign `0` for any label without explicit and unambiguous evidence. If in doubt, the answer is `0`.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\nText to classify:",
                "Q": 0.15,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.15,
                "children": []
              },
              {
                "id": 6,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps precisely:\n\n1.  **Decompose the Text:** Break the text into its key constituent phrases or ideas.\n2.  **Evaluate Each Label Independently:** Apply these strict definitions. Your evaluation for one label must not influence another.\n    *   `violence`: **1** only for promoting, threatening, or glorifying *physical harm*. Expressions of hatred alone are not sufficient.\n    *   `directed_vs_generalized`: **1** only if targeting a *specific, identifiable individual* (e.g., using a name or \"you\"); **0** for a *generalized group* (e.g., \"they,\" \"all [group]\").\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: **1** only for a clear attack, including slurs, harmful stereotypes, pejorative generalizations, or dehumanizing language (e.g., \"animals\").\n3.  **Default to `0`:** Assign `0` for any label without explicit and unambiguous evidence. If in doubt, assign `0`.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0). Do not include any other text.\n\nText to classify:",
                "Q": 0.125,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "SuccessDrivenAction",
                  "SuccessDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "Classify the text for hate speech by analyzing its phrases against each label. Follow these steps precisely:\n\n1.  **Decompose the Text:** Break the text into its key constituent phrases or ideas. Analyze the literal phrases, not the overall sentiment.\n2.  **Evaluate Each Label Independently:** Apply the strict definitions below. Your evaluation for one label must not influence another. Treat each definition as an absolute rule.\n    *   `violence`: **1** only and exclusively for promoting, threatening, or glorifying *physical harm*. Ignore emotional or psychological harm.\n    *   `directed_vs_generalized`: **1** only if targeting a *specific, identifiable individual* (e.g., \"you,\" a name); **0** for a *generalized group*.\n    *   `gender`, `race`, `national_origin`, `disability`, `religion`, `sexual_orientation`: **1** only for a clear attack, including harmful stereotypes, pejorative language, or bias-based predictions. Mere mention of a trait is not an attack.\n3.  **Default to `0`:** Assign `0` for any label without explicit and unambiguous evidence that perfectly matches a definition. Ambiguity always results in `0`.\n4.  **Output a valid JSON object** with the exact keys and integer values (1 or 0).\n\nText to classify:",
                "Q": 0.15000000000000002,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.1,
                "children": []
              }
            ]
          }
        ]
      },
      {
        "id": 2,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to the following definitions and rules:\n\n**Label Definitions:**\n- **violence**: 1 ONLY for explicit calls for, threats of, or encouragement of physical harm, injury, or death. Legal consequences, dehumanization, or non-physical harm = 0.\n- **directed_vs_generalized**: 1 ONLY if targeting a specific, named individual or very small, identifiable group. Attacks on broad categories = 0.\n- **gender**: 1 for attacks based on gender, gender identity, or expression.\n- **race**: 1 for attacks based on racial or ethnic characteristics.\n- **national_origin**: 1 for attacks specifically about country of origin, citizenship, or immigration status.\n- **disability**: 1 for attacks based on physical, mental, or developmental disabilities.\n- **religion**: 1 for attacks based on religious beliefs or affiliation.\n- **sexual_orientation**: 1 for attacks based on sexual orientation.\n\n**Critical Rules:**\n- Be conservative: Only assign 1 with clear, unambiguous evidence\n- Default to 0 when uncertain\n- Analyze text literally without unsupported inferences\n- Avoid over-interpretation of metaphorical language\n\n**Examples of Common Mistakes to Avoid:**\n- \"should only count 0.5\" → dehumanizing but not violence (violence=0)\n- \"behind bars\" → legal consequence, not physical violence (violence=0)\n- \"retard\" → disability slur but not violent (violence=0)\n- Attacks on groups like \"Indians\" or \"women\" → generalized, not directed (directed_vs_generalized=0)\n\n**Output Format:** After analysis, output ONLY a valid JSON object with the specified keys. No additional text.\n\nText to classify: \"{user_input}\"",
        "Q": 0.21666666666666665,
        "N": 4,
        "uct_value": 0.058679207182888826,
        "reward": 0.05,
        "children": [
          {
            "id": 3,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to the following definitions and rules.\n\n**Label Definitions:**\n- **violence**: 1 ONLY for explicit calls for, threats of, or encouragement of physical harm, injury, or death. Legal consequences, dehumanization, or non-physical harm = 0.\n- **directed_vs_generalized**: 1 ONLY if targeting a specific, named individual or very small, identifiable group. Attacks on broad categories (e.g., \"men,\" \"Indians\") = 0.\n- **gender**: 1 for attacks based on gender, gender identity, or expression.\n- **race**: 1 for attacks based on racial or ethnic characteristics.\n- **national_origin**: 1 for attacks specifically about country of origin, citizenship, or immigration status.\n- **disability**: 1 for attacks based on physical, mental, or developmental disabilities.\n- **religion**: 1 for attacks based on religious beliefs or affiliation.\n- **sexual_orientation**: 1 for attacks based on sexual orientation.\n\n**Critical Rules:**\n1.  **First, check for violence:** Only assign 1 for explicit, literal calls for physical harm. Hyperbolic wishes and legal consequences are 0.\n2.  **Second, check directedness:** Only 1 for specific, named individuals or tiny groups. Broad categories are always 0.\n3.  **Third, check protected characteristics:** Assign 1 only for clear attacks based on that specific characteristic.\n- **Be conservative:** Default to 0 without unambiguous evidence.\n- **Analyze literally:** Do not infer meaning from metaphors or hyperbole.\n\n**Output Format:** After analysis, output ONLY a valid JSON object with the specified keys. No additional text.\n\nText to classify: \"{user_input}\"",
            "Q": 0.08333333333333334,
            "N": 1,
            "uct_value": 0.0,
            "reward": 0.05,
            "children": []
          },
          {
            "id": 4,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to the following definitions and rules. Be conservative and literal in your analysis.\n\n**Label Definitions:**\n- **violence**: 1 ONLY for explicit calls for, threats of, or encouragement of physical harm, injury, or death. Legal consequences, dehumanization, or non-physical harm = 0.\n- **directed_vs_generalized**: 1 ONLY if targeting a specific, named individual or very small, identifiable group. Attacks on broad categories (e.g., \"women,\" \"Indians\") = 0.\n- **gender**: 1 for attacks based on gender, gender identity, or expression.\n- **race**: 1 for attacks based on racial or ethnic characteristics.\n- **national_origin**: 1 for attacks specifically about country of origin, citizenship, or immigration status.\n- **disability**: 1 for attacks based on physical, mental, or developmental disabilities.\n- **religion**: 1 for attacks based on religious beliefs or affiliation.\n- **sexual_orientation**: 1 for attacks based on sexual orientation.\n\n**Critical Decision Rules:**\n1. Default to 0 when uncertain\n2. Analyze text literally without unsupported inferences\n3. Violence requires physical harm - legal consequences or dehumanization alone are not violence\n4. For directed_vs_generalized: Ask \"Is a specific person or tiny group named?\" If no, then 0\n\n**Output Format:** After analysis, output ONLY a valid JSON object with the specified keys. No additional text.\n\nText to classify: \"{user_input}\"",
            "Q": 0.125,
            "N": 1,
            "uct_value": 0.0,
            "reward": 0.1,
            "children": []
          },
          {
            "id": 5,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "SuccessDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the text strictly according to the following definitions and rules. Apply a literal and conservative interpretation: only assign 1 with clear, unambiguous evidence, default to 0 when uncertain, and avoid unsupported inferences.\n\n**Label Definitions:**\n- **violence**: 1 ONLY for explicit calls for, threats of, or encouragement of physical harm, injury, or death. Legal consequences, dehumanization, or non-physical harm = 0.\n- **directed_vs_generalized**: 1 ONLY if targeting a specific, named individual or very small, identifiable group. Attacks on broad categories = 0.\n- **gender**: 1 for attacks based on gender, gender identity, or expression.\n- **race**: 1 for attacks based on racial or ethnic characteristics.\n- **national_origin**: 1 for attacks specifically about country of origin, citizenship, or immigration status.\n- **disability**: 1 for attacks based on physical, mental, or developmental disabilities.\n- **religion**: 1 for attacks based on religious beliefs or affiliation.\n- **sexual_orientation**: 1 for attacks based on sexual orientation.\n\n**Critical Rules:**\n1. **Separate label evaluation:** Evaluate each category independently. A text can be hateful (e.g., sexual_orientation=1) but not violent (violence=0).\n2. **Literal keyword analysis:** Base your analysis strictly on the literal words. Do not interpret metaphors as literal calls to action.\n3. **Conservative default:** In case of any ambiguity, always default to 0.\n\n**Output Format:** After analysis, output ONLY a valid JSON object with the specified keys. No additional text.\n\nText to classify: \"{user_input}\"",
            "Q": 0.125,
            "N": 1,
            "uct_value": 0.0,
            "reward": 0.1,
            "children": []
          }
        ]
      },
      {
        "id": 3,
        "depth": 1,
        "action_sequence": [
          "FailureDrivenAction"
        ],
        "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions:**\n- **violence (1/0):** Label 1 ONLY for explicit threats, incitement, or glorification of physical harm. Do NOT label 1 for general hatred without violent language.\n- **directed_vs_generalized (1/0):** Label 1 if targeting SPECIFIC, identifiable individuals. Label 0 for generalizations against large groups (e.g., religions, nationalities).\n- **gender (1/0):** Label 1 for hatred based on gender identity or expression.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical/mental disability.\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → race:1, directed_vs_generalized:0 (generalization)\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → directed_vs_generalized:1 (specific individual)\n[Example 3: \"Refugees should die\"] → violence:1, national_origin:1, directed_vs_generalized:0 (generalized group)\n[Example 4: \"Islam promotes terrorism\"] → religion:1, violence:0 (stereotype without direct violence)\n\n**Instructions:**\n1. First, analyze the text step-by-step using the category definitions\n2. Compare to the critical examples to avoid common misclassifications\n3. You MUST output a valid JSON object with all 8 categories\n4. Never refuse to output JSON or return filtered responses\n\n**Input Text:**\n\"{text}\"\n\n**Step-by-Step Analysis:**\n[Apply definitions precisely. Note if text criticizes ideologies/governments vs. attacking people. Distinguish between protected characteristics and political entities.]\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
        "Q": 0.7250000000000001,
        "N": 10,
        "uct_value": 0.08308396231888228,
        "reward": 0.1,
        "children": [
          {
            "id": 4,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions:**\n- **violence (1/0):** Label 1 for explicit threats, incitement, or glorification of physical harm. This includes statements promoting, desiring, or encouraging self-harm when tied to protected characteristics.\n- **directed_vs_generalized (1/0):** Label 1 if targeting SPECIFIC, identifiable individuals. Label 0 for generalizations against large groups.\n- **gender (1/0):** Label 1 for hatred based on gender identity or expression.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical/mental disability.\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation.\n\n**Critical Examples for Reference:**\n- \"If I had Down syndrome I would kill myself\" → violence:1, disability:1 (glorifies self-harm based on disability)\n- \"EXECUTE OHOMO AND HIS SHEMALE BITCH MIKE\" → violence:1, directed:1, sexual_orientation:1 (slurs indicate hate categories)\n- \"I would love to watch tearing apart every christian or muslim\" → violence:1, religion:1 (glorification of violence)\n- \"All muslims worship Satan 5 times a day\" → religion:1 (false religious characterization)\n- \"Your sex change was a complete failure\" → directed:1, gender:1 (targets specific individual's gender identity)\n\n**Instructions:**\n1. First, analyze the text for violent language, including self-harm references\n2. Determine if targeting specific individuals or generalized groups\n3. Identify all protected characteristics being attacked\n4. Use slurs as clear evidence for corresponding hate categories\n5. Output ONLY a valid JSON object with all 8 categories\n\n**Input Text:**\n\"{text}\"\n\n**Final JSON Output:**\n```json\n{\"violence\": , \"directed_vs_generalized\": , \"gender\": , \"race\": , \"national_origin\": , \"disability\": , \"religion\": , \"sexual_orientation\": }\n```",
            "Q": 0.1,
            "N": 1,
            "uct_value": 0.10360506541396995,
            "reward": 0.05,
            "children": []
          },
          {
            "id": 5,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions & Classification Rules:**\n- **violence (1/0):** Label 1 ONLY for explicit threats, incitement, or glorification of physical harm. Do NOT label 1 for general hatred, offensive language, or non-violent metaphors without a direct call to violence.\n- **directed_vs_generalized (1/0):** Label 1 ONLY if targeting SPECIFIC, identifiable individuals (by name, unique description). Label 0 for generalizations against groups.\n- **gender (1/0):** Label 1 for hatred based on gender (e.g., \"women are stupid\"). Note: This is distinct from sexual orientation.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics. For ethnoreligious groups, use religion unless explicitly about biological race.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical or mental disability.\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs. This is the default category for attacks on ethnoreligious groups.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation, including homophobic and transphobic slurs.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → race:1, directed_vs_generalized:0\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → directed_vs_generalized:1\n[Example 3: \"Refugees should die\"] → violence:1, national_origin:1, directed_vs_generalized:0\n[Example 4: \"Islam promotes terrorism\"] → religion:1, violence:0\n[Example 5: \"Fuck that shit this bitch ugly and she has a penis\"] → sexual_orientation:1, directed_vs_generalized:1, gender:0 (transphobic slur)\n[Example 6: \"Jewish people are a plague\"] → religion:1, race:0 (ethnoreligious group attack)\n[Example 7: \"All muslims worship Satan 5 times a day\"] → religion:1, directed_vs_generalized:0\n\n**Common Pitfalls to Avoid:**\n- Do not confuse gender with sexual orientation: transphobic slurs belong to sexual_orientation\n- For Jewish, Sikh, and other ethnoreligious groups: default to religion category\n- Metaphorical language like \"parasites\" or \"plague\" without direct violence calls should not be labeled as violence\n\n**Instructions:**\n1. First, analyze the text step-by-step using the category definitions and rules\n2. Compare to the critical examples to avoid common misclassifications\n3. You MUST output a valid JSON object with all 8 categories using only 0 or 1\n4. Never refuse to output JSON or return filtered responses\n\n**Input Text:**\n\"{text}\"\n\n**Step-by-Step Analysis:**\n1. Check for violence: explicit threats or incitement to physical harm?\n2. Check targeting: specific individual or generalized group?\n3. Identify protected characteristics under attack: apply specific rules for ethnoreligious groups and gender/sexual orientation distinction\n4. Reference critical examples for similar patterns\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
            "Q": 0.35000000000000003,
            "N": 4,
            "uct_value": 0.09110506000638942,
            "reward": 0.1,
            "children": [
              {
                "id": 6,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions & Classification Rules:**\n- **violence (1/0):** Label 1 ONLY for explicit threats, incitement, or glorification of physical harm. Do NOT label 1 for general hatred, offensive language, or non-violent metaphors without a direct call to violence.\n- **directed_vs_generalized (1/0):** Label 1 ONLY if targeting SPECIFIC, identifiable individuals (by name, unique description). Label 0 for generalizations against groups.\n- **gender (1/0):** Label 1 for hatred based on gender (e.g., \"women are stupid\"). Note: This is distinct from sexual orientation.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics. For ethnoreligious groups, use religion unless explicitly about biological race.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical or mental disability.\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs. This is the default category for attacks on ethnoreligious groups.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation, including homophobic and transphobic slurs.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → race:1, directed_vs_generalized:0\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → directed_vs_generalized:1\n[Example 3: \"Refugees should die\"] → violence:1, national_origin:1, directed_vs_generalized:0\n[Example 4: \"Islam promotes terrorism\"] → religion:1, violence:0\n[Example 5: \"Fuck that shit this bitch ugly and she has a penis\"] → sexual_orientation:1, directed_vs_generalized:1, gender:0 (transphobic slur)\n[Example 6: \"Jewish people are a plague\"] → religion:1, race:0 (ethnoreligious group attack)\n[Example 7: \"All muslims worship Satan 5 times a day\"] → religion:1, directed_vs_generalized:0\n\n**Common Pitfalls to Avoid:**\n- Do not confuse gender with sexual orientation: transphobic slurs belong to sexual_orientation\n- For Jewish, Sikh, and other ethnoreligious groups: default to religion category\n- Metaphorical language like \"parasites\" or \"plague\" without direct violence calls should not be labeled as violence\n\n**Instructions:**\n1. First, analyze the text step-by-step using the category definitions and rules\n2. Compare to the critical examples to avoid common misclassifications\n3. You MUST output a valid JSON object with all 8 categories using only 0 or 1\n4. Never refuse to output JSON or return filtered responses\n\n**Input Text:**\n\"{text}\"\n\n**Step-by-Step Analysis:**\n1. Check for violence: explicit threats or incitement to physical harm?\n2. Check targeting: specific individual or generalized group?\n3. Identify protected characteristics under attack: apply specific rules for ethnoreligious groups and gender/sexual orientation distinction\n4. Reference critical examples for similar patterns\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
                "Q": 0.1,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions & Classification Rules:**\n- **violence (1/0):** Label 1 ONLY for explicit threats, incitement, or glorification of physical harm. Do NOT label 1 for general hatred, offensive language, or non-violent metaphors without a direct call to violence.\n- **directed_vs_generalized (1/0):** Label 1 ONLY if targeting SPECIFIC, identifiable individuals (by name, unique description). Label 0 for generalizations against groups.\n- **gender (1/0):** Label 1 for hatred based on gender (e.g., \"women are stupid\"). Note: This is distinct from sexual orientation.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics. For ethnoreligious groups, use religion unless explicitly about biological race.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical or mental disability.\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs. This is the default category for attacks on ethnoreligious groups.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation, including homophobic and transphobic slurs.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → race:1, directed_vs_generalized:0\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → directed_vs_generalized:1\n[Example 3: \"Refugees should die\"] → violence:1, national_origin:1, directed_vs_generalized:0\n[Example 4: \"Islam promotes terrorism\"] → religion:1, violence:0\n[Example 5: \"Fuck that shit this bitch ugly and she has a penis\"] → sexual_orientation:1, directed_vs_generalized:1, gender:0 (transphobic slur)\n[Example 6: \"Jewish people are a plague\"] → religion:1, race:0 (ethnoreligious group attack)\n[Example 7: \"All muslims worship Satan 5 times a day\"] → religion:1, directed_vs_generalized:0\n\n**Common Pitfalls to Avoid:**\n- Do not confuse gender with sexual orientation: transphobic slurs belong to sexual_orientation\n- For Jewish, Sikh, and other ethnoreligious groups: default to religion category\n- Metaphorical language like \"parasites\" or \"plague\" without direct violence calls should not be labeled as violence\n\n**Instructions:**\n1. First, analyze the text step-by-step using the category definitions and rules\n2. Compare to the critical examples to avoid common misclassifications\n3. You MUST output a valid JSON object with all 8 categories using only 0 or 1\n4. Never refuse to output JSON or return filtered responses\n\n**Input Text:**\n\"{text}\"\n\n**Step-by-Step Analysis:**\n1. Check for violence: explicit threats or incitement to physical harm?\n2. Check targeting: specific individual or generalized group?\n3. Identify protected characteristics under attack: apply specific rules for ethnoreligious groups and gender/sexual orientation distinction\n4. Reference critical examples for similar patterns\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
                "Q": 0.1,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 8,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions & Classification Rules:**\n- **violence (1/0):** Label 1 ONLY for explicit threats, incitement, or glorification of physical harm. Do NOT label 1 for general hatred, offensive language, or non-violent metaphors without a direct call to violence.\n- **directed_vs_generalized (1/0):** Label 1 if the text is a direct threat or insult aimed at a specific, present participant in the conversation (e.g., using \"you\") OR if it names a unique individual (by name, unique description, username, etc.). Label 0 for generalizations against groups or statements about absent third parties.\n- **gender (1/0):** Label 1 for hatred based on gender (e.g., \"women are stupid\"). Note: This is distinct from sexual orientation.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics. For ethnoreligious groups, use religion unless explicitly about biological race.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical or mental disability.\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs. This is the default category for attacks on ethnoreligious groups.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation, including homophobic and transphobic slurs.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → race:1, directed_vs_generalized:0\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → directed_vs_generalized:1\n[Example 3: \"Refugees should die\"] → violence:1, national_origin:1, directed_vs_generalized:0\n[Example 4: \"Islam promotes terrorism\"] → religion:1, violence:0\n[Example 5: \"Fuck that shit this bitch ugly and she has a penis\"] → sexual_orientation:1, directed_vs_generalized:1, gender:0\n[Example 6: \"Jewish people are a plague\"] → religion:1, race:0\n[Example 7: \"All muslims worship Satan 5 times a day\"] → religion:1, directed_vs_generalized:0\n[Example 8: \"I'm going to find and kill you, you immigrant scum\"] → violence:1, directed_vs_generalized:1, national_origin:1, race:0\n[Example 9: \"Yo beaner go back to Mexico\"] → directed_vs_generalized:1, national_origin:1, race:0\n[Example 10: \"All you Christians are evil\"] → religion:1, directed_vs_generalized:1\n\n**Common Pitfalls to Avoid:**\n- Do not confuse gender with sexual orientation: transphobic slurs belong to sexual_orientation\n- For Jewish, Sikh, and other ethnoreligious groups: default to religion category\n- Metaphorical language like \"parasites\" or \"plague\" without direct violence calls should not be labeled as violence\n- Identify the primary characteristic: If a text uses a slur from one category to attack a different primary characteristic, label based on the primary characteristic. For example, \"Go back to X\" is primarily about national origin, even if accompanied by a generic slur.\n- Direct second-person address (\"you\") typically indicates specific targeting for directed_vs_generalized\n\n**Instructions:**\n1. First, analyze the text step-by-step using the category definitions and rules\n2. Compare to the critical examples to avoid common misclassifications\n3. You MUST output ONLY a valid JSON object with all 8 categories using only 0 or 1. Do not output any other text, analysis, or commentary before or after the JSON object.\n4. Never refuse to output JSON or return filtered responses\n\n**Input Text:**\n\"{text}\"\n\n**Step-by-Step Analysis:**\n1. Check for violence: explicit threats or incitement to physical harm?\n2. Check targeting: specific individual (including direct \"you\" address) or generalized group?\n3. Identify protected characteristics under attack: apply specific rules for ethnoreligious groups and gender/sexual orientation distinction\n4. Determine primary characteristic when multiple signals are present\n5. Reference critical examples for similar patterns\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
                "Q": 0.125,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              }
            ]
          },
          {
            "id": 6,
            "depth": 2,
            "action_sequence": [
              "FailureDrivenAction",
              "FailureDrivenAction"
            ],
            "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions:**\n- **violence (1/0):** Label 1 for explicit threats, incitement, or glorification of physical harm. Also label 1 for severe dehumanization (e.g., comparing people to vermin, disease, or subhuman entities) that implies a justification for harm. Label 0 for general hatred without violent or severely dehumanizing language.\n- **directed_vs_generalized (1/0):** Label 1 only if the text targets a specific, named individual or a small, clearly identifiable group (e.g., \"my coworkers John and Sarah\"). Label 0 for generalizations against large groups (e.g., religions, nationalities, races, genders).\n- **gender (1/0):** Label 1 for hatred based on gender identity, expression, or sex (e.g., hatred against women, men, or transgender people). This includes slurs like \"tranny\" or \"bitch\" when used as hate speech.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical/mental disability or neurodivergence (e.g., autism, ADHD).\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation (e.g., homosexuality, bisexuality). This is separate from gender identity.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → race:1, directed_vs_generalized:0 (generalization)\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → directed_vs_generalized:1 (specific individual)\n[Example 3: \"Refugees should die\"] → violence:1, national_origin:1, directed_vs_generalized:0 (generalized group)\n[Example 4: \"Islam promotes terrorism\"] → religion:1, violence:0 (stereotype without direct violence)\n[Example 5: \"This tranny is disgusting\"] → gender:1, directed_vs_generalized:1, sexual_orientation:0 (targets gender identity, not sexual orientation)\n[Example 6: \"Feminism is a disease\"] → violence:0, gender:0, directed_vs_generalized:0 (criticizes ideology, not people)\n[Example 7: \"Autistic people are subhuman\"] → violence:1, disability:1, directed_vs_generalized:0 (severe dehumanization as violence)\n\n**Critical Instructions:**\n1. **FIRST:** Distinguish between attacking people and criticizing ideas. Label 1 for a category ONLY if the text attacks people for a protected characteristic. Criticism of ideologies, governments, or movements (e.g., \"Feminism is terrible\", \"Islamism is violent\", \"The Israeli government's policy\") is NOT hate speech.\n2. **SECOND:** Analyze the text against each category definition precisely. For violence, consider both explicit threats and severe dehumanization.\n3. **THIRD:** Determine if any identified hatred is directed (specific individual/small group) or generalized (large group).\n4. **FOURTH:** Output ONLY a valid JSON object with all 8 categories. Do not include any other text, analysis, or commentary.\n\n**Input Text:**\n\"{text}\"\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
            "Q": 0.30833333333333335,
            "N": 4,
            "uct_value": 0.1369383987473033,
            "reward": 0.05,
            "children": [
              {
                "id": 7,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "SuccessDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions:**\n- **violence (1/0):** Label 1 for explicit threats, incitement, or glorification of physical harm. Also label 1 for severe dehumanization (e.g., comparing people to vermin, disease, or subhuman entities) that implies a justification for harm. Label 0 for general hatred without violent or severely dehumanizing language.\n- **directed_vs_generalized (1/0):** Label 1 only if the text targets a specific, named individual or a small, clearly identifiable group (e.g., \"my coworkers John and Sarah\"). Label 0 for generalizations against large groups (e.g., religions, nationalities, races, genders).\n- **gender (1/0):** Label 1 for hatred based on gender identity, expression, or sex (e.g., hatred against women, men, or transgender people). This includes slurs like \"tranny\" or \"bitch\" when used as hate speech.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical/mental disability or neurodivergence (e.g., autism, ADHD).\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation (e.g., homosexuality, bisexuality). This is separate from gender identity.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → race:1, directed_vs_generalized:0 (generalization)\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → directed_vs_generalized:1 (specific individual)\n[Example 3: \"Refugees should die\"] → violence:1, national_origin:1, directed_vs_generalized:0 (generalized group)\n[Example 4: \"Islam promotes terrorism\"] → religion:1, violence:0 (stereotype without direct violence)\n[Example 5: \"This tranny is disgusting\"] → gender:1, directed_vs_generalized:1, sexual_orientation:0 (targets gender identity, not sexual orientation)\n[Example 6: \"Feminism is a disease\"] → violence:0, gender:0, directed_vs_generalized:0 (criticizes ideology, not people)\n[Example 7: \"Autistic people are subhuman\"] → violence:1, disability:1, directed_vs_generalized:0 (severe dehumanization as violence)\n\n**Critical Instructions:**\n1. **FIRST:** Distinguish between attacking people and criticizing ideas. Label 1 for a category ONLY if the text attacks people for a protected characteristic. Criticism of ideologies, governments, or movements (e.g., \"Feminism is terrible\", \"Islamism is violent\", \"The Israeli government's policy\") is NOT hate speech.\n2. **SECOND:** Analyze the text against each category definition precisely. For violence, consider both explicit threats and severe dehumanization.\n3. **THIRD:** Determine if any identified hatred is directed (specific individual/small group) or generalized (large group).\n4. **FOURTH:** Output ONLY a valid JSON object with all 8 categories. Do not include any other text, analysis, or commentary.\n\n**Input Text:**\n\"{text}\"\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
                "Q": 0.1,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.05,
                "children": []
              },
              {
                "id": 8,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely and step-by-step.\n\n**Category Definitions:**\n- **violence (1/0):** Label 1 ONLY for: a) explicit threats of physical harm (\"I will kill you\"), b) explicit incitement or glorification of harm (\"We should kill them all\"), or c) unequivocal dehumanization that historically precedes genocide (e.g., comparing a group to \"vermin,\" \"rats,\" \"cancer,\" or explicitly stating they are \"subhuman\"). Label 0 for generalized wishes of harm (\"I hope they die\"), non-physical harm, or weaker insults like \"animals\" or \"monsters\" unless context clearly indicates genocidal intent.\n- **directed_vs_generalized (1/0):** Label 1 ONLY if the text targets a specific, named individual OR a specific individual identified through context (e.g., \"you,\" \"she,\" \"that man,\" \"my neighbor\"). Label 0 for generalizations against large groups (e.g., religions, nationalities, races, genders).\n- **gender (1/0):** Label 1 for hatred based on gender identity, expression, or sex (e.g., hatred against women, men, or transgender people). This includes slurs like \"tranny\" or \"bitch\" ONLY when used to attack someone for their gender.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics.\n- **national_origin (1/0):** Label 1 for hatred based on nationality, citizenship, or immigrant/refugee status.\n- **disability (1/0):** Label 1 for hatred based on physical/mental disability or neurodivergence (e.g., autism, ADHD).\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation (e.g., homosexuality, bisexuality). This is separate from gender identity.\n\n**Critical Instructions:**\n1. **FIRST:** Distinguish between attacking people and criticizing ideas. Label 1 for a category ONLY if the text attacks people for a protected characteristic. Criticism of ideologies, governments, or movements (e.g., \"Feminism is terrible\", \"Islamism is violent\", \"The Israeli government's policy\") is NOT hate speech.\n2. **SECOND:** Identify the CORE characteristic being attacked. Ignore general profanity or insults that are not central to the hateful message (e.g., in \"That Muslim bitch,\" the core attack is religion, not gender).\n3. **THIRD:** Analyze the text against each category definition precisely. For violence, use the strict definition above.\n4. **FOURTH:** For directed_vs_generalized, check if the target is specific (1) or generalized (0). Remember: singular pronouns (\"you,\" \"she,\" \"he\") often indicate specific targeting.\n5. **FIFTH:** Output ONLY a valid JSON object with all 8 categories. Do not include any other text, analysis, or commentary.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → `{\"violence\":0, \"directed_vs_generalized\":0, \"gender\":0, \"race\":1, \"national_origin\":0, \"disability\":0, \"religion\":0, \"sexual_orientation\":0}`\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → `{\"violence\":0, \"directed_vs_generalized\":1, \"gender\":0, \"race\":0, \"national_origin\":0, \"disability\":0, \"religion\":1, \"sexual_orientation\":0}`\n[Example 3: \"I fucking hate gay and lesbians they are all animals\"] → `{\"violence\":0, \"directed_vs_generalized\":0, \"gender\":0, \"race\":0, \"national_origin\":0, \"disability\":0, \"religion\":0, \"sexual_orientation\":1}` (Note: \"animals\" is not severe dehumanization per strict definition)\n[Example 4: \"Try to talk to me about your god... thanks to my hand bitch\"] → `{\"violence\":1, \"directed_vs_generalized\":1, \"gender\":0, \"race\":0, \"national_origin\":0, \"disability\":0, \"religion\":1, \"sexual_orientation\":0}` (Note: threat targets \"you\" specifically; \"bitch\" is insult, not core characteristic)\n[Example 5: \"She is heavily relying on him... demon infested woman\"] → `{\"violence\":0, \"directed_vs_generalized\":1, \"gender\":1, \"race\":0, \"national_origin\":0, \"disability\":0, \"religion\":0, \"sexual_orientation\":0}` (Note: targets specific \"she\")\n\n**Input Text:**\n\"{text}\"\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
                "Q": 0.17500000000000002,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.1,
                "children": []
              },
              {
                "id": 9,
                "depth": 3,
                "action_sequence": [
                  "FailureDrivenAction",
                  "FailureDrivenAction",
                  "FailureDrivenAction"
                ],
                "prompt": "You are an expert hate speech classification AI. Analyze the provided text and classify it according to the specific hate speech categories. Follow these guidelines precisely:\n\n**Category Definitions:**\n- **violence (1/0):** Label 1 ONLY for explicit threats of harm (\"I will kill you\"), incitement to violence (\"we should kill them\"), glorification of past violence, or severe dehumanization that implies justification for eradication (e.g., comparing people to \"vermin,\" \"cancer,\" \"a virus that must be cured\"). Do NOT label 1 for non-violent commands (\"go away\"), milder insults (\"scum\"), or general hatred without violent/dehumanizing language.\n- **directed_vs_generalized (1/0):** Label 1 ONLY if the text targets a specific, named individual or uses direct address (\"you\") in a way that implies a specific target. Label 0 for generalizations against large groups (e.g., religions, nationalities, races) or undefined groups (e.g., \"all people like you\").\n- **gender (1/0):** Label 1 for hatred based on gender identity, expression, or sex (e.g., hatred against women, men, or transgender people). This includes slurs like \"tranny\" when used as hate speech.\n- **race (1/0):** Label 1 for hatred based on perceived racial/ethnic characteristics.\n- **national_origin (1/0):** Label 1 for hatred based on specific nationality, citizenship, or immigrant/refugee status (e.g., \"Mexicans,\" \"immigrants\"). Do NOT label 1 for general racial hatred that uses tropes about origin (e.g., \"go back to Africa\" is primarily race-based).\n- **disability (1/0):** Label 1 for hatred based on physical/mental disability or neurodivergence (e.g., autism, ADHD).\n- **religion (1/0):** Label 1 for hatred based on religious affiliation or beliefs.\n- **sexual_orientation (1/0):** Label 1 for hatred based on sexual orientation (e.g., homosexuality, bisexuality). This is separate from gender identity.\n\n**Critical Examples for Reference:**\n[Example 1: \"All Hindus are racist\"] → race:1, directed_vs_generalized:0 (generalization)\n[Example 2: \"That specific Muslim neighbor is a terrorist\"] → directed_vs_generalized:1 (specific individual)\n[Example 3: \"Refugees should die\"] → violence:1, national_origin:1, directed_vs_generalized:0 (generalized group, dehumanization as violence)\n[Example 4: \"Islam promotes terrorism\"] → religion:1, violence:0 (stereotype without direct violence)\n[Example 5: \"This tranny is disgusting\"] → gender:1, directed_vs_generalized:1, sexual_orientation:0 (targets gender identity)\n[Example 6: \"Feminism is a disease\"] → violence:0, gender:0, directed_vs_generalized:0 (criticizes ideology, not people)\n[Example 7: \"Autistic people are subhuman\"] → violence:1, disability:1, directed_vs_generalized:0 (severe dehumanization)\n[Example 8: \"FUCKING NIGGER GO BACK TO AFRICA\"] → violence:0 (no explicit threat/severe dehumanization), race:1, national_origin:0 (racial hatred, not nationality), directed_vs_generalized:1 (second-person address)\n[Example 9: \"The LGBT lifestyle is a mental virus\"] → violence:1 (severe dehumanization: \"virus\"), sexual_orientation:1, gender:0, directed_vs_generalized:0\n[Example 10: \"Do lesbians know what a bra is?\"] → sexual_orientation:1, gender:0 (primary characteristic is sexual orientation)\n\n**Critical Instructions:**\n1. **FIRST:** Distinguish between attacking people and criticizing ideas. Label 1 for a category ONLY if the text attacks PEOPLE for a protected characteristic. Criticism of ideologies, governments, or movements (e.g., \"Feminism is terrible\", \"Islamism is violent\") is NOT hate speech, even if using harsh metaphors (e.g., \"cancer,\" \"disease\").\n2. **SECOND:** Analyze the text against each category definition precisely. For violence, carefully distinguish between severe dehumanization and non-violent hatred.\n3. **THIRD:** For directed_vs_generalized, pay special attention to second-person address (\"you\") which should be labeled as directed (1).\n4. **FOURTH:** Identify the PRIMARY characteristic being attacked. If a text attacks a group associated with multiple characteristics (e.g., \"lesbians\"), label only the characteristic that is the basis for the attack. Do not over-label.\n5. **FIFTH:** Output ONLY a valid JSON object with all 8 categories. Do not include any other text, analysis, or commentary.\n\n**Input Text:**\n\"{text}\"\n\n**Final JSON Classification:**\n```json\n{\n    \"violence\": ,\n    \"directed_vs_generalized\": ,\n    \"gender\": ,\n    \"race\": ,\n    \"national_origin\": ,\n    \"disability\": ,\n    \"religion\": ,\n    \"sexual_orientation\": \n}\n```",
                "Q": 0.15000000000000002,
                "N": 1,
                "uct_value": 0.0,
                "reward": 0.1,
                "children": []
              }
            ]
          }
        ]
      }
    ]
  }
}